<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Lukas&#39;s Writings</title>
  
  
  <link href="http://loreley.one/atom.xml" rel="self"/>
  
  <link href="http://loreley.one/"/>
  <updated>2025-11-05T00:10:42.576Z</updated>
  <id>http://loreley.one/</id>
  
  <author>
    <name>Lukas</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Modern 800m Training Overview</title>
    <link href="http://loreley.one/2025-11-Modern-800m-training-overview/"/>
    <id>http://loreley.one/2025-11-Modern-800m-training-overview/</id>
    <published>2025-11-04T23:00:00.000Z</published>
    <updated>2025-11-05T00:10:42.576Z</updated>
    
    <content type="html"><![CDATA[<p>Over the last few weeks I’ve been developing my training program for the 2026 season. It’s been really hard to find high quality writing specifically dedicated to the 800. By far the best I got was the below overview from OpenAI’s DeepResearch feature after I gave it a very detailed prompt. Getting this output was only possible because I’m familiar enough with the sport to know what keywords to reference (such as a mention of Justin Rinaldi). I’m sharing it below so others may benefit. </p><h1 id="Modern-800m-Training-Frameworks-for-Sub-Elite-Athletes"><a href="#Modern-800m-Training-Frameworks-for-Sub-Elite-Athletes" class="headerlink" title="Modern 800m Training Frameworks for Sub-Elite Athletes"></a><strong>Modern 800m Training Frameworks for Sub-Elite Athletes</strong></h1><h2 id="Big-Picture-Training-Models-Speed-vs-Endurance-Emphasis"><a href="#Big-Picture-Training-Models-Speed-vs-Endurance-Emphasis" class="headerlink" title="Big-Picture Training Models: Speed vs. Endurance Emphasis"></a><strong>Big-Picture Training Models: Speed vs. Endurance Emphasis</strong></h2><p><strong>Different Paths to 1:48.</strong> Athletes arrive at 1:48 from varying backgrounds – some are 400m sprinters moving up, others are milers moving down. Modern training takes an individualized approach, often using the <strong>Anaerobic Speed Reserve (ASR)</strong> model to classify 800m runners as <strong>“400&#x2F;800 speed-types,”</strong> <strong>“pure 800 hybrids,”</strong> or <strong>“800&#x2F;1500 endurance-types.”</strong> This influences total running volume and workout mix. For example, an elite speed-type might handle ~40–60 km per week, whereas an endurance-type might run 100+ km&#x2F;week (sub-elite volumes will be lower but follow the same pattern). The <strong>speed-centric 400&#x2F;800 approach</strong> favors lower mileage and more anaerobic work, while the <strong>endurance-centric 800&#x2F;1500 approach</strong> uses higher mileage and more aerobic work . Each model “works,” but the best fit depends on the athlete’s natural strengths and history.</p><p><strong>Matching Program to Athlete.</strong> A former sprinter with a blazing 400m PB likely needs to emphasize aerobic development (threshold runs, longer intervals) to complement their speed. In contrast, a mileage-heavy 1500m runner moving to 800m must not neglect speed development (sprints, plyometrics) to improve their kick. Masters athletes and those with injury histories lean toward the conservative side – for example, extra recovery days and reduced plyometric intensity for an older runner with achy joints. Crucially, no single formula fits all: research shows huge individual variation in how <strong>aerobic vs. anaerobic</strong> energy is utilized in an 800m . One runner’s race might be only ~15% anaerobic while another’s is ~33% anaerobic for the same finish time . This explains why both a 22-second 200m sprinter and a 3:36 1500m runner can each run 1:48 via different training routes. A coach should be <em>pragmatic, not dogmatic</em>, in mixing these schools of thought – the program must be tuned to the athlete’s profile, leveraging their strengths while shoring up weaknesses.</p><h2 id="Energy-Systems-and-Pace-Domains"><a href="#Energy-Systems-and-Pace-Domains" class="headerlink" title="Energy Systems and Pace Domains"></a><strong>Energy Systems and Pace Domains</strong></h2><p><strong>800m &#x3D; Sprint + Endurance.</strong> The 800m demands contributions from all three major energy systems in overlapping fashion. Off the start and during any all-out surge, the <strong>alactic</strong> <strong>ATP-PC system</strong> provides instant energy for ~5–10 seconds without producing lactate . As the race progresses, the <strong>glycolytic (anaerobic)</strong> system kicks in to sustain fast speeds, generating energy with lactate by-products for efforts lasting 20–60 seconds or so . Meanwhile, the <strong>aerobic system</strong> is steadily increasing its output; by around 60 seconds into an 800m race, the aerobic share of energy approaches equal footing with anaerobic . (In fact, modern testing shows about <strong>55–65% of the 800m energy comes from aerobic metabolism</strong> on average , much higher than old myths suggested.) The practical upshot: a sub-2:00 800m runner needs both the anaerobic power to run a fast opening lap <em>and</em> the aerobic capacity to hang on. <strong>Training must develop both.</strong> You can’t train 800m as a pure sprint (or you’ll tie up late), nor as a pure distance run (or you’ll lack speed) – a balanced approach is key .</p><p><strong>Training Zones Mapped to 800m Performance.</strong> Coaches translate these energy systems into concrete training intensity zones, each with a purpose and typical use:</p><ul><li><p><strong>Recovery Jog &amp; Easy Pace:</strong> Very relaxed running used for active recovery and base mileage. This might be &lt;50% of 800m race speed – truly slow jogging . These runs build an aerobic base and promote blood flow for recovery. They can be done frequently (even daily easy runs), as they carry minimal fatigue cost. For example, a 1:48 runner (~54 s per 400m) may do easy runs at 4:30–5:00&#x2F;km or slower. The “why” here is general endurance and recovery; keeping these days easy (conversational effort) ensures hard days can be truly hard.</p></li><li><p><strong>Steady&#x2F;Aerobic Base Runs:</strong> Moderate-pace runs a notch above easy, used occasionally to build aerobic capacity. These might correspond to ~60–75% of 800m speed (e.g. around 10k to half-marathon race pace) and often last 20–40 minutes . They provide a stronger aerobic stimulus without tipping into heavy fatigue. A sub-elite 800m runner might include a steady run or high-end aerobic fartlek once a week. The cost is still relatively low – a day’s recovery – but one wouldn’t stack them right before key interval sessions.</p></li><li><p><strong>Tempo &#x2F; Threshold (LT) Work:</strong> Sustained runs or intervals at lactate threshold (<del>1-hour race pace for many athletes). This intensity (</del>80% of 800m speed, roughly 5k–10k pace) improves the lactate clearance and cruising ability. For an 800m runner, tempo runs or threshold intervals (e.g. 20 minutes continuous or 5×5 minutes) build the aerobic engine that supports later high-intensity work. A sub-elite might do this kind of session about once a week or once every 10–14 days . The recovery cost is moderate – typically require easy running the day after, but because intensity is controlled, these workouts are sustainable in a base phase.</p></li><li><p><strong>“Critical Velocity” &#x2F; Aerobic Power Intervals:</strong> This zone borders between heavy and severe intensity – think 3k to 5k race pace, or the speed you could hold for ~15–20 minutes in an all-out effort. Some coaches call it <strong>critical velocity (CV)</strong> or <strong>VO₂max pace</strong>. Workouts here might be 800m or 1000m repeats, or short hill repeats, at ~85–90% of 800m speed . The purpose is to increase aerobic power (VO₂max) and extend one’s ability to sustain high percentages of VO₂max. These sessions are often done weekly or biweekly in a base or early specific phase. They incur a higher recovery cost – a hard CV interval session might need 48 hours of lighter training afterwards – but they develop the aerobic “ceiling” critical for middle-distance performance.</p></li><li><p><strong>3k&#x2F;5k Pace Intervals:</strong> Overlapping with CV, slightly shorter repeats at 3k–5k race pace (for a 1:48 800m runner, this might be around 60–65 seconds per 400m) also target VO₂max. For instance, 5×600 m at 3k pace with 2:00 rest pushes the aerobic system to its max. These sessions are usually done once every 1–2 weeks. They provide a bridge between pure aerobic work and the more anaerobic work to come. Recovery cost is fairly high (24–48 hours needed) because of the high heart rates and some lactate accumulation.</p></li><li><p><strong>1500m Pace (Speed Endurance):</strong> Intervals at or around 1500m race pace (~95% of 800m speed) hit a mix of aerobic and anaerobic conditioning. Examples might be 4×400 m at 1500 pace with short rest. These improve what coaches call <strong>“speed endurance”</strong> – the ability to hold a fast rhythm for a few minutes. Sessions at 1500 pace are a staple in bridging the gap between pure endurance and the specific 800m pace. They can appear year-round but grow in importance as racing season nears. Typically done once every 1–2 weeks, often alternating with threshold or VO₂ sessions. They create moderate lactate levels, so an easy day before and a lighter day after are warranted.</p></li><li><p><strong>800m Race-Pace Sessions:</strong> These are the bread-and-butter specific workouts, done at roughly 800m goal pace (100%–105% of race speed) . Examples include repetitions like 3×600 m at 800 pace with full recoveries, or cut-downs totaling 800m (e.g. 500m + 300m at race pace). The “why” is to build <strong>lactate tolerance, race-specific strength, and pacing familiarity</strong>. These sessions produce high lactate (the familiar “burn” and heavy legs), so they carry a high recovery cost. In a specific preparation phase, an athlete might do one race-pace session every 7–10 days . Because they are so taxing, athletes often need 2+ days of lighter training afterward. The benefit is clear: these workouts directly simulate the demands of the 800m – by training at 1:48 pace, the body learns to buffer lactate and maintain form under fatigue.</p></li><li><p><strong>400m Pace and Faster (Anaerobic Speed Reserve training):</strong> Running faster than 800m race pace – e.g. 200m repeats at 400m race speed, or flying 150m sprints – builds <strong>speed reserve</strong>. By improving maximum sprint speed, 800m race pace becomes easier by comparison . These fast reps (often ~110–120% of 800m speed) train the neuromuscular system and anaerobic capacity. A typical approach is to include a <strong>speed endurance session</strong> every 1–2 weeks, such as 6×200 m at 400m pace with ample rest. The focus is on quality (sharp, relaxed running form) rather than volume. These workouts are very anaerobic but usually brief in volume; they do create significant fatigue (both metabolic and neural), so allow several days before the next similar session. They are especially crucial for the 400&#x2F;800-type athlete who relies on speed, but even endurance-types include some faster-than-800 work to raise their upper limit.</p></li><li><p><strong>Alactic Speed and Acceleration (Short Sprints&#x2F;Strides):</strong> At the top end, athletes do <strong>alactic sprints</strong> – very short (5–60 m) accelerations at near-max intensity with full recovery – to develop pure speed and neuromuscular coordination. These are often incorporated as <strong>strides</strong> (e.g. 4×100 m at 90% sprint speed) at the end of easy runs, or as hill sprints (8–10 seconds uphill) early in workouts. They recruit maximum muscle fibers and improve mechanics without building up lactate (efforts are too brief to flood the system). Frequency can be high: alactic strides can be sprinkled 1–3 times per week year-round because the volume is low and they don’t leave one sore or exhausted. For instance, doing 4×60 m accelerations twice a week helps maintain leg speed and efficiency. Coaches include these to keep the “neural engine” tuned – as one modern 800m program notes, <em>maximal sprint work at least one day per week</em> is ideal, and it <strong>does not</strong> heavily fatigue the athlete for aerobic training the next day . In practice, an athlete might do a sprint&#x2F;technique session (short bursts, plyometrics, drills) on Monday, then an aerobic workout Tuesday.</p></li></ul><p>Each of these zones has its place in an 800m program. Training “the whole spectrum” ensures the athlete has no weak links – from a powerful start and speed reserve to a high aerobic base. <strong>Crucially, the zones support each other:</strong> improving threshold pace, for example, raises the ceiling for 800m-specific work because you can do more reps or recover faster . Conversely, developing pure speed improves economy at submaximal paces. The event is a balancing act: too much focus on one zone at the expense of others and performance will plateau. As a rule, an 800m runner touches all paces during a training cycle (even if emphasis shifts with the season) .</p><h2 id="Session-Archetypes-and-Weekly-Structure"><a href="#Session-Archetypes-and-Weekly-Structure" class="headerlink" title="Session Archetypes and Weekly Structure"></a><strong>Session Archetypes and Weekly Structure</strong></h2><p><strong>Key Workout Types.</strong> In a given week or 7–10 day microcycle, a sub-elite 800m runner will include several recurring session types: an <strong>aerobic development workout</strong>, a <strong>speed&#x2F;neuromuscular workout</strong>, and a <strong>race- or pace-specific workout</strong> – plus plenty of easy running and strides for recovery and maintenance. For example, a microcycle might feature one threshold or CV interval session (for aerobic power), one short sprint or hill sprint session (for pure speed and mechanics), and one lactate-intensive session at 800&#x2F;1500 pace (for race specificity). In between, the athlete does easy runs, cross-training, strides, and perhaps a longer run for base endurance. This approach reflects the need to “squeeze in” many elements for 800m training – speed, strength, endurance – within a limited time.</p><p>A sample allocation (not a strict schedule) could be:</p><ul><li><p><strong>1 × Aerobic Workout:</strong> e.g. tempo run or 5 × 800 m at 5k pace on a mid-week day.</p></li><li><p><strong>1 × Speed&#x2F;Power Workout:</strong> e.g. short sprints, flying 30m repeats, or 8 × 10s hill sprints on a separate day (often after a rest&#x2F;easy day when the athlete is fresh).</p></li><li><p><strong>1 × Race-Pace or Special Endurance Workout:</strong> e.g. 3 × 500 m at 800m race pace with full recovery, or a blended session like 300m at 95% effort + 4 min rest + 300m fast to simulate the “bear” of the second lap. This is often on a weekend or at least 2–3 days after the aerobic workout to allow recovery.</p></li><li><p><strong>Strides and Drills:</strong> 2–3 times a week after easy runs, the athlete might do 4–6×100 m strides or technique drills to maintain speed and reinforce good form.</p></li><li><p><strong>Easy Runs and Rest:</strong> The remaining days are low-intensity runs or rest days to accumulate aerobic volume and ensure recovery.</p></li></ul><p><strong>Flexible 7–10 Day Cycle.</strong> Many sub-elites use a 7-day week out of practicality, but some prefer a 9–10 day cycle to fit in everything with more recovery. For instance, a coach may schedule hard sessions on Tuesday, Friday, and Sunday, with lighter runs on other days. Others follow a “hard day, easy day” principle. The exact timing is adjusted to the athlete’s needs: a speed-type athlete might need more recovery after a heavy speed endurance session, whereas an endurance-type might need extra recovery after a rare pure speed day (since sprinting is novel stress for them). It’s common to pair heavy training elements on the same day when possible – for example, doing weights after the track workout – so that other days can be fully dedicated to recovery. One modern approach is to do the pure sprint session at the start of the week (e.g. Monday) when the athlete is freshest , because high-quality speed requires full neuromuscular freshness. The next day can then shift to metabolic work (e.g. longer intervals), since the short sprints didn’t deplete the aerobic system heavily. By contrast, after a grueling 800m-pace session, coaches often prescribe 1–2 days of only easy runs or rest due to the high glycolytic stress.</p><p><strong>Adjusting for Fatigue and Background.</strong> The weekly distribution is not rigid; coaches and athletes must monitor fatigue and adjust. If an athlete is showing signs of overload (poor sleep, heavy legs, elevated heart rate), it’s wise to delay the next intense session or swap it for additional easy running. Background plays a role in dosing: an ex-sprinter might do well with only one big aerobic workout per cycle and relatively more pure speed maintenance (since speed comes easily to them), while an ex-distance runner might handle two aerobic workouts but needs to be careful with too much plyometric or speed volume (to avoid injury). Some sub-elites alternate emphasis week to week – e.g. one week include two endurance-type workouts and one speed session, the next week two speed-oriented sessions and one aerobic – to cover all bases. The overall goal is to hit each energy system regularly (so no capacity atrophies), but not so often that the athlete can’t recover. Coaches often talk about finding the maximum <strong>“trainable load”</strong> that an athlete can absorb; for a 1:48 runner this might mean 3–4 hard efforts (of varying types) per 8–10 days. Trying to cram more quality than the athlete can absorb will lead to diminishing returns or injury.</p><h2 id="Periodization-and-Season-Flow"><a href="#Periodization-and-Season-Flow" class="headerlink" title="Periodization and Season Flow"></a><strong>Periodization and Season Flow</strong></h2><p><strong>Phased but Continuous Development.</strong> Modern 800m programs are typically periodized into macro-phases, each with a shifting focus, while maintaining the core elements year-round. A typical season plan might include: an <strong>off-season transition</strong>, a <strong>general preparation (base) phase</strong>, a <strong>special preparation phase</strong>, a <strong>specific&#x2F;competition phase</strong>, followed by <strong>taper</strong> and <strong>post-season rest</strong>. The guiding principle is to move from general to specific, gradually layering higher intensity as the goal races approach, but <strong>never dropping any quality entirely</strong> (you always keep some speed, some endurance – only the ratio changes) .</p><ul><li><p><strong>Transition&#x2F;Off-Season (Regeneration):</strong> Immediately after the competitive season, athletes take a break to recover <strong>physically and mentally</strong>. This might be ~2 weeks of total rest or unstructured light activity . It’s critical for repairing minor injuries and preventing burnout. After this short rest, a brief <strong>introductory period</strong> of 2–4 weeks reintroduces easy running and basic drills gradually . During this time, the athlete keeps training very light – mostly comfortable mileage, cross-training, and short strides or hill sprints – to “recharge the batteries”  without losing too much fitness. No hard workouts are scheduled; by the end of the intro phase the runner builds up to their normal running volume.</p></li><li><p><strong>General Preparation (Fundamental Base Phase):</strong> This is the longest phase (often 3–5 months, e.g. November through early spring for an outdoor 800m runner)  . The focus is on high volume of training and gradual development of aerobic capacity, basic strength, and running mechanics. <strong>Mileage increases progressively</strong> to the target peak (which depends on the athlete’s profile – maybe 40–50 miles&#x2F;week for a speed-type, or 60–70+ for an endurance-type at sub-elite level). Interval training is introduced and increased in volume, but <strong>intensities are mostly moderate</strong> at this stage . For example, an athlete might do lots of threshold runs, longer intervals at 5k&#x2F;3k pace, and short hill sprints. Importantly, <strong>workouts avoid extreme lactic intensity</strong> in base phase – intervals are “hard but controlled” to build aerobic capacity without the stagnation that can come from too much anaerobic work too soon  . Speed training is present year-round but is kept in a facilitative mode: short hills or 60m strides (no exhaustive 300m repeats yet). The idea is to <strong>establish a broad aerobic and strength foundation</strong> (“train the engine”) that will later support the race-specific training. Coaches often include a weekly long run (for 800m perhaps 60–90 minutes) during this phase for aerobic strength and capillary development. If an athlete does an <strong>indoor season</strong>, it usually overlaps late base or early special phase – some will insert a brief competition block in Jan&#x2F;Feb as a checkpoint, then resume training.</p></li><li><p><strong>Special Preparation Phase:</strong> After a solid base, training shifts to <strong>“sharpening the knife”</strong> – more workouts at race-relative paces from 400m to 3000m pace. This phase might be ~6–8 weeks leading into the competitive season. The intensity of interval sessions increases <strong>progressively week to week</strong> , while overall volume of intervals might slightly decrease to accommodate that intensity. For example, where base phase intervals were at 5k&#x2F;3k pace, now many workouts move to 1500m pace, 800m pace, and even some 400m pace work. The phrase “specific preparation” is often used interchangeably here – essentially, it’s the phase bridging general fitness to very race-specific fitness. A typical week in special prep might include a 1500m-paced session (e.g. 5×300m at 1500 pace), a pure speed session (e.g. 2×150m all-out, plus plyos), and perhaps an over-distance aerobic session like 6×800m at 3k pace. <strong>Lactate tolerance workouts start appearing</strong> in this phase, but sparingly at first – perhaps a session of 2×500m at 800m pace with full rest to introduce the body to high lactate. As intensity goes up, recovery within and between sessions is carefully monitored; longer rests are given to achieve quality speeds, and easy days become easier or slightly more frequent. By the end of the special phase, the athlete has <strong>“connected” their speed and endurance</strong>: they can run fast splits and have the aerobic strength to repeat them.</p></li><li><p><strong>Competition&#x2F;Specific Phase:</strong> In the heart of racing season, training becomes highly specific to the 800m race demands. Workouts now often mimic the shape or feel of the race: for example, 600m at race pace + 30s rest + 200m fast (to practice the painful finish), or 3×300m at slightly faster than race pace with short rest (to practice running on tired legs). Intensity is at its peak (800m race pace and faster), so <strong>the total volume of fast running is lower and recovery is paramount</strong> . Key sessions might come only once every 5–7 days now, with the other days focused on maintaining aerobic fitness (through brief tempos or moderate runs) and staying fresh. In this phase, <strong>race modeling</strong> is emphasized – for instance doing a time trial over 600m or broken 800m to rehearse pacing. It’s noted that as athletes do more race-pace work, they often need <strong>more days between hard sessions and more recovery within sessions</strong> . The coach’s job is to balance continuing to build&#x2F;maintain fitness with allowing the athlete to be rested enough to race well. Many programs keep a small dose of longer work (e.g. a fortnightly tempo run or longer intervals at 3k pace) even in competition phase to maintain aerobic base  – but these are done at lower volume to avoid fatigue. If the athlete has a dense racing schedule, formal workouts are reduced; races themselves replace some high-intensity days. Every 3–4 weeks, a lighter “down week” may be inserted to prevent overtraining during the competitive grind.</p></li><li><p><strong>Taper and Peak:</strong> Leading into the goal race or championship, training load is tapered. There is no single formula for tapering 800m runners, but commonly the final 10–14 days see a drop in total running volume (e.g. reduce mileage by ~20–30%) while keeping a few intense but short tune-up sessions to stay sharp. For example, 1 week out one might do an 800m race-pace workout but with fewer reps (to boost confidence and stimulus without exhaustion). Intensity remains high during the taper, but the volume of fast running is low and recovery is prioritized . The last few days before a big race might include strides and one short “blow-out” (like 2×200m at race pace) just to stay crisp. The goal of tapering is to shed accumulated fatigue and bring the athlete to the line <strong>fresh, but not feeling flat</strong>. Many 800m runners actually feel best with a relatively short taper – they don’t want to drop volume too early and lose the feeling of fitness, so the art is reducing just enough load to super-compensate in time for race day.</p></li><li><p><strong>Post-Season Reset:</strong> After the final races, a recovery phase is critical (as mentioned in transition). This is when the body heals and the mind unwinds from the stress of competition. Any niggling injuries are addressed. It’s also when a coach and athlete reflect on the season’s lessons (e.g. what workouts correlated with improvements, how well the peaking strategy worked, etc.), which will inform the next cycle. Skipping this phase can lead to cumulative fatigue or injury in the long term, so even highly motivated athletes are urged to take their off-season rest seriously .</p></li></ul><p><strong>Adjusting for Indoors, Climate, and Travel.</strong> Many sub-elite athletes compete indoors in winter. A typical approach is a shortened base phase, then a mini <strong>specific phase for indoor</strong> (focusing on speed and anaerobic work a bit earlier in the year). After the indoor season (which might last 4–6 weeks), smart athletes take a brief rest (a week off or very easy) before rebuilding toward outdoors – essentially creating a double peak in the year. Training for an indoor 800 (often on a 200m track) might include a bit more speedwork for handling tight bends and a premium on positioning, whereas outdoors allows a longer straightaway. Environmental factors like heat and altitude also affect periodization. In a hot climate or summer training, athletes will schedule key workouts in cooler morning&#x2F;evening hours and emphasize hydration (more on that later). Altitude training, if utilized, usually happens in base or early specific prep – at altitude, paces for aerobic work are adjusted slower, and the focus might shift to volume and strength (since hitting 800m race pace is harder in thin air). After returning from altitude (10–14 days post-return), athletes often see a performance boost if timed correctly with the competition phase. Travel to competitions is another consideration: long travel can mimic a hard training stress. Coaches often plan an easy day or rest after transcontinental travel and arrive at major meets a few days early to acclimate. Small adjustments like compression during flights, light shakeout runs after travel, and maintaining sleep routines become part of the elite traveling 800m runner’s plan.</p><p>In summary, periodization guides the athlete from <em>extensive to intensive</em> work: lots of volume and strength in early phases, shifting toward race-pace, anaerobic power, and finally peak freshness. But this is done without completely abandoning any ingredient – even in high volume phases, a bit of speed is kept, and in racing season, a bit of aerobic maintenance is kept . The athlete thus carries their aerobic base, strength, and speed all the way through, simply changing the mix as needed. The result is an athlete who is aerobically strong, maximally speedy, and specifically ready for the 800m when it counts.</p><h2 id="Strength-Power-and-Mobility-for-the-800m"><a href="#Strength-Power-and-Mobility-for-the-800m" class="headerlink" title="Strength, Power, and Mobility for the 800m"></a><strong>Strength, Power, and Mobility for the 800m</strong></h2><p><strong>The Role of Strength Training.</strong> Unlike pure distance runners, 800m athletes benefit greatly from well-developed strength and power. The event is sometimes called “a sprint with an endurance component,” and having higher <strong>maximal strength</strong> allows an athlete to apply more force with each stride, especially in the first 200m and during the kick. Key areas of focus are the legs (glutes, quads, hamstrings, calves) and core. In practice, 800m runners incorporate weight training 1–3 times per week depending on the season. The emphasis is on <strong>quality over quantity</strong> – low-rep, heavy lifts to build strength without adding bulk, and explosive movements to translate that strength into running-specific power. Typical exercises include squats or split-squats, step-ups or lunges, hip thrusts, deadlifts or Olympic lift variations, and upper body exercises for posture (e.g. pull-ups, rows, push-ups). Coaches avoid “bodybuilder” style routines – indeed, doing lots of bench press and biceps curls has <strong>no positive impact on 800m performance</strong> . Instead, the 800m athlete zeroes in on <strong>functional strength</strong>: single-leg exercises that mimic running, hip and glute strength for stability, and core work for an efficient gait . Strength training is done with explosive intent – e.g. lifting a relatively heavy weight but in a controlled, fast manner – to develop not just force, but the <strong>rate of force development</strong> critical in sprinting.</p><p><strong>Power and Plyometrics.</strong> To run 1:48, raw strength isn’t enough; you need to convert it to <strong>power</strong> (the ability to exert force quickly). This is where <strong>plyometrics and power drills</strong> come in. Plyometric exercises (such as jumping bounds, box jumps, hurdle hops, and depth jumps) train the <strong>stretch-shortening cycle</strong> of muscles and tendons, improving running economy and speed. The goal for an 800m runner’s plyos is not maximum jump height per se, but to generate high force in a short ground contact time – essentially improving leg stiffness and explosiveness . For example, doing 3×10 bounding strides or hurdle hops can increase the power with which an athlete pushes off the track. Medicine ball throws and Olympic lift derivatives (like power cleans) can also build power. These exercises are typically done 1–2 times per week, often in the same sessions as or immediately following a speed workout, when the neuromuscular system is warmed up. Plyos and power moves are kept low in volume to avoid injury or soreness; a little goes a long way. Importantly, they are introduced gradually – often starting in general prep with basic drills and low jumps, and intensifying to more dynamic plyos in special prep. By the competition phase, plyos might be scaled back or maintained at low volume just to keep the bounce in the legs.</p><p><strong>Mobility and Mechanics.</strong> Running two laps fast also demands good mobility and technique. Flexibility in key areas (ankles, hips, hamstrings) allows the athlete to achieve optimal stride length and frequency. 800m runners use dynamic stretching, hurdle mobility drills, and foam rolling to maintain range of motion. Sprint drills (A-skips, B-skips, high-knee runs, etc.) are a mainstay in weekly routines to reinforce proper mechanics like high knee lift, upright posture, and active foot strike. These drills, often borrowed from sprinters, enhance <strong>coordination and stiffness</strong> – qualities that help an athlete <strong>maximize each stride</strong> . Coordination training might include footwork ladders or wicket runs to ingrain an efficient cadence. Middle-distance runners don’t need the exaggerated form of a 100m sprinter, but they do benefit from sprint-like efficiency: relaxed shoulders, a high hips position, quick ground contacts. Many modern coaches explicitly train such mechanics, as research has shown that neuromuscular&#x2F;mechanical improvements (like better <strong>vertical force application and shorter ground contact times</strong>) translate to faster 800m racing .</p><p><strong>Hill Sprints and Resistive Sprints.</strong> Hills are a favorite tool to build strength and power in a running-specific way. Short, steep <strong>hill sprints (6–10 seconds)</strong> develop explosive strength with minimal injury risk – the incline naturally enforces correct form and reduces impact . During base phases, coaches often include sets of very short hill sprints (e.g. 8×8-second dash up a steep hill, walking back) to recruit maximum muscle fibers and strengthen tendons . As the season progresses, slightly longer hills (15–20 seconds up a moderate incline) serve as a bridge between pure power and speed endurance . Hill sprints can be done year-round, often after an easy run or as part of a speed session. Similarly, some programs use resistive sprinting (sled pulls, weighted vests, or running into a strong headwind) to increase force output. The guiding idea is to overload the muscle recruitment slightly, then allow transfer to normal sprinting. These are used sparingly and carefully (usually early in a session and not to exhaustion).</p><p><strong>When to Do Strength Work.</strong> Scheduling strength and power training in the week is strategic. A common approach is to pair heavy lifting or plyos with a hard track session – for example, doing a weightlifting session later on the same day as interval training. This “combining stressors” approach means hard days are very hard, and easy days can be fully easy for recovery. For instance, an athlete might do a speed workout in the morning and a weight session that evening, then have the next day relatively light. Another method is to do strength training on the day before a pure speed session, sometimes using lifting as a sort of potentiation (though one must be careful not to induce fatigue that harms the speed session). Coaches avoid doing heavy lower-body lifts the day before a big track interval workout – a hard squat workout can leave the legs sore or fatigued, reducing track quality. The exact pattern varies: some do two lifting days (e.g. Monday and Friday) focusing on different aspects (max strength vs power), others maintain just one full-body strength session per week in-season for maintenance. <strong>Communication between the coach and strength trainer is crucial</strong> so that the weight work complements, rather than compromises, the running workouts. For example, one performance coach noted they eliminated heavy exercises that caused excessive muscle soreness so the athlete could still perform high-quality track sessions . During competition&#x2F;taper, strength moves to maintenance mode – lower weight or volume – just enough to keep the neuromuscular gains but without tiring the athlete. Mobility and core routines, however, remain throughout for injury prevention and efficiency.</p><p>In summary, a well-rounded 800m program addresses strength, power, and mobility as “supporting actors” to running workouts. A strong and explosive athlete can achieve <strong>“relaxed speed at fast race paces”</strong> – meaning they use less effort to maintain 1:48 pace, because each stride is powerful. This pays off in the final 200m of the race when form begins to break down; the stronger athlete can hold technique longer against the “bear on the back.” As a bonus, strength training and plyos also contribute to <strong>injury resilience</strong> – stronger muscles and stiffer tendons can handle the training loads of intense interval work. Many top 800m runners credit their weight room and hill work as the secret sauce that keeps them explosive and healthy through long seasons.</p><h2 id="Recovery-and-Load-Management"><a href="#Recovery-and-Load-Management" class="headerlink" title="Recovery and Load Management"></a><strong>Recovery and Load Management</strong></h2><p><strong>“Train Hard, Recover Harder.”</strong> In the 800m, the difference between breakthrough and breakdown often comes down to recovery. These athletes walk a fine line: workouts must be intense to elicit adaptation, but without proper recovery, the athlete can’t absorb the training. Key recovery components include <strong>sleep</strong>, <strong>nutrition</strong>, <strong>rest days&#x2F;easy days</strong>, and <strong>monitoring of fatigue</strong>.</p><p><strong>Sleep and Rest:</strong> Sleep is the foundation – aiming for around 8+ hours of quality sleep per night (and even more after very hard sessions or when accumulating fatigue). Many sub-elites with daytime jobs or school have to be very disciplined with bedtime to get this critical recovery. Some may also benefit from short naps on heavy training days. It’s during deep sleep that growth hormone is released, muscle repair happens, and the nervous system recharges – all vital after brutal 800m workouts. Coaches often prescribe at least <strong>one rest day per week</strong> (or at minimum, an active recovery day with just light cross-training or jogging) to allow for full recovery. During high-load training blocks, a common pattern is three weeks building followed by one “deload” week with significantly reduced volume or intensity. This lighter week (or a few consecutive recovery days) helps consolidate gains and prevents overtraining. For example, an athlete might drop from 50 miles in a week down to 30 miles in a recovery week, cutting out one hard session and focusing on low-intensity runs and extra stretching. It’s better to err on the side of slightly too much recovery than too little – a day off can save an injury, whereas pushing through mounting fatigue can lead to illness or a pulled muscle.</p><p><strong>Easy Day Intensity:</strong> “Easy means easy” is a mantra of successful middle-distance runners. On recovery run days, the intensity should be <strong>truly low</strong> – often <strong>slower than 50–60% of 800m race pace</strong> (which for a 1:48 runner might be 5:30&#x2F;km or slower jogging). Heart rate and RPE (rating of perceived exertion) should be in zone 1–2 (very comfortable; you could hold a conversation). This ensures the run is aiding recovery by increasing blood flow and loosening muscles, rather than adding more stress. Many athletes fall into the trap of doing “moderate” runs on what should be easy days, which can accumulate fatigue. Using a heart rate monitor or just strict self-discipline can help keep these runs honest. It’s often said that the difference between good and great athletes is not in how hard they go on hard days, but how easy they go on easy days – respecting that polarization. Incorporating techniques like <strong>active recovery</strong> (light cycling, swimming, or even brisk walking) can also help flush the legs without impact. The day after a race or lactic workout, for instance, the athlete might do 20–30 minutes of very gentle exercise just to promote circulation and reduce soreness.</p><p><strong>Spacing of Intense Sessions:</strong> High-intensity anaerobic sessions (like 800m race-pace intervals or all-out 300s) impose a heavy recovery cost – not just muscle soreness but also central nervous system fatigue and biochemical stress (depleted glycogen, accumulated hydrogen ions, etc.). Coaches therefore space these <strong>glycolytic workouts</strong> strategically. A classic rule is to allow at least 48–72 hours before the next big anaerobic session. For example, if an athlete does a hard lactic session on Tuesday, the earliest they might attempt another comparable session would be Friday or Saturday. In between, they might do low-intensity running or a different type of workout (e.g. pure speed or a light threshold run) that taps different systems. <strong>During the specific competition phase, recovery between hard sessions is even more emphasized</strong> – with more days of easy training required as workouts become more race-like . If workouts are not spaced enough, the athlete risks carrying fatigue into the next session and not hitting the intended paces, or worse, accumulating excessive stress that can lead to overtraining or injury. Listening to the body is key: if an athlete is still very sore or reports a high resting heart rate and poor sleep after 48 hours, the coach might postpone the next intense workout an extra day.</p><p><strong>Monitoring and Red Flags:</strong> Successful athletes use both <strong>subjective</strong> and <strong>objective</strong> metrics to gauge recovery. Subjectively, an athlete should note their morning mood and appetite, general soreness, and willingness to train. If motivation is dropping or irritability is rising, it may indicate fatigue. The <strong>Rating of Perceived Exertion (RPE)</strong> in workouts is also telling: if what should be a moderate effort feels like a maximal grind, the athlete could be under-recovered. Objectively, some use morning <strong>resting heart rate</strong> or <strong>heart rate variability (HRV)</strong> – an upward drift in resting HR over a few days, or a drop in HRV, can signal accumulating fatigue or an oncoming illness. Simplest of all, monitoring <strong>training performance</strong> itself is useful: e.g., if an athlete normally handles 6×400 at 60s with ease but is suddenly struggling to hit 62–63s and feels bad, it’s a sign that recovery is insufficient. Another check is to do a short <strong>“reactive” test</strong> – for instance, a few hops or a vertical jump in the warm-up – if the numbers are well below normal, the nervous system might be fatigued. Weight fluctuation can also hint at dehydration or stress (significant unexpected drops may indicate dehydration; increases might indicate inflammation).</p><p><strong>Recovery Modalities:</strong> Apart from rest and nutrition (covered next), athletes may use stretching, foam rolling, massage, or hydrotherapy (ice baths, contrast showers) to speed recovery. These modalities can help with muscle soreness and perceived recovery, though the science is mixed. If it helps the athlete mentally or physically, it can be included judiciously (e.g. foam rolling nightly, or a massage in a deload week). <strong>Injury prevention</strong> also ties in – doing prehab exercises (like core work, hip stability drills, ankle strengthening) on easy days can address minor weaknesses that, if left unchecked, could lead to injury under heavy training load.</p><p><strong>Early Warning Signs:</strong> A well-trained 800m runner becomes attuned to their body’s warning signs. Some red flags that demand an immediate adjustment (rest or lighter training) include: persistent elevated morning heart rate (e.g. 5–10 bpm above normal for several days), persistently high fatigue or poor sleep, loss of motivation or an unusually depressed mood, soreness that doesn’t improve after 48–72 hours, and any niggling pain that starts to worsen (indicating potential injury). Catching these signs early and responding (with a day off, extra sleep, or seeing a physio if needed) can prevent small issues from becoming season-ending problems. It’s far better to miss one workout than to push through and miss weeks later. Coaches encourage open communication – athletes should feel it’s okay to say “I’m feeling wiped today,” and the plan can be adjusted. An old saying goes, “It’s better to be 10% undertrained than 1% overtrained,” underscoring that slightly backing off when in doubt is usually the prudent choice.</p><p>In essence, effective 800m training is as much about recovery as it is about workouts. The training adaptations (speed, endurance, power) only manifest if the athlete recovers enough to rebuild stronger. With proper load management – good sleep, smart scheduling, easy-day discipline, and attention to body signals – a sub-elite 800m runner can string together months of consistent training. That consistency, more than any one killer workout, is what leads to breakthroughs.</p><h2 id="Nutrition-and-Body-Management"><a href="#Nutrition-and-Body-Management" class="headerlink" title="Nutrition and Body Management"></a><strong>Nutrition and Body Management</strong></h2><p><strong>Eating for Performance.</strong> Training hard for the 800m requires high energy availability and the right mix of macronutrients. A sub-elite 800m runner needs to fuel like both a sprinter and an endurance athlete. This means maintaining muscle mass and power with ample <strong>protein</strong>, while also stocking enough <strong>carbohydrate</strong> to support heavy training loads and fast repeats. General protein guidelines for middle-distance runners are about <strong>1.2 to 1.6 grams of protein per kilogram of bodyweight per day</strong> . For a 70 kg athlete, that’s ~84–112 g of protein daily. Spreading this protein across meals (e.g. ~20–25 g in breakfast, lunch, dinner, plus recovery snacks) optimizes absorption and muscle repair. High-quality sources like lean meats, fish, eggs, dairy, legumes, and tofu are staples. Protein is especially important right after workouts – consuming ~20 g protein in the 30-60 minutes post-exercise helps kickstart muscle repair and adaptation.</p><p><strong>Carbohydrate Periodization:</strong> Carbs are the primary fuel for hard running. An 800m runner should not shy away from carbohydrates; they drive the high-intensity efforts and aid recovery by replenishing glycogen. Many athletes follow a concept of <strong>carbohydrate periodization</strong> – aligning carb intake with training demands. On key workout days or days with multiple sessions, carb intake is kept high (e.g. extra grains, fruit, sports drinks) to ensure full glycogen stores. Before a high-intensity session, the athlete might have a carb-rich meal (such as oatmeal or pasta a few hours prior) and perhaps a small easily-digested snack 60–90 minutes before (like a banana or energy bar) for quick energy. After the session, <strong>refueling within the first hour</strong> with a mix of carbs and protein (e.g. chocolate milk, a recovery shake, or a sandwich) is crucial – muscles are hungriest then, and this speeds glycogen recovery for the next training . On lighter days or rest days, the athlete can reduce carbohydrate proportion slightly (focusing more on proteins, fats, and vegetables) to avoid excess calories, especially if weight management is a goal. This doesn’t mean “low-carb” – rather, adjusting portion sizes: maybe smaller servings of rice or bread on a rest day compared to big servings on workout days. This strategy supports training when needed but also can help optimize body composition. That said, any reduction must be done carefully – 800m training even on easy days still needs fuel for recovery. It’s a delicate balance to ensure energy availability remains high enough to prevent fatigue or hormonal issues.</p><p><strong>Hydration and Electrolytes:</strong> Middle-distance runners can’t neglect hydration, particularly in warm weather or during longer training sessions. Even moderate dehydration can impair performance and recovery. Athletes are advised to hydrate throughout the day (urine color should be light straw). During training, if it’s a longer run or a very sweaty day, drinking water or a sports drink is important. <strong>Electrolytes</strong>, especially sodium, should be replenished after heavy sweating. This can be through sports drinks, electrolyte tablets, or salty snacks. For an 800m athlete, a key time to think about hydration is the day before a race or hard session – coming in well-hydrated (not chugging last minute water) can improve the quality of performance. On double session days (morning and afternoon workouts), rehydrating after the first session is critical: that means water, electrolytes, and even a recovery drink to ensure the athlete isn’t starting the second session in a fluid deficit . In hot climates or altitude, fluid needs increase, so athletes might add an extra electrolyte drink or two per day. A good practical habit is to sip fluids with every meal and carry a water bottle throughout the day.</p><p><strong>Iron and Micronutrients:</strong> Middle-distance runners, especially women, are prone to <strong>iron deficiency</strong> due to iron loss from sweat, foot-strike hemolysis (breaking blood cells while running), and dietary shortfalls. Iron is crucial for oxygen transport (hemoglobin) and energy production. Even without anemia, low iron stores (low ferritin) can cause fatigue and performance decline . Sub-elite athletes should get blood work at least yearly (and if feeling unexplained fatigue) to check ferritin and hemoglobin. If ferritin is low (often considered &lt;30 µg&#x2F;L as a concern, &lt;15 µg&#x2F;L severe deficiency), iron supplementation under a doctor’s guidance might be needed. Day to day, consuming iron-rich foods – red meat, spinach, legumes – and pairing them with vitamin C (which aids iron absorption) is wise. Avoiding consuming iron sources at the same time as calcium or coffee&#x2F;tea (which inhibit absorption) can help. Other micronutrients of note: Vitamin D (for bone health and muscle function) if sun exposure is low; B-vitamins for energy metabolism (found in whole grains, meats, leafy greens); and antioxidants (vitamin C, E) from fruits and veggies to help immune function. However, mega-doses of antioxidants right after training might blunt adaptation, so it’s better to get them from natural foods spread out, not as a big supplement pill at training time.</p><p><strong>Fueling on Double-Session Days:</strong> Many sub-elites might run and lift, or run twice, in the same day. Proper fueling around these sessions ensures the second workout is productive. After the first session (say a morning track workout), the athlete should have a <strong>recovery meal</strong> as soon as possible – ideally within 30 minutes. A classic formula is about <strong>1.2 grams of carbohydrate per kg</strong> of bodyweight, plus <strong>~0.25 g&#x2F;kg of protein</strong>, in that recovery meal. For a 70 kg runner, that’s ~85 g carbs and ~18 g protein. This could look like a large fruit smoothie with whey protein, or chicken and rice, or a bagel with peanut butter and a sports drink – whatever sits well and provides those nutrients. This rapid refueling helps restock muscle glycogen and provides amino acids to repair muscle, so by the afternoon the athlete can train effectively again. Additionally, including salty foods or an electrolyte drink in this meal helps rehydrate from the first session. Between sessions, sticking to low-fiber, high-carb foods prevents GI upset and ensures quick digestion (e.g. pretzels, yogurt, fruit). If the break is short (&lt;4 hours), liquid nutrition might work best (like a recovery shake or chocolate milk) since it digests faster. Then, before the second session, a small top-up snack (e.g. a banana, energy bar, or some sports drink) can keep blood sugar up. Essentially, treat a double as one continuous fuel-demand – don’t wait until you’re starving or dehydrated.</p><p><strong>Body Composition Considerations:</strong> 800m runners generally benefit from being <strong>lean and muscular</strong> – carrying excess fat can slow you down, and a strong power-to-weight ratio is ideal. Elite male 800m runners often hover around ~8% body fat, and elite females around ~12% . However, these are not magic numbers for everyone; there’s a healthy range and it’s highly individual . Sub-elite athletes should pursue a body composition that is performance-optimal <em>for them</em>, not chase an arbitrary weight. Often, when training volume and intensity are high and diet is tuned for performance, body composition naturally improves (fat loss, muscle retention). If an athlete does aim to lose a few pounds of fat, it should be done gradually in the general prep phase – <em>not</em> during the competitive season – and always with enough fuel to support training. Rapid weight loss or overly restrictive diets can lead to <strong>RED-S (Relative Energy Deficiency in Sport)</strong>, impairing performance, recovery, and health (loss of menstrual cycle in women, low testosterone in men, frequent illness, etc.). Coaches encourage athletes to think of food as fuel: rather than “dieting,” focus on nutrient-dense foods and timing intake around workouts.</p><p>It’s also important to acknowledge genetics: some runners are naturally more muscular or have a bigger frame. An ex-sprinter 800m runner might weigh more (with more muscle mass in upper body) than an ex-5k runner of the same height. As long as the extra mass is functional (helps in producing speed) and not significantly hindering aerobic cost, it can be an advantage (think of Donavan Brazier’s powerful build vs. a lankier 1500m type). Conversely, an endurance-type shouldn’t force themselves into weightlifting to add bulk that might not suit them. <strong>Optimal race weight</strong> is a subtle balance – usually the weight at which the athlete feels strong and fast, not weak or hungry. Monitoring can be done by periodic weigh-ins or body fat measurements, but these are just data points. The mirror and performance indicators (how times are improving) often tell the story. As Hal Higdon famously said, there’s <em>no one perfect number</em> – aim for the zone where you personally perform best, and ensure you’re not sacrificing health for a lighter scale reading .</p><p><strong>Putting It Together:</strong> Practically, a day in the life of a well-fueled sub-elite might look like: a solid breakfast (e.g. oatmeal with fruit and nuts, plus eggs) to start the day before morning training; a recovery shake and banana right after the workout; a lunch with plenty of protein and carbs (say chicken, quinoa, vegetables, and yogurt); a snack like a granola bar or trail mix in afternoon; an evening easy run followed by dinner (salmon, sweet potatoes, salad, with some fruit for dessert); and perhaps a casein-rich snack before bed (like Greek yogurt or milk) to aid overnight muscle repair. Hydration is attended to at each meal and during training. Iron-rich foods are sprinkled in (spinach in the salad, red meat twice a week, etc.). By paying attention to these details, the athlete ensures they are giving themselves the best shot to maximize training adaptations and show up to each session with the energy to execute it. Just as a high-performance car needs quality fuel, an 800m runner needs a high-performance diet to hit those 1:48 goals.</p><h2 id="Progress-Tracking-and-Testing"><a href="#Progress-Tracking-and-Testing" class="headerlink" title="Progress Tracking and Testing"></a><strong>Progress Tracking and Testing</strong></h2><p><strong>Why Test?</strong> In the training mix described, it’s useful for a coach and athlete to assess whether the training is producing the desired adaptations. While lab tests (VO₂ max, lactate threshold measurements) are nice, sub-elite athletes can get plenty of information from simple <strong>field tests and key workout indicators</strong>. These help track improvement, inform pace targets, and guide adjustments to training emphasis. Here are several commonly used metrics and how a coach might use them:</p><ul><li><p><strong>Flying 30m or 60m Sprints:</strong> This test involves a running start into an all-out 30m sprint (often measured with timing gates or a stop-watch over a known distance). It gauges <strong>maximal velocity</strong>. An improving flying 30m time (for example, dropping from 3.3s to 3.1s in a 30m fly) indicates gains in pure speed. This is crucial for 800m because it relates to speed reserve. If an athlete can hit a higher top-end speed, then cruising at 90% of that speed (race pace) will feel easier. Coaches often test flying sprints every few weeks during general prep and special prep. If the 30m fly isn’t improving despite heavy speed training, it might signal the need for more strength&#x2F;power work or technique refinement. Conversely, once the athlete has achieved a solid max velocity, further focus might shift more to speed endurance (knowing that raw speed is no longer the limiting factor). A related test is a standing 60m or 150m acceleration to measure explosive power and acceleration mechanics.</p></li><li><p><strong>150m&#x2F;200m Repetition Times:</strong> By timing fast but not fully all-out 150m or 200m repeats in training, a coach can infer the athlete’s <strong>speed endurance</strong> and speed reserve. For instance, say an athlete can run a single 200m in ~23.5 seconds in practice and repeat 4×200m averaging 24.5 with full rest; this indicates excellent 400m speed, suggesting they are a speed-type 800m runner. Another athlete might only manage 26s for 200m, pointing to more endurance type. Such data can guide race modeling: the first athlete might aim for a faster opening 400m in an 800 race, whereas the second needs to be more conservative. Improvement in repeat 150s (e.g. being able to do 3×150m all under 18.5s when previously 19s was the best) shows that anaerobic capacity and mechanics are improving. Coaches sometimes use a “4×200m test”: run 4×200 with ~2 minutes rest, average time predicts 800m capability (this is a rough predictor some use: average 200m in 25s with short rest might correlate to a 1:50–1:52 800m). It’s not foolproof, but comparing this indicator over time can show progress.</p></li><li><p><strong>600m Time Trial:</strong> Perhaps the most classic specific test for 800m runners is an all-out or near all-out 600m timed effort. This is usually done a couple of weeks out from competition phase or as a rust-buster indicator. A common pattern: an athlete runs a 600m trial and from it, the coach extrapolates an 800m potential (e.g. adding ~7–10 seconds to the 600m time, though the exact add-on depends on the athlete’s profile). If an athlete clocks 1:18 in a 600m solo time trial, it suggests something around 1:48–1:50 is in reach in a race with competition. The 600m test has the benefit of being close enough to race demand to test lactate tolerance, but not so long as to completely exhaust the athlete (and mentally it’s easier to go all-out for 600 than 800 in training). Improvement in the 600m test over the season is a strong confidence booster. Coaches will also watch <em>how</em> the athlete runs it: are they smooth and relaxed through 400m and just tying up in the last 100m (which is expected), or are they rigging badly with 250m to go (which might indicate insufficient endurance)? This can guide the fine-tuning of workouts: e.g. if the athlete died early, more endurance work might be needed; if they cruised but lacked finishing power, perhaps more speed endurance sessions are in order.</p></li><li><p><strong>1000m Time Trial or Aerobic Time Trial:</strong> On the other end, a hard 1000m or even a mile time trial can gauge the athlete’s aerobic development relative to their speed. For example, a 1:48 800m runner might run around 2:22–2:25 for a full effort 1000m, or ~4:00–4:10 for 1500m (roughly). If an athlete with 1:48 speed is only managing 2:30 for 1000m in practice, it shows an aerobic shortfall – they may have great 400m speed but fade after 600m. Conversely, someone able to run 2:24 in practice but still only 1:54 in the 800 probably has the endurance but needs more speed or better racing tactics. So comparing one’s 600m and 1000m capabilities gives insight: a relatively better 1000m (or 1500m) implies an endurance-oriented runner, while a relatively better 400m&#x2F;600m implies a speed-oriented runner. This can validate whether the training needs rebalancing. Aerobic time trials can be done a bit more frequently since they aren’t as traumatic as an all-out lactic test; some athletes do a 4–5 minute all-out test (like a mile) every 6–8 weeks to see if their aerobic engine is improving (faster time or lower average HR for the same time).</p></li><li><p><strong>Steady-State and Tempo Indicators:</strong> Not all tests are all-out. A great measure of aerobic progress is the <strong>heart rate or lactate response at a given submaximal effort</strong>. For instance, an athlete might do a standard session like 3×10 minutes at a steady pace with a 2-minute jog recovery. They can track distance covered or average pace for those repeats over time – ideally, the same effort yields a faster pace as fitness improves, or the same pace yields a lower heart rate. Some use a <strong>heart rate drift test</strong>: run 30 minutes at a steady moderate effort (e.g. marathon pace or a set HR like 160 bpm) and see if heart rate drifts upwards significantly. As aerobic fitness improves, HR drift tends to lessen (heart rate stays more stable), indicating better endurance and efficiency. Similarly, in a threshold session (say 4×5 minutes at lactate threshold), if the athlete notices their pace in later reps is now higher than it was a month ago for the same perceived effort, that’s tangible progress. If lactate measuring is available (some competitive sub-elites or their coaches have handheld lactate meters), they might check that a given workload produces lower lactate than before (e.g. 4 mmol lactate now at 4:40&#x2F;mile pace whereas it used to be at 4:50&#x2F;mile, indicating an improved threshold). These submax indicators are less taxing than time trials and can be peppered into training regularly.</p></li><li><p><strong>Lactate Testing in Workouts:</strong> If resources allow, occasional <strong>lactate sampling</strong> during tough workouts can inform training. For example, measuring blood lactate after a set of 3×300 at 800m pace can tell how the athlete’s buffering capacity is – one athlete might show 12 mmol, another 18 mmol for the same workout. If an athlete consistently shows extremely high lactate and struggles to clear it, the coach might implement more aerobic capacity work or longer recovery between reps to safely train that system. Lactate tests can also pinpoint true threshold pace (if done in a controlled incremental run) which can calibrate training paces more precisely than relying on race times. However, many sub-elites may not have routine access to this, so they rely on perceived effort and performance data instead.</p></li><li><p><strong>Consistency and “Signature Workouts”:</strong> Many coaches have signature workouts that they repeat periodically under similar conditions, using them as benchmarks. For instance, a workout like 5×300 m with 3 minutes rest at 95% effort – how the athlete performs there in April vs. June can reveal improvements in speed endurance. If earlier they averaged 42 seconds and later they’re averaging 40 seconds for 300s, that’s a clear gain. Another example: 3×600 m with 10 min rest – if the athlete can hit, say, 1:24, 1:23, 1:22 by late season whereas those times were not possible earlier, it’s a strong confidence sign. Coaches also note recovery markers: maybe early in the season an athlete needed full 3 min rest to get their 300m times consistent, but later in season they can do the same times on 2 min rest – indicating better repeatability and endurance. Tracking these qualitative improvements (less rest needed, less rigging at the end of reps, lower RPE for the same session) is just as important as raw times.</p></li></ul><p><strong>Using Test Data to Adjust Training.</strong> The purpose of collecting these metrics isn’t just to pat oneself on the back; it’s to guide training decisions. For example:</p><ul><li><p>If flying sprints show stagnation, the coach might incorporate more sprint technical work, or ensure full recovery and perhaps taper the volume to allow the neuromuscular system to adapt. They might also look at the strength program to see if more plyo or power work is needed.</p></li><li><p>If the aerobic indicators (e.g. 1000m time or threshold paces) are lagging, it’s a cue to add or emphasize more aerobic training – perhaps extend the base phase, or include a weekly tempo run where previously it was bi-weekly.</p></li><li><p>If the athlete’s 600m time trial is strong but they consistently fade in the last 200 of an 800, they might practice more race-specific workouts (like 300m reps with short recovery, or negative-split 500s) to simulate that back-end fatigue. They may also work on race tactics (not going out too hard).</p></li><li><p>Conversely, if the athlete’s 600m time is underperforming relative to training, it might indicate they’re not fresh (maybe overtrained) or not used to the lactic strain – so the coach might slightly reduce training load or add a couple more race-pace workouts to improve confidence and physiological tolerance.</p></li><li><p>Regular testing also helps with <strong>goal-setting and pacing</strong>. If all signs point to a 1–2% improvement in fitness, the athlete can adjust race targets (e.g. aim for a 1:47.5 instead of 1:49) and pacing strategies (maybe target a 52.x first lap instead of 53.x).</p></li></ul><p><strong>Mental Aspect of Testing:</strong> It’s worth noting that these “tests” also have a psychological component. Hitting a personal best in a training 300m or seeing your flying 30m time improve can be a huge mental boost – a sign that the training is working. It adds to an athlete’s confidence lining up for races. On the flip side, if a test goes poorly, a good coach will contextualize it (maybe the athlete was tired that day, or weather was bad) and not let it shatter confidence. The data should be used constructively, not punitively. The coach might say, “Okay, your 1500m race last week suggests great aerobic fitness, but your 200m speed is a bit off – let’s add a couple of sessions of pure speed in the next block.” It becomes a feedback loop where training feeds testing and testing feeds back into training adjustments.</p><p>In summary, progress tracking for 800m runners uses a combination of <strong>short sprint tests (for speed), mid-distance trials (for speed endurance and specific endurance), and submaximal markers (for aerobic fitness)</strong>. By keeping an eye on these, coaches can ensure the athlete is advancing on all fronts and can catch any stagnation early. This data-driven approach, blended with coach’s intuition, helps fine-tune the program so that the athlete is peaking with all systems “go” come race day.</p><h2 id="Race-Modeling-and-Tactics"><a href="#Race-Modeling-and-Tactics" class="headerlink" title="Race Modeling and Tactics"></a><strong>Race Modeling and Tactics</strong></h2><p><strong>From Training to Racing.</strong> All the training elements – speed, endurance, strength – ultimately converge in the actual 800m race, which is as much a tactical chess match as it is a physical trial. Understanding race dynamics and practicing them is crucial for the 1:48 athlete. Modern research and race data have shown that the <strong>optimal 800m strategy is a controlled fast start followed by a positive split</strong> (first lap slightly faster than second) . Unlike a 1500m or 5k, trying to even-split or negative-split an 800 usually underperforms what the athlete is capable of – the event naturally involves deceleration, and the goal is to minimize slowing in the second lap rather than avoid it entirely .</p><p><strong>Opening 200m and 400m – Controlled Speed:</strong> The first 200m of the race is about <strong>getting up to race pace efficiently and securing good position</strong>. For a sub-elite race, this might mean going out in ~25–26 seconds for 200m if aiming for a ~54 first lap. This requires the athlete to use their speed but not red-line – essentially, accelerate quickly but <strong>relax into the pace</strong>. A common refrain is “push hard the first 50m, then float”: get out to establish position by 100m (when the break from lanes happens), then ease slightly to avoid going lactic too soon. Many athletes practice this in workouts: for example, doing 200m reps where they blast the first 50m, then consciously relax the next 150m at race rhythm. The idea is to simulate the adrenaline of the start but train the ability to dial in goal pace and not panic. Hitting the 400m mark in the right time is critical – if too slow, you’ve left time on the table; if too fast (beyond your ability), “the bear” will come very early. Generally, the first lap should be about 1.5–2.5 seconds faster than the second lap for an optimal performance (elite data shows ~1.8 s differential on average) . For instance, a 1:48 might be best as 53.0 + 55.0 rather than two 54s. Training sessions like <strong>“split 400s”</strong> (e.g. 200m fast, 200m relaxed in one rep) can help the athlete learn that feeling of a quick start with control.</p><p><strong>Positioning and Tactics in the Pack:</strong> In championship 800s, positioning is paramount. Sub-elite runners must practice running in a crowd, pace changes, and tactical positioning. Key tactical principles include:</p><ul><li><p><strong>Break smart at 100m:</strong> When the staggers break, know whether you want the lead or a specific spot. A 1:48 runner might not be the frontrunner in an elite race but should avoid getting boxed or stuck at the very back. Practicing a fast 100m and cut-in as part of training reps (say 3×600 where the first 100m is fast and you cut to lane one behind a rabbit) can mimic this.</p></li><li><p><strong>Energy saving:</strong> Running the 200m from 200m to 400m mark with relaxation is important. This is often where inexperienced runners waste energy by floating wide or surging unnecessarily. Athletes can practice pace change restraint – for example, 500m repeats where the first 200m is fast, the next 200m is slightly eased (but still quick), then a kick at the end. This trains the ability to settle mid-race without falling asleep.</p></li><li><p><strong>Third 200m – “the Bear”:</strong> The 400m to 600m segment is notoriously the hardest – lactate is flooding, and mental demons appear. This is where many races are won or lost, as some maintain pace better while others rig badly. Training the “bear phase” is crucial: workouts like 3×300 with short rest or a 600m time trial specifically push athletes into that discomfort zone so they learn to <strong>maintain form under fatigue</strong>. Coaches may instruct runners to focus on a key cue during this phase: “knees up” or “keep your hips tall” or “pump the arms.” By repeating a mantra or focusing on form, athletes can distract from the pain and minimize slowdown. Specific session examples include <strong>broken 800s</strong> (e.g. 500m at race pace, 30s rest, 300m fast) which force the athlete to run a second lap under extreme fatigue – very specific to handling the bear. Another is doing a 300m at 800 race pace, taking a 60s rest (to build some lactic), then immediately another 300m – the second one simulates the third 200m of a race when legs are heavy. The goal is to practice keeping relaxation and efficiency when the body is screaming. Over time, the athlete’s physiology and psychology adapt so that in an actual race, the third 200m, while still brutal, is familiar territory.</p></li><li><p><strong>Final 200m – Kicking and Competitiveness:</strong> The last straight is where competitive drive and specific training intersect. Even though the athlete is decelerating, so is everyone else – winning that phase is about decelerating <em>less</em> than your opponents  . Training wise, short fast reps at the end of workouts build this capacity (e.g. a workout of 4×150m fast, with the last rep done at the end of a longer session to simulate kicking on tired legs). Some programs like to have athletes do <strong>“burnouts”</strong> – e.g. an all-out 200m – at the very end of a training session to teach closing speed with nothing left in the tank. Mentally, athletes rehearse scenarios: if you’re in contention at 600m, how will you respond? Common tactics are deciding whether to swing wide in the final turn or sneak up the inside if a gap opens. In practice, doing relays or head-to-head reps with teammates can develop that racing edge – e.g. two runners practice the last 300m of an 800 side by side, each trying to edge the other out. This builds the habit of <strong>“competing”</strong> in the final stages rather than just running against the clock.</p></li></ul><p><strong>Even vs Positive Split – The Evidence:</strong> As mentioned, a slight positive split is physiologically optimal. Coaches discourage athletes from intentionally trying an even or negative split 800 in competition – it almost always means you didn’t use your full anaerobic capacity early enough. The reason behind this is the 800’s unique energy profile: going out fast (but not absurdly fast) takes advantage of your anaerobic reserves while you’re fresh, and the aerobic system is still catching up . If you go out too slow, by the time your aerobic system is fully contributing, you have unused anaerobic capacity that you can’t fully make up in the second lap. Analysis of world records and top performances shows that <strong>virtually all fast 800s involve a faster first lap</strong> . For example, David Rudisha’s world record: ~49.3 + 51.6 – about a 2.3-second positive split . Nearly all Olympic finals are won with a positive split. So, tactically, 800m runners train to go out hard enough to optimize the physiology, then focus on hanging on with minimal slowdown.</p><p>However, <strong>the trade-off</strong> is in how hard that first lap should be. There is a concept of a critical speed or “critical velocity” – essentially the highest pace you can go out without redlining too early . If an athlete sprints the first 200 far above their capacity, they will crash and burn (we’ve all seen the kid who goes out in 23 and comes back in 40+). The sweet spot might be going out at ~95% of one’s 400m speed for the first 200. For instance, if you can run a 400m in 50s, you might target ~26s for the first 200 in an 800 (which is 52s pace at 400m). If your 400m best is 54, you shouldn’t try to hit 26 for 200 – that’s 100% effort – you might target 27–28. In training, an athlete can gauge this by practicing 300m at goal 800m pace: it should feel hard but controlled, not an all-out 300m. If the athlete cannot complete a 300m in goal 800 pace without tying up, the goal might be too aggressive.</p><p><strong>Sessions That Tie to Tactics:</strong> Many workouts are designed with race tactics in mind. For example:</p><ul><li><p><strong>“Sit and kick” workout:</strong> A long fast rep followed by a very short rest and a quick add-on (e.g. 500m at strong pace, 10s rest, 100m sprint). This mimics sitting in the pack then unleashing a kick.</p></li><li><p><strong>Pacing change workouts:</strong> e.g. 3×400m where each 400 is done as 200m fast, 100m moderate, 100m fast – training the body to handle changing gears, much like surging or covering someone’s move mid-race.</p></li><li><p><strong>Lactate stacking workouts:</strong> where the rest is intentionally short so you start the next rep partially fatigued, teaching you to run on lactic legs. A classic is 2×300m with 30s rest – that second 300m feels like the third 200m of a race.</p></li><li><p><strong>Over-distance and under-distance races:</strong> Some coaches have 800m runners do a 400m race in training or competition to work on pure speed and the aggressive first lap feeling, and a 1000m or 1500m race to work on strength and pacing patience. These give the athlete perspective – a 400m race reminds them that even 800m pace is “comfortable” compared to an all-out 400, so they gain confidence to attack the first lap; a 1000m race teaches them not to go out too crazily because the bear will be even bigger.</p></li><li><p><strong>Mental rehearsal:</strong> This is not a session per se, but athletes are encouraged to visualize different race scenarios: leading from the front, sitting in second, boxed on the rail and needing to push out, a slow tactical first lap vs a hot pace, etc. Being mentally prepared helps tactical decision-making in the moment. Some practices end with “situational drills” – perhaps the coach has two athletes break at the 300m mark and jostle a bit to simulate physicality, or practice running the first 100m in lanes then cutting in smoothly.</p></li></ul><p><strong>Even Pace vs Optimal Pace in Practice:</strong> It’s interesting to note that in training, athletes often do repeats at even pace (because that’s how intervals are structured), but for the race, they must understand even pace is not the goal. For example, if goal 800m pace is 54 per lap, in training you might do 600s aiming for ~1:21 (which is even 54 pace through 400). But come race, you plan 52+56 or 53+55. The athlete needs to mentally and physically be ready to go out a bit faster than what they’ve often practiced. This is why some workouts specifically practice that fast-first-lap: such as 400m time trials, or doing the first 300 of a rep very fast then cruising – to simulate how the race will actually feel. Coaches will communicate this clearly: “We train with even splits for simplicity, but race day you will go out a little quicker – trust your training that you can handle it.” If an athlete has executed some fast-start workouts, they’ll trust that race plan.</p><p><strong>Mid-Race Position and Moves:</strong> The 800m is short, so tactical moves often happen quickly. A typical move is at ~500m – someone might bolt or the athlete decides to pass then. Practicing responding to moves (fartlek style workouts where the coach blows a whistle and the athlete must surge) can develop that skill. “Surges in race pace” is something noted as lacking in old training but emphasized now – training to respond to changes in pace. That could mean occasionally doing an interval session where not every rep is the same: e.g. 400m in 57, 400m in 55, 400m in 57 – so the athlete learns to slow and re-accelerate.</p><p><strong>Final Straight Strategy:</strong> Coaches often tell athletes to <strong>“keep form and run through the tape”</strong> – essentially, maintain your form and pump arms all the way, as many races are decided by tenths in the last 10m when someone ties up and another doesn’t. Specific training for this might be running 120m at the end of a workout at race closing speed, focusing on relaxation. The last 15m of an 800 can feel like slow motion; athletes who have practiced relaxation techniques (deep quick breath, stay upright, knees up) can eek out a bit more. Also, knowing that everyone is in pain is oddly comforting – a well-trained racer will think “I’ve been here in training, I know I can tolerate a bit more.” This mental callus is as important as the physical one.</p><p><strong>Race Analysis Feedback:</strong> After races, good coaches will review with the athlete: Did you hit the intended split? How did the third 200m feel? Did you get boxed or have to run extra distance in lane 2? This feedback loops into training. If an athlete consistently struggles with the first 200 (e.g. they get buried in the pack), they might do more block start work or even some 300m races to improve their opening speed and confidence. If they fade badly, maybe more over-distance work or pacing adjustments next time. Race modeling is iterative: each race teaches something to apply in training, and each training aims to fix a flaw from the last race.</p><p>In conclusion, <strong>training for tactics</strong> is an integral part of 800m preparation. Sessions develop the qualities needed for optimal pacing (speed reserve for the fast start , strength for the third 200m, kick speed for the finish) and sometimes literally mimic race scenarios. The athlete goes into races with a clear race plan (often: aggressive controlled start, position by 200m, relax down the backstretch, attack at 600m, then give it all) and contingency plans (if boxed, if the pace is slow or super fast, etc.). By the time the goal meet arrives, a sub-elite 800m runner should feel like they’ve “raced” that race many times already – in their mind and in pieces at practice – so come the real day, it’s almost second nature to execute, adjusting on the fly as needed. That, combined with the fitness built over months, is the recipe for personal bests.</p><h2 id="Individualization-and-Constraints"><a href="#Individualization-and-Constraints" class="headerlink" title="Individualization and Constraints"></a><strong>Individualization and Constraints</strong></h2><p><strong>One Size Does Not Fit All.</strong> Perhaps the most important principle in modern training is individualization – tailoring the program to the athlete’s unique characteristics, circumstances, and constraints. We’ve touched on the spectrum from 400&#x2F;800 “speed monsters” to 800&#x2F;1500 “endurance animals.” In practice, two athletes aiming for 1:48 might have training that looks quite different in balance, even if the core principles remain. A wise coach observes how the athlete responds to training and adjusts. For example, a speed-first athlete might thrive on two high-intensity track sessions per week and find that a third one causes diminishing returns or injury; the rest of their week might be filled with lower-intensity cross-training rather than more running. An endurance-first athlete might handle more total volume and even two quality sessions back-to-back (e.g. a tempo run one day, 300m repeats the next) because their recovery rate from aerobic work is high – but if they did heavy sprint work too often, they might strain a hamstring. <strong>Knowing the type</strong> guides these decisions. Coaches must avoid the trap of giving every athlete the same regime – a program heavily biased to endurance will improve the endurance-type runners but might burn out the speed-types, and vice versa . As noted earlier: <em>“The coach should be pragmatic, not dogmatic”</em> .</p><p><strong>Profiles and Emphasis:</strong></p><ul><li><p><strong>Speed-Oriented Athlete (“Sprinter in Disguise”):</strong> Likely has a fast 400m PB (e.g. 47–48s) but comparatively weaker 1500m. This athlete should continue to leverage their speed – keeping plyometrics, short sprints, and speed endurance as staples. They might do slightly lower weekly mileage (perhaps 30–40 miles&#x2F;week) and avoid overly long grind sessions that could cause injury or fatigue without proportional benefit. For instance, this athlete might replace a 6-mile continuous run with 3 miles + strides, or a long threshold run with split tempos (to get aerobic stimulus without the staleness). Their program might favor two days of speed&#x2F;power for each one day of threshold work. They still need aerobic training, but in measured doses – e.g. maybe 20-minute tempos instead of 40 minutes, or 4×1000m instead of 8×1000m. They benefit from <strong>longer recoveries between reps</strong> in anaerobic sessions to truly maximize pace and quality (since speed is their ace, we want those reps really fast). They also may do more cross-training to supplement aerobic fitness (like cycling or pool) if they struggle with high run volume. For a speedster, <strong>the minimum effective dose of endurance training</strong> might be just enough threshold running to not die in the last 100m, but not so much that it hampers their explosiveness. They might also lift heavier in the weight room (because they respond well to strength) and do well with a 7-day microcycle (more rest days in between hard sessions).</p></li><li><p><strong>Endurance-Oriented Athlete (“Distance type with wheels”):</strong> Has a strong 1500m or cross-country background (maybe a 3:45–3:50 1500m) but a relatively pedestrian 400m (~51–52s). This athlete thrives on aerobic work and might carry a higher mileage (50–70 miles&#x2F;week). They can handle longer workouts like 5k pace intervals, long tempos, even occasional 90-minute long runs, which for a pure speed type would be too much. However, their weakness is raw speed and anaerobic power. For them, <strong>the minimum effective dose of speed training</strong> must be respected: they might need to do strides or short sprints three times a week to keep improving form and recruit fast-twitch fibers. They may need more frequent sprint drilling (because it doesn’t come naturally) and perhaps train with sprinters occasionally to learn to relax at high speeds. In their case, too few speed sessions and they risk getting “stale” (great endurance but no ability to kick). They might incorporate short hill sprints year-round for power, and do more fast 200s in practice to drag their 400m speed down. In the weight room, they might emphasize explosive lifts and plyos over heavy slow lifts (to target rate of force development which they lack). They may also benefit from extending some speed workouts with slightly shorter rests to blend in a bit of endurance (since they can recover faster between reps). Essentially, for an endurance-type, it’s about bolting on enough speed work to complement their natural strengths, even if that means taking slightly away from excessive endurance training (because they already have that).</p></li><li><p><strong>“Pure” 800m (Hybrid):</strong> Falls in between – decent 400 and 1500, but 800 is clearly the best. This athlete likely needs a truly blended approach: moderate mileage (40–50 mpw), equal attention to threshold and speed endurance, etc. They might alternate blocks of training that skew one way or the other to tease out both qualities. The pure type can usually handle a bit more variety in a week (because they aren’t extremely biased one way). The key for them is sequencing: ensuring that neither their speed nor endurance gets neglected at any point for too long.</p></li></ul><p><strong>Age and Experience:</strong> Age is another factor. <strong>Young athletes</strong> (teens, early 20s) often recover faster and can handle relatively intense workloads, but might be more injury-prone if overloaded because their bodies are still adapting. They also might not have the aerobic base yet, so a coach might prioritize base-building in younger athletes (to develop that aerobic capacity early) while drip-feeding speed work to maintain their natural quickness. <strong>Older athletes (Masters or late-career)</strong> face different issues: recovery is slower, injuries niggle more, and speed declines with age faster than endurance. Thus, masters 800m runners often emphasize quality over quantity – perhaps only two hard sessions a week with more rest, and a lot of preventative exercises (mobility, pre-run warm-ups) to avoid pulls. They may also do more <strong>cross-training</strong> to reduce impact (e.g. replacing some runs with elliptical or biking). Masters need to keep touching speed year-round to minimize its decline – short sprints also help maintain muscle mass. But they must be cautious with plyos and very intense lactic sessions, as these can lead to Achilles or calf injuries in masters if not introduced carefully. An experienced older runner might rely more on strength and wisdom in races (tactics, efficient form) to compensate for any lost raw speed.</p><p>Training age (experience) matters too. A novice 800m runner (even if 25 years old but new to structured training) should start with simpler patterns – maybe 2 workouts a week – and gradually layer in more complexity (like doubles, gym sessions) as they adapt. A highly experienced runner might thrive on a complex plan with multiple stimuli, whereas a new runner could burn out on the same plan. <strong>Minimum effective dose</strong> is a useful concept: find the least number of hard sessions and miles that still produce improvement, then only increase if needed. If an athlete is improving on 35 miles a week and 2 quality sessions, there’s no need to do 50 miles and 3 quality “just because” – doing more could risk injury for marginal gain.</p><p><strong>Injury History Adaptations:</strong> Athletes with known weak links adapt their training to work around them. For example, someone with recurring shin splints or a stress fracture history may incorporate regular <strong>low-impact cardio</strong> (swimming, aqua jogging, cycling) to replace some runs, especially in base phase, to build aerobic fitness with less pounding. They might also choose softer surfaces for easy runs (grass, trails) and be strict about shoes and orthotics if needed. Another example: a runner with hamstring issues will be careful with maximum velocity sprinting – they might do more hill sprints (which are easier on hamstrings) and stick to 90% effort fast runs until they’ve built enough strength. Or they may need extra hamstring eccentric strengthening (Nordic curls, etc.) in their plan. If an athlete has a <strong>tendonopathy</strong> (say Achilles), explosive plyometrics might be limited or swapped for more controlled strength until it’s resolved. The training plan should have <em>built-in modifications</em>: e.g., an alternate pool workout on standby if a niggle flares, or a technique to still get stimulus without aggravation (maybe using a bike for interval training if running is off-limits short-term).</p><p><strong>Lifestyle and Time Constraints:</strong> Sub-elite athletes often juggle work or studies, which affects training time and recovery. If an athlete can only train once a day due to work, the coach might design slightly longer single sessions or emphasize quality to make up for the lack of doubles. For instance, instead of a double (morning tempo, evening shakeout), they might combine into one slightly extended tempo run. If someone has only 5 days a week to train because of family commitments, they might do 2 harder days, 2 easy days, 1 medium long run, and take 2 full rest days. It’s possible to run 1:48 off 5 days&#x2F;week if the training is smartly focused. In such cases, <strong>quality trumps quantity</strong>: every session must have a clear purpose. Weights might be done on an easy day lunch break if track work consumes other days. Additionally, life stress counts – a coach will consider if an athlete is under high work stress or poor sleep due to a newborn at home; the training load might need to be dialed back because the body perceives all stress cumulatively.</p><p>One excellent example is the case of an athlete in medical school described by a coach: she could only meet her track coach 3 times per week for hard sessions, and did 1–2 gym sessions in her limited free time . Despite lower frequency, those sessions were highly targeted, and she achieved high performance. This underlines that <strong>consistency and specificity</strong> matter more than sheer volume when time is limited. If you only have, say, 8 hours a week to train, you allocate them to cover the crucial elements (some speed, some aerobic, some strength) and prune the less essential (maybe very long runs or excessive supplementary stuff).</p><p><strong>Scaling Volume and Intensity Safely:</strong> Progression in an individual’s program should be gradual. A good heuristic is to change only one variable at a time (either volume or intensity, not both at once) and to build in recovery weeks. For example, if increasing mileage, keep the intensity stable or reduced; if adding a new type of intense workout, maybe hold mileage constant. Additionally, <strong>listening to the body</strong> remains paramount. The program might say 8x400m, but if the athlete is struggling by rep 5 (and it’s not a mental issue but genuine physical limit), the coach might cap it at 6 that day. That’s not a failure; it’s intelligent adjustment. Over time, that athlete will likely reach 8 reps when ready.</p><p><strong>Minimum Effective Dose:</strong> Each athlete has an optimal training “dose” – enough to improve but not so much that it causes burnout. For some speed-type runners, 25 miles&#x2F;week and two tough sessions might actually yield personal bests – adding more miles might just make them flat. For an endurance-type, 25 miles&#x2F;week might be insufficient stimulus and they need 45–50 to really see gains. By keeping good training logs and noting performance against training load, a coach can hone in on this dose. If an athlete stagnates, a change is needed (not always more – sometimes different or even less). If they’re improving, continuity is key – don’t drastically overhaul what’s working.</p><p>In summary, individualization means the training framework we’ve outlined is a template that gets molded for each athlete. The coach considers: athlete type, age, injury history, schedule constraints, and even psychological makeup (some athletes thrive on variety, others on routine; some need confidence from higher mileage, others get confidence from feeling fresh and snappy). The best programs often involve a dialogue between athlete and coach – adjusting on the fly when needed. By scaling volume and intensity to what the athlete can handle, providing alternatives when constraints arise, and focusing on <em>their</em> needs rather than a generic ideal, sub-elite athletes can maximize their potential. After all, the ultimate goal is for the athlete to arrive on the start line healthy, excited, and prepared – and that often means doing <em>your</em> optimal training, not someone else’s. As Renato Canova puts it, training is about <strong>adding</strong> specific elements on top of a maintained base, not replacing what works – so each athlete ends up with a bespoke blend that addresses all the core areas in a way that fits their puzzle.</p><hr><p><strong>Sources:</strong></p><ul><li><p>Rinaldi, J. – ASR training framework separating 400&#x2F;800 vs 800&#x2F;1500 profiles   . Emphasis on year-round speed development  .</p></li><li><p>“The Long Sprint – Reclassifying the 800m,” Mike Cox (USTFCCCA) – Research on energy system contributions and pacing strategy for 800m  . Importance of balanced training (aerobic &amp; anaerobic) .</p></li><li><p>800mTraining blog – Multi-pace training philosophy and periodization outline   . Differentiating training for 400&#x2F;800 vs 800&#x2F;1500 types  . Guidance on strength and plyometrics for 800m  .</p></li><li><p>SpeedEndurance articles – Multi-pace training (400m, 800m, 1500m speeds each week) and need for speed reserve . Example weekly components for 800m training .</p></li><li><p>SimpliFaster (X. Roy) – Notes on 800m being ~55-65% aerobic and the integration of strength&#x2F;track work  .</p></li></ul>]]></content>
    
    
      
      
        
        
    <summary type="html"></summary>
        
      
    
    
    
    
    <category term="athletics" scheme="http://loreley.one/tags/athletics/"/>
    
  </entry>
  
  <entry>
    <title>WASM</title>
    <link href="http://loreley.one/2024-12-wasm/"/>
    <id>http://loreley.one/2024-12-wasm/</id>
    <published>2024-12-06T23:00:00.000Z</published>
    <updated>2025-11-05T00:10:42.689Z</updated>
    
    <content type="html"><![CDATA[ <style>     /* Default: hide the iframe and show the message for mobile portrait mode */     #robot-frame {         display: none;     }     #mobile-message {         display: none;         font-size: 1.5em;         text-align: center;         color: red;         font-weight: bold;         margin-top: 50px;     }     /* Show the iframe on landscape mode */     @media screen and (orientation: landscape) and (max-width: 768px) {         #robot-frame {             display: block;         }         #mobile-message {             display: none;         }     }     /* Show the iframe for desktops */     @media screen and (min-width: 769px) {         #robot-frame {             display: block;         }         #mobile-message {             display: none;         }     } </style>  <!-- The iframe --> <iframe id="robot-frame" src="robot.html" style="width: 100%; height: 850px; border: none;"></iframe> <!-- Message for mobile portrait mode --> <div id="mobile-message">     The game cannot load in portrait mode. <br>      Please turn your phone sideways or visit on a desktop device. </div><script>    // JavaScript to handle dynamic behavior    function handleOrientationChange() {        const iframe = document.getElementById('robot-frame');        const message = document.getElementById('mobile-message');        if (window.innerWidth < 769) {            if (window.matchMedia("(orientation: landscape)").matches) {                iframe.style.display = "block";                message.style.display = "none";            } else {                iframe.style.display = "none";                message.style.display = "block";            }        } else {            iframe.style.display = "block"; // Desktop always shows            message.style.display = "none";        }    }    // Initial check    handleOrientationChange();    // Listen for orientation changes    window.addEventListener('resize', handleOrientationChange);</script><p><em>Note: This puzzle is based on a <a href="https://loreley.one/2023-07-cpu/">previous blog post </a> of mine. I’ve reprinted it below (with slight modifications) to provide the context of how this CPU works and why I made it. The functionality of the CPU is derived entirely from simulated logic gates. All the operations and control flow are based on the underlying properties of the logic gates, and changing their operation leads to corresponding changes in function.</em><br><em>I’ve been playing around with WASM (Web Assembly) and decided to resuscitate my old python code from that blogpost in order to run it in the browser. The entire python interpreter is bundled with the game code using WASM in the browser. I thought this was pretty cool.</em><br><em>Thanks to Ivan for reviewing an earlier version of this post.</em></p><button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#tutorial" aria-expanded="false" aria-controls="collapse">Tutorial</button><button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#old-cpu-post" aria-expanded="false" aria-controls="collapse">Old Blog Post</button><div class="collapse" id="tutorial"><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>This CPU features six general-purpose registers (<code>reg0</code> to <code>reg5</code>), along with a special register (<code>reg6</code>) used for both input and output. It supports four main types of operations:</p><ul><li><strong>Immediate Values</strong></li><li><strong>Arithmetic Operations</strong></li><li><strong>Copying</strong></li><li><strong>Comparisons and Control Flow</strong></li></ul><p>All values in the CPU are 8-bit and overflow after 255 to 0.<br>For comparisons, values are treated as <strong>signed</strong> (range -128 to 127).</p><hr><h3 id="Immediate-Values"><a href="#Immediate-Values" class="headerlink" title="Immediate Values"></a>Immediate Values</h3><p>Load a constant value into <code>reg0</code>. Only values up to 63 are allowed because the two most significant bits (MSB) are reserved for instruction encoding.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">0       # Load 0 into reg0 (min allowed immediate)</span><br><span class="line">36      # Load 36 into reg0</span><br><span class="line">63      # Load 63 into reg0 (max allowed immediate)</span><br></pre></td></tr></table></figure><hr><h3 id="Copying-Data"><a href="#Copying-Data" class="headerlink" title="Copying Data"></a>Copying Data</h3><p>The <code>copy</code> instruction copies data between registers or between a register and the input&#x2F;output.<br><strong>Input and Output</strong>: <code>reg6</code> serves as both the input and output register.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">copy &lt;source_register&gt; &lt;destination_register&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">copy 6 1   # Copy value from input (reg6) to reg1</span><br><span class="line">copy 3 6   # Copy value from reg3 to output (reg6)</span><br><span class="line">copy 5 3   # Copy value from reg5 into reg3</span><br><span class="line">copy 6 6   # Copy from input directly to output</span><br><span class="line">copy 1 1   # NOP</span><br></pre></td></tr></table></figure><hr><h3 id="Arithmetic-and-Logical-Operations"><a href="#Arithmetic-and-Logical-Operations" class="headerlink" title="Arithmetic and Logical Operations"></a>Arithmetic and Logical Operations</h3><p>Arithmetic and logical operations use <code>reg1</code> and <code>reg2</code> as operands and store the result in <code>reg3</code>.<br>Arithmetic uses 2’s complliment. Values overflow if they exceed 255. </p><p><strong>Addition</strong> (<code>add</code>): Adds <code>reg1</code> and <code>reg2</code>.<br><strong>Subtraction</strong> (<code>sub</code>): Subtracts <code>reg2</code> from <code>reg1</code>.<br><strong>Bitwise AND</strong> (<code>and</code>): Performs a bitwise AND operation on <code>reg1</code> and <code>reg2</code>.<br><strong>Bitwise OR</strong> (<code>or</code>): Performs a bitwise OR operation on <code>reg1</code> and <code>reg2</code>.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1        # Load 1 into reg0</span><br><span class="line">copy 0 1 # Move 1 into reg1</span><br><span class="line">2        # Load 2 into reg0</span><br><span class="line">copy 0 2 # Move 2 into reg2</span><br><span class="line">add      # reg3 = reg1 + reg2 = 1 + 2 = 3</span><br><span class="line">copy 3 6 # Output result (3)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">32</span><br><span class="line">copy 0 1</span><br><span class="line">32</span><br><span class="line">copy 0 2</span><br><span class="line">add  # 64 in reg 3</span><br><span class="line">copy 3 2</span><br><span class="line">copy 3 1</span><br><span class="line">add # 128 in reg 3</span><br><span class="line">copy 3 1</span><br><span class="line">copy 3 2</span><br><span class="line">add </span><br><span class="line">copy 3 6</span><br><span class="line"># total 256, so output is 0 == [0,0,0,0,0,0,0,0]</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">0</span><br><span class="line">copy 0 1</span><br><span class="line">2</span><br><span class="line">copy 0 2</span><br><span class="line">sub  # 0 - 2 = -2</span><br><span class="line">copy 3 6</span><br><span class="line"># cpu output is -2 == [1,1,1,1,1,1,1,0]</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">32</span><br><span class="line">copy 0 1</span><br><span class="line">32</span><br><span class="line">copy 0 2</span><br><span class="line">add  # 64 in reg 3</span><br><span class="line">copy 3 2</span><br><span class="line">copy 3 1</span><br><span class="line">add # 128 in reg 3</span><br><span class="line">1</span><br><span class="line">copy 0 2</span><br><span class="line">copy 3 1</span><br><span class="line">sub     # 128 - 1</span><br><span class="line">copy 3 6</span><br><span class="line"># result is 127 == [0,1,1,1,1,1,1,1] in 2&#x27;s compliment</span><br></pre></td></tr></table></figure><hr><h3 id="Comparisons-and-Control-Flow"><a href="#Comparisons-and-Control-Flow" class="headerlink" title="Comparisons and Control Flow"></a>Comparisons and Control Flow</h3><p>The <code>eval</code> instruction compares the <strong>signed</strong> value in <code>reg3</code> against <code>0</code>. If the condition is true, the program counter jumps to the address in <code>reg0</code>.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">eval &lt;condition&gt;</span><br></pre></td></tr></table></figure><p>Supported conditions:</p><ul><li><code>eval always</code>: Always jump.</li><li><code>eval never</code>: Never jump.</li><li><code>eval =</code>: Jump if <code>reg3 == 0</code>.</li><li><code>eval !=</code>: Jump if <code>reg3 != 0</code>.</li><li><code>eval &lt;</code>: Jump if <code>reg3 &lt; 0</code> (signed).</li><li><code>eval &lt;=</code>: Jump if <code>reg3 &lt;= 0</code> (signed).</li><li><code>eval &gt;</code>: Jump if <code>reg3 &gt; 0</code> (signed).</li><li><code>eval &gt;=</code>: Jump if <code>reg3 &gt;= 0</code> (signed).</li></ul><hr><h3 id="Labels"><a href="#Labels" class="headerlink" title="Labels"></a>Labels</h3><p>Labels act as named locations in the program. Defining a label associates it with its position in the program. Calling labels is just shorthand for using immediate values (and thus labels can’t be placed after line 63).</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">label &lt;name&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">label start # start = 0</span><br><span class="line">copy 6 1    # Read input into reg1</span><br><span class="line">add         # Add reg1 and reg2</span><br><span class="line">start       # Label used here as jump target, equvalent to using immediate 0</span><br><span class="line">eval &gt;=     # Jump to &quot;start&quot; if reg3 &gt;= 0</span><br></pre></td></tr></table></figure><p>This program copies <code>1</code> into <code>reg1</code>, adds it with <code>reg2</code>, and stores the result in <code>reg2</code> in a loop until the result overflows and becomes negative.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1           # Load 1 into reg0</span><br><span class="line">copy 0 1    # Copy 1 into reg1</span><br><span class="line">label loop</span><br><span class="line">add         # reg3 = reg1 + reg2</span><br><span class="line">copy 3 2    # Store result in reg2</span><br><span class="line">copy 3 6    # print result to output</span><br><span class="line">loop</span><br><span class="line">eval &gt;=     # Jump to label loop if reg3 &gt;= 0</span><br></pre></td></tr></table></figure><p><strong>For comparisons, the value is treated as signed (<code>127</code> is interpreted as <code>127</code>, but <code>128</code> is interpreted as <code>-128</code>). The last result the cpu prints is thus <code>128</code></strong></p><h3 id="Robot-Instructions"><a href="#Robot-Instructions" class="headerlink" title="Robot Instructions"></a>Robot Instructions</h3><p>output (controls):<br><code>1 == turn left</code><br><code>2 == turn right</code><br><code>3 == step forward</code></p><p>input:<br><code>1 == wall</code><br><code>0 == clear</code></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># this will rotate you left</span><br><span class="line">1</span><br><span class="line">copy 0 6</span><br><span class="line"></span><br><span class="line"># rotate right 3 times</span><br><span class="line">2</span><br><span class="line">copy 0 6</span><br><span class="line">copy 0 6</span><br><span class="line">copy 0 6</span><br><span class="line"></span><br><span class="line"># walk in a line until blocked</span><br><span class="line">label start_loop</span><br><span class="line">3</span><br><span class="line">copy 0 6</span><br><span class="line">copy 6 3    # put the input in reg3</span><br><span class="line">start_loop</span><br><span class="line">eval =      # if reg3 is 0 there is no wall</span><br></pre></td></tr></table></figure><p>glhf</p><button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#tutorial" aria-expanded="false" aria-controls="collapse">Hide Tutorial</button></div><div id="old-cpu-post" class="collapse"><h2 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h2><p>A while back, I came across an interesting game on Steam called <a href="https://turingcomplete.game/">Turing Complete</a>. The objective of the game is to design a Turing complete CPU from scratch, and it guides players through each step of the process. Starting with the basics of logic gates, the building blocks of a CPU, players tackle progressively more complex challenges until they reach the point of designing their own CPU capable of performing simple tasks. I found the game captivating and spent an unhealthy amount of time in it.</p><center><img src="turing_complete.jpg" alt="turing_complete"><p><small>Turing Complete gameplay</small></p></center><p>After completing the game, I decided it would be fun to create a Python emulator for a similar CPU. While it’s relatively easy to make a basic emulator in Python due to its high-level nature, I chose a different approach. I wanted to build the CPU entirely from scratch, deriving the logic from the fundamental functionality of logic gates. This means that the CPU’s performance is determined solely by simple true&#x2F;false comparisons, without relying on if statements or other control flows. It does make the code a bit harder to read at times, but by merely changing the function of a single logic gate, you can witness how the entire CPU’s behavior is affected (spoiler alert: it stops working).</p><h2 id="Logic-Gates"><a href="#Logic-Gates" class="headerlink" title="Logic Gates"></a>Logic Gates</h2><p>The design starts with the logic gates. Here is an excerpt of the code</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">and_</span>(<span class="params">*args:<span class="built_in">bool</span></span>)-&gt;<span class="built_in">int</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">int</span>(<span class="built_in">all</span>(args))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">or_</span>(<span class="params">*args:<span class="built_in">bool</span></span>)-&gt;<span class="built_in">int</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">int</span>(<span class="built_in">any</span>(args))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">not_</span>(<span class="params">*args:<span class="built_in">bool</span></span>)-&gt;<span class="built_in">int</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">int</span>(<span class="keyword">not</span> <span class="built_in">all</span>(args))</span><br></pre></td></tr></table></figure><p>The logic gates perform simple comparisons and return true or false values. For example the AND gate returns true only if both of its inputs are true. The OR gate returns true if either (or both) of its inputs are true.</p><center><img src="logic_gates.jpg" alt="logic gates"><p><small>logic gate symbols</small></p></center><h2 id="Basic-Components"><a href="#Basic-Components" class="headerlink" title="Basic Components"></a>Basic Components</h2><p>Once we have our logic gates, we can utilize them to create essential components. Let’s focus on the full adder as an example. The purpose of a full adder is to take two binary inputs and perform addition on them. For instance, when adding 0 + 0, the output is 0. When adding 1 + 0, the output is 1.</p><p>However, adding 1 + 1 poses a challenge. In binary, we lack a way to represent the number 2 directly. To address this, we introduce a carry bit, similar to carrying over a digit when adding two decimal numbers that exceed 9. Additionally, we have a carry input to account for any carry bits from previous additions. </p><center><img src="full_adder.png" alt="full adder"><p><small>full adder design</small></p></center><p>A truth table is a structured table that presents the outputs corresponding to all possible input combinations. It allows us to systematically analyze the behavior of a component or system under consideration. In the case of a full adder, the truth table would display the sum output and carry output for each potential combination of binary inputs and carry input. By checking the truth table, we can see how the full adder operates under different input scenarios.</p><center><img src="full_adder_truth.png" alt="full adder truth table"><p><small>full adder truth table</small></p></center><p>The above design can now be implemented in code, using the logic gates we have created previously. In this case I first made a half adder (which doesn’t have a carry in) and used two of them to construct a full adder. Notice how all the logic is dependent on the operation of the logic gates.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">half_adder</span>(<span class="params">input1:<span class="built_in">bool</span>, input2:<span class="built_in">bool</span></span>):</span><br><span class="line">    <span class="keyword">return</span> sn(</span><br><span class="line">        <span class="built_in">sum</span>=xor(input1, input2),</span><br><span class="line">        carry=and_(input1, input2)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">full_adder</span>(<span class="params">input1:<span class="built_in">bool</span>, input2:<span class="built_in">bool</span>, carry_in:<span class="built_in">bool</span></span>):</span><br><span class="line">    half_adder1 = half_adder(input1, input2)</span><br><span class="line">    half_adder2 = half_adder(half_adder1.<span class="built_in">sum</span>, carry_in)</span><br><span class="line">    out = sn(</span><br><span class="line">        <span class="built_in">sum</span>=half_adder2.<span class="built_in">sum</span>,</span><br><span class="line">        carry=or_(half_adder1.carry, half_adder2.carry)</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><p>After creating a number of basic components such as these, we create more complex components such as an ALU (Arithmetic Logic Unit). The task of the ALU is to perform simple mathematical operations, such as adding 2 numbers together. In this case the ALU can only perform 4 simple operations; Addition, Subtraction, and two logical operations AND and OR. I took inspiration from this 32 bit ALU design, although mine only has 8 bits.</p><center><img src="ALU.png" alt="alu"><p><small>32 bit ALU design</small></p></center><h1 id="The-Full-CPU"><a href="#The-Full-CPU" class="headerlink" title="The Full CPU"></a>The Full CPU</h1><p>The design of the CPU includes six registers, an input, and an output, along with an ALU. Additionally, there is a unit that can carry out comparisons between a value and zero. The CPU operates by executing one of four distinct types of operations:</p><ul><li><p>Immediate: This operation is for moving a given value into register 0. </p></li><li><p>Arithmetic: The operands for any arithmetic operation are always register 1 and register 2, and the resultant output is stored in register 3.</p></li><li><p>Copy: This operation is used for duplicating values from one register to another or to the output. For example, ‘copy 0 6’ copies from register 0 to the output, and ‘copy 5 3’ copies from register 5 to register 3.</p></li><li><p>Evaluation: This operation evaluates the value in register 3 against zero. If the condition is true, it sets the program counter to the value in register 0.</p></li></ul><center><img src="full_cpu.jpeg" alt="full cpu"><p><small>CPU design</small></p></center><p>Using just these operations we have already created a turing complete CPU! However writing code for it would be challenging as we would have to write everything in binary. To solve this I made a simple assembler that converts assembly code to binary. It also adds a number of useful features such as labels. Labels allow you to mark a point in your program to jump to, making it much easier to create loops in your code. Let’s take a look at a simple program. </p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># create a marker at the beginning of the program where we can jump to</span><br><span class="line">label start </span><br><span class="line"></span><br><span class="line"># read from input (reg6) into reg1</span><br><span class="line">copy 6 1</span><br><span class="line"></span><br><span class="line"># add reg 1 and 2, store the result in reg3</span><br><span class="line">add</span><br><span class="line"></span><br><span class="line">#copy result from reg3 into reg2</span><br><span class="line">copy 3 2</span><br><span class="line"></span><br><span class="line"># loop back to start if the value is not negative</span><br><span class="line">start</span><br><span class="line">eval &gt;=</span><br><span class="line"></span><br><span class="line"># Once the value overflows and becomes negative send the result to the output</span><br><span class="line">copy 3 6</span><br></pre></td></tr></table></figure><p>In the simple program above, the value 1 is continuously being fed into the CPU. At each iteration we add it to the value stored in register 2, so that we increment the value by 1. Eventually the value grows large enough that it overflows and becomes negative. This is because in binary the most significant bit (MSB) is used to represent negative numbers. Once this happens the program stops running.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><center><img src="norway.jpeg" alt="NORway"><p><small>NOR-way</small></p></center><p>I thought this was quite a fun project, I certainly learnt a lot. While it may seem a little complicated, the truth is that by playing Turing Complete, you learn in a very intuitive manner. I can highly recommend it. If you want to write your own program or take a look at the code, it is all available on my <a href="https://github.com/BasedLukas/cpu_simulator">github</a>.</p><button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#old-cpu-post" aria-expanded="false" aria-controls="collapse">Hide Blog Post</button></div>]]></content>
    
    
      
      
        
        
    <summary type="html"></summary>
        
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Book Review: Unraveling Principal Component Analysis</title>
    <link href="http://loreley.one/2024-09-pca/"/>
    <id>http://loreley.one/2024-09-pca/</id>
    <published>2024-09-08T22:00:00.000Z</published>
    <updated>2025-11-05T00:10:42.657Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Intuition-and-Motivation"><a href="#Intuition-and-Motivation" class="headerlink" title="Intuition and Motivation"></a>Intuition and Motivation</h2><p>Out of everything I’ve read, one of the pieces that resonated most with me was Lockhart’s Lament. Lockhart, a mathematician, published this work around 20 years ago, critiquing the way mathematics is typically taught to students. The gist of his argument is that too much emphasis is placed on memorization, notation, formulae and dry proofs. Instead he argues we should teach students to appreciate mathematics first and foremost - which can later be followed by formalism. A brilliant example:</p><blockquote style="text-align: left" >Lets take the case of a triangle inside a semicircle<br></br><center><img src="angle1.png" alt=""></center><br></br>Now the beautiful truth about this pattern is that no matter where on the circle you place thetip of the triangle, it always forms a nice right angle. (…)<br></br><center><img src="angle2.png" alt=""></center><br></br>Here is a case where our intuition is somewhat in doubt. It’s not at all clear that this shouldbe true; it even seems unlikely— shouldn’t the angle change if I move the tip? What we havehere is a fantastic math problem! Is it true? If so, why is it true? What a great project! What aterrific opportunity to exercise one’s ingenuity and imagination! Of course no such opportunity is given to the students, whose curiosity and interest is immediately deflated by:<br></br><center><img src="angle3.png" alt=""></center><br></br>Could anything be more unattractive and inelegant? Could any argument be moreobfuscatory and unreadable? This isn’t mathematics! <strong>A proof should be an epiphany from the Gods, not a coded message from the Pentagon</strong> (emphahsis added). This is what comes from a misplaced sense of logical rigor: ugliness. The spirit of the argument has been buried under a heap of confusing formalism.No mathematician works this way. No mathematician has ever worked this way. This is acomplete and utter misunderstanding of the mathematical enterprise. Mathematics is not about erecting barriers between ourselves and our intuition, and making simple things complicated. Mathematics is about removing obstacles to our intuition, and keeping simple things simple. Compare this unappetizing mess of a proof with the following argument devised by one of my seventh-graders:<br></br>“Take the triangle and rotate it around so it makes a four-sided box inside the circle. Since the triangle got turnedcompletely around, the sides of the box must be parallel,so it makes a parallelogram. But it can’t be a slanted boxbecause both of its diagonals are diameters of the circle, sothey’re equal, which means it must be an actual rectangle.That’s why the corner is always a right angle.”<br></br><center><img src="angle4.png" alt=""></center><br></br>(…) I was able to point out several stylistic and logicalproblems, and the student was then able to improve the argument. For instance, I wasn’tcompletely happy with the bit about both diagonals being diameters— I didn’t think that wasentirely obvious— but that only meant there was more to think about and more understanding tobe gained from the situation. And in fact the student was able to fill in this gap quite nicely:<p>“Since the triangle got rotated halfway around the circle, the tip<br>must end up exactly opposite from where it started. That’s why<br>the diagonal of the box is a diameter.”</p></blockquote>I felt his argument deeply. As a highschool student I was completely uninterested in “math”. University only reinforced this in me - studying for exams was an exercise in memorization combined with very basic stepwise logical reasoning. There was never any reason to do the math - it was just a constant that existed.<p>Fast forward a few years and I became interested in statistical learning. The ideas involved invite you to think intuitively. You can visualize what happens when you overfit a data set. You can intuit why your residuals should be normally distributed. You can think of gradient descent as traversing a 3D surface with local and global minima. Suddenly there was a reason to “do math”. Excited by this discovery, I decided to revisit everything I had learnt (and forgotten) at university. Armed with heaps of motivation, I bought the acclaimed “Linear Algebra Done Right”. A bulwark. A classic. Used by students all over the world. I slogged through the early chapters. It was rather dull, one definition after another. It is not difficult to follow the logic, and to see how one theorem follows from another. But the entire enterprise was draining and unfulfilling. It felt random. “Follow these steps and you get the result you are looking for. I can prove that it works because of the following …”. But <strong>why</strong> does it work? What possessed you to follow this path as opposed to any other? How might I have come up with this myself? The book provided no answers and I was left feeling deeply unsatisfied.</p><p>Fortunately, I stumbled across Unravelling Principal Component Analysis by Peter Bloem. Right from the get go it’s clear that this isn’t a textbook. This book is for:</p><blockquote style="text-align: left">People who have learned linear algebra and then forgotten it. Who feel a measure of regret that they didn’t pay full attention the first time around. If you’ve ever marveled at the magical results that PCA produces, and you’d like to really understand it, all the way down to the fundament, then this book will provide you with a guide. But perhaps it’s best to think of this as a guided tour of the forests of linear algebra.</blockquote><p>This book is an opportunity to start afresh. At no point does it feel like reading an encyclopedia. Instead it is one big intuition building exercise. We learn why each step is necessary and are left feeling we could have come up with it ourselves.</p><h2 id="Understanding-First"><a href="#Understanding-First" class="headerlink" title="Understanding First"></a>Understanding First</h2><p>Another gripe I have with math education is the order in which the material is presented. The typical textbook seems designed to confuse - leading with notation, discussing edge cases, and rarely pausing to explain. This book is structured differently. First each definition is patiently motivated, explained, then expanded upon, and finally followed by notation and formalization. The explanation of eigenvectors is a case in point:</p><blockquote style="text-align: left">The most common, and probably the most intuitive way to think about matrices is as transformations of points in space. If we have some vector x and we multiply it by a matrix A, we get a new point y = Ax. If A is square, then x and y are in the same space. A good way to visualize this is by domain coloring. We take a large number of points, arranged in a grid, and we color them by some image. This could be a simple color gradient, but we can also choose a photograph or some other image. Following Wikipedia’s example, we’ll use a picture of the Mona Lisa.<br></br><center><img src="lisa1.png" alt="An increasingly fine-grained domain coloring using the Mona Lisa."><p><small>An increasingly fine-grained domain coloring using the Mona Lisa.</small></p></center><br></br>If we apply the transformation A to each of these points, we can tell what effect the matrix has on this space.<br></br><center><img src="lisa2.png" alt=""></center><br></br>All the points are mapped to a new position by A and poor Lisa ends up squished and stretched in various directions. Transformations expressible in a matrix are linear transformations. These are the transformations for which a line in the original image is still a line in the transformed image. This means that we can rotate, stretch, squish and flip the image in any direction we like, but we can’t warp, bend or tear it.In this language of transformation, we can very naturally define what eigenvectors are. The eigenvectors of a square matrix A are defined as those vectors (i.e. points in the image) for which the direction doesn’t change under transformation by A.It’s simplest to see what this looks like for a diagonal matrix. For instance in the transformation<br></br> <div style="text-align:center;">   <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">y</mi><mo>=</mo><mrow><mo fence="true">(</mo><mtable rowspacing="0.16em" columnalign="center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>2</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle></mtd></mtr></mtable><mo fence="true">)</mo></mrow><mi mathvariant="bold">x</mi></mrow><annotation encoding="application/x-tex">\mathbf{y} = \begin{pmatrix} 2 &amp; 0 \\ 0 &amp; \frac{1}{2} \end{pmatrix} \mathbf{x}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6389em;vertical-align:-0.1944em;"></span><span class="mord mathbf" style="margin-right:0.01597em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.4051em;vertical-align:-0.9526em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size3">(</span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.4526em;"><span style="top:-3.6126em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span></span></span><span style="top:-2.4074em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9526em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.4526em;"><span style="top:-3.6126em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">0</span></span></span><span style="top:-2.4074em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9526em;"><span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size3">)</span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathbf">x</span></span></span></span> </div><br></br>the matrix acts independently on the first and second dimensions, squishing one, and stretching the other.<br></br><center><img src="lisa3.png" alt=""></center><br></br>In this image we’ve also drawn two vectors: one to the middle of Mona Lisa’s left eye, and one to middle of the right. Since Leonardo put the right eye dead center in the painting (not by accident, I imagine), the red vector shrinks, but doesn’t change direction. The green vector is affected by both the squishing and the stretching, so its direction and magnitude both change. Hence, the red vector is an eigenvector, and the green vector isn’t. In a diagonal matrix, the eigenvectors are always the vectors that point in the same directions as the axes, so they’re easy to identify. In general square matrices, finding the eigenvectors is more tricky.</blockquote>After first giving us this intuitive, geometric (and color coded) explanation Peter takes us to the definition.<br></br><blockquote style="text-align: left">Formally, a vector v is an eigenvector of A if the following holds for some scalar λ:<p>Av &#x3D; λv</p><p>This is just a symbolic version of what we said in words above: if v is an eigenvector of A, then transforming it changes its magnitude but not its direction, which is the same as saying we can multiply it by a scalar instead of by a matrix. The scalar corresponding to the eigenvector, in this case λ is called an eigenvalue of the matrix.</p></blockquote>and assures us:<br></br><blockquote style="text-align: left">It’s not at all clear from the definition why these vectors should be meaningful or special. For now, just trust me that eigenvectors are worth knowing about.</blockquote>This is what I love about his style. Commonly we would start by reading a list of rules, likely irrelevant to understanding the core of the concept. Mathematicians can’t refrain from interjecting. ”This only applies to finite fields and real numbers”. But why lead with that? The reader is still struggling to grasp the new concept. You are overloading their poor brain before they even have a chance to get started.<p>I believe this is partially what makes learning new math difficult - having to actively refresh what each definition in a statement means in order to understand the full statement. It takes time to build automaticity - instant, effortless understanding of a concept. When an author provides a number of definitions and rules and then immediately proceeds to build upon them he does the reader a disservice. It takes working with an idea, until the definitions and concepts become second nature. An author should strive to reduce the cognitive load on the reader. We know you are smart, you don’t have to prove it. Gently introduce the idea, keeping it to the bare minimum. Later you can write all the notation you want. Knock yourself out! Yes, I would like to know how we can prove it. Maybe I even care about the case where a vector is a zero vector, or where we are dealing with imaginary numbers. But not right now! Start with the explanation.</p><h2 id="Review"><a href="#Review" class="headerlink" title="Review"></a>Review</h2><p>Only the first 2 chapters actually deal with PCA. We start by examining the problem. Why do we need to reduce the dimensionality of our data? What is a principal component and why is it useful? Then we move on to understanding the basic process. How do we arrive at the solution, where do the principal components come from?<br>At this point you could stop and use the PCA implementation from your library in ignorant bliss. But that’s not actually what this book is about. It is a tour of linear algebra, hiding under the guise of PCA. If you stick with it you’ll be taken deeper. The spectral theorem, determinants and the singular value decomposition are all given the same treatment.<br>It isn’t perfect, and mathematicians will probably complain about rigor. I get it. In my opinion the dynamic runs as follows: Math is hard and requires a modicum of intelligence. This leads to a search for shortcuts to avoid the most difficult parts. So mathematicians are rightly suspect when someone comes along claiming to teach “subjective” or “intuitive” math without the rigor. In fact, any time you come across math without rigor and notation you would do well to ask why. But this book is not an attempt to avoid hard work. It’s here to satisfy your curiosity and that unease you feel when you don’t quite understand why something works as well as it does.<br>A feature of this style is that at times it can be a little hard to follow. It’s easy to follow a logical proof step by step and verify its validity. Due to the narrative style of this book it can sometimes be a little unclear exactly what is happening. But Peter acquits himself remarkably well in this regard. For the most part rereading a paragraph will be sufficient. There were however a number of times where despite rereading a section I still couldn’t make sense of it and was forced to find some other source to help me understand.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>I’m pleased to have found this book, it’s been a game changer for me. A friend recommended his <a href="https://peterbloem.nl/blog/?ref=loreley.one">blog</a> to me when I was looking for explanations of the transformer architecture, which is how I found it. The full book is available on his blog as a <a href="https://peterbloem.nl/publications/unraveling-pca?ref=loreley.one">free PDF</a>, or you can buy it on amazon. Lockhart’s Lament is also a fun read and only around 25 pages. It is available <a href="lockharts_lament.pdf">here (PDF)</a>.</p>]]></content>
    
    
      
      
        
        
    <summary type="html"></summary>
        
      
    
    
    
    
    <category term="models" scheme="http://loreley.one/tags/models/"/>
    
    <category term="math" scheme="http://loreley.one/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>ACDL 2024</title>
    <link href="http://loreley.one/2024-06-acdl/"/>
    <id>http://loreley.one/2024-06-acdl/</id>
    <published>2024-06-14T22:00:00.000Z</published>
    <updated>2025-11-05T00:10:42.645Z</updated>
    
    <content type="html"><![CDATA[<p>The <a href="https://acdl2024.icas.events/">ACDL</a> is a summer school, which, as the name implies, focuses on data science and to a larger extent machine learning. I recently had the pleasure of attending for the first time. This year it was held on the Tuscan coast, and attracted a mostly European audience with many participants from Italy.</p><center><img src="beach.jpg"  width=900  alt="beach"><p><small>The beach at Hotel Riva del Sole, where the event was held</small></p></center><p>The course revolves around the talks, with subjects ranging from clustering and network analysis to large language models. LLM hype seems to be everywhere, and took up roughly half of the lecture time. The talks were mostly technical, and given on a phd student (the majority of the attendees) level. Here follows a list of some of my favorite talks from the event.</p><h4 id="Multimodal-Foundation-Models"><a href="#Multimodal-Foundation-Models" class="headerlink" title="Multimodal Foundation Models"></a>Multimodal Foundation Models</h4><p>by <a href="https://gabrielbm.com/">Gabriel Barth-Maron</a></p><p>This series of lectures was the highlight of the entire course for me, due to its relevance to my work. Gabriel discussed the architecture of multimodal models, as well as techniques for combining and using pretrained backbones in multimodal models. In particular, the <a href="https://arxiv.org/abs/2402.11530">bunny series of models</a> is something I plan to look into more in the future.</p><h4 id="Machine-Learning-for-Modeling-and-Control-of-Complex-Systems"><a href="#Machine-Learning-for-Modeling-and-Control-of-Complex-Systems" class="headerlink" title="Machine Learning for Modeling and Control of Complex Systems"></a>Machine Learning for Modeling and Control of Complex Systems</h4><p>by <a href="https://seas.harvard.edu/person/petros-koumoutsakos">Petros Koumoutsakos</a></p><p>This series was very wide-ranging, but often touched on Koumoutsakos’s work with fluid mechanics. I wasn’t able to follow it in its entirety but still found it very interesting. His work on <a href="https://cse-lab.seas.harvard.edu/research-artificial_intelligence-reinforcement_learning">experience replay</a> is something I will have to come back to.</p><h4 id="Diffusion-Models"><a href="#Diffusion-Models" class="headerlink" title="Diffusion Models"></a>Diffusion Models</h4><p>by <a href="https://chinweihuang.com/">Chin-Wei Huang</a></p><p>I like diffusion models and have played around with them quite a bit. But the math behind them is a little more complex than what I am used to, so I was happy to attend this introductory talk. On a side note <a href="https://www.chenyang.co/diffusion.html">this blog post</a> is still the best introduction to diffusion models that I’m aware of.</p><h4 id="Fusing-Machine-Learning-and-Optimization-for-Engineering"><a href="#Fusing-Machine-Learning-and-Optimization-for-Engineering" class="headerlink" title="Fusing Machine Learning and Optimization for Engineering"></a>Fusing Machine Learning and Optimization for Engineering</h4><p>by <a href="https://sites.gatech.edu/pascal-van-hentenryck/">Pascal van Hentenryck</a></p><p>Optimization is not something I understand very well (yet), but this series of talks was nonetheless great. Pascal spoke about the challenges of optimizing an energy grid, and the unique methods that have been developed to do this.</p><h2 id="Overall"><a href="#Overall" class="headerlink" title="Overall"></a>Overall</h2><p>In my opinion, one of the best parts of attending these events are the people you meet. It’s quite special to meet so many other people who share the same interests as you and are happy to discuss their research (where else can you discuss optimization algorithms on the beach?). I would certainly recommend that you seek out a similar event if you’ve never done so before. <a href="https://acdl2025.icas.events/">Here</a> is the website for next years event. <a href="https://www.summerschoolsineurope.eu/search">This search engine</a> was how I found the ACDL and might also be of use to you.</p>]]></content>
    
    
      
      
        
        
    <summary type="html"></summary>
        
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Double Q-Learning Explained</title>
    <link href="http://loreley.one/2024-03-double_q/"/>
    <id>http://loreley.one/2024-03-double_q/</id>
    <published>2024-03-15T23:00:00.000Z</published>
    <updated>2025-11-05T00:10:42.651Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Q-learning is a popular reinforcement learning algorithm, used for solving Markov Decision Processes (MDP). In some cases, Q-learning doesn’t work well and takes a long time to converge, due to an issue known as the optimizer’s curse or maximization bias. In this post, we’ll take a look at the problem as well as a proposed solution. What follows is a brief recapitulation of MDP’s and Q-learning, followed by a deep dive into Double Q learning, the proposed solution to the problem.</p><h2 id="Recap-of-Q-learning"><a href="#Recap-of-Q-learning" class="headerlink" title="Recap of Q learning"></a>Recap of Q learning</h2><p>A Markov chain consists of states connected through transition probabilities, which determine the likelihood of moving from one state to another. This probability depends only on the current state, not on the sequence of events that preceded it. Some states are accessible only from specific other states, forming a directed network.</p><p>A Markov Decision Process (MDP) extends the concept of a Markov chain by incorporating decisions. It substitutes transition probabilities with actions that represent available choices. Each state in an MDP is linked with a reward, indicating the value of reaching that state. The distinct feature of MDPs is the decision-making aspect, where an agent selects actions to transition between states and accumulate rewards. The goal in an MDP is to find an optimal policy, which is a set of rules defining the best action to take in each state to maximize rewards.</p><p>A trajectory through an MDP is represented using the notation: starting at state <code>S</code>, an action <code>A</code> is chosen from the available options in state <code>S</code>. This leads to a transition to state <code>S'</code> with probability <code>P</code>, and a reward <code>R</code> is received. The tuple <code>(S, A, P, R)</code> describes this process, where <code>P</code> is defined as <code>Pr(S' | S, A)</code>. This sequence repeats at each step until a terminal state is reached, outlining the full trajectory:</p><p><code>S<sub>0</sub>, A<sub>0</sub>, R<sub>1</sub>, S<sub>1</sub>, A<sub>1</sub>, R<sub>2</sub>, S<sub>2</sub>, A<sub>2</sub>, R<sub>3</sub>, ...</code></p><p>The Q(action, value) function under a policy <code>π</code> is formally defined as:</p><p><code>Q<sub>π</sub>(s, a) = E[G | s, a, π]</code></p><p>where <code>E[G | s, a, π]</code> represents the expected total (discounted) reward given that we start in state <code>s</code>, take action <code>a</code>, and then follow policy <code>π</code> for all subsequent decisions. This expectation accounts for the sum of rewards received, starting from <code>s</code> and <code>a</code>, under the guidance of policy <code>π</code>.</p><p>In Q-learning, the objective is to approximate the optimal Q function, which represents the best action values under an optimal policy, regardless of the initial policy used to generate training data. The policy generating our training data decides actions, which might not be optimal. Our aim is to iteratively refine our Q function based on these examples. The algorithm is as follows:</p><center><img src="q_algo.png" alt="q learning algorithm"><p><small>Q-learning algorithm (Sutton and Barto)</small></p></center><p>For a full discussion of Q-learning I recommend the following 2 sources:</p><ul><li><a href="https://huggingface.co/learn/deep-rl-course/unit2/introduction">Hugging Face course on Q-learning</a>. This is a nice quick overview.</li><li>For a full treatment see the RL book by Sutton and Barto, which is available free <a href="http://incompleteideas.net/book/the-book.html">here</a>.</li></ul><h2 id="Dissection-of-the-problem"><a href="#Dissection-of-the-problem" class="headerlink" title="Dissection of the problem"></a>Dissection of the problem</h2><p>A common issue with Q-learning involves how it handles variance in rewards. Consider an MDP where we start in state <code>A</code> with the options to move to <code>B</code> or directly to a terminal state <code>T</code>, neither transition offering any reward. From <code>B</code>, we can transition to several secondary states, <code>C<sub>1</sub>, C<sub>2</sub>, ..., C<sub>n</sub></code>, each associated with a reward from a normal distribution with a negative mean (e.g., -0.1) and a variance (e.g., 1). Transitioning from any <code>C<sub>n</sub></code> to <code>T</code> yields no reward. Ideally, the optimal strategy is to move from <code>A</code> to <code>T</code>, avoiding negative rewards in the <code>C</code> states. However, the stochastic nature of rewards means a visit to any <code>C</code> state might yield a positive reward. The likelihood of receiving a positive reward increases with the number of <code>C</code> states.</p><p>This variance introduces a challenge in Q-learning. The algorithm estimates the Q-value for transitioning from <code>A</code> to <code>B</code> based on the maximum reward obtainable from moving to any <code>C</code> state. Given the rewards are drawn from a distribution, it’s probable to encounter a positive reward in one of the <code>C</code> states during exploration. Consequently, the Q-function may overestimate the value of moving from A to B. Essentially, the “max” operation in the update rule can cause a single overoptimistic estimate to skew the Q-values, leading to incorrect policy decisions.</p><p>A more in-depth explanation can be found in the paper <a href="optimizers_curse.pdf">The Optimizer’s Curse: Skepticism and Postdecision Surprise in Decision Analysis (pdf)</a> by Smith and Winkler.</p><h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><p>A solution to this problem, Double Q-learning, was proposed by <a href="https://papers.nips.cc/paper_files/paper/2010/file/091d584fced301b442654dd8c23b3fc9-Paper.pdf">Hasselt (pdf)</a>. Let’s view our problem through a slightly different lens. The issue arises because we are using the same samples of <code>C</code> twice. Once to estimate the value of taking an action; <code>Q(B, move to C<sub>i</sub>)</code>. Secondly when performing the maximizing operation to determine which <code>C<sub>i</sub></code> is best to move to from state b; <code>max<sub>i</sub> Q(B, C<sub>i</sub>)</code>. If we instead use 2 independent estimates, <code>Q<sub>1</sub></code> and <code>Q<sub>2</sub></code>, we alleviate this problem. <code>Q<sub>1</sub></code> might overestimate the value of moving to a particular state <code>C<sub>i</sub></code>, but it’s very unlikely that <code>Q<sub>2</sub></code> also estimates moving to <code>C<sub>i</sub></code> to be the best action to take from <code>B</code>.</p><p>This sounds confusing, so let’s walk through the original Q-learning update again. After removing the discount (unimportant for our purposes) we are left with:</p><p><code>Q(s, a) = Q(s, a) + α * (reward + max a Q(s’,a) - Q(s,a))</code></p><p>Conceptually we are doing the following:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># the old/current estimate of taking action a in state s</span><br><span class="line">old = q(s,a)</span><br><span class="line"></span><br><span class="line"># new estimate is the reward, plus our best estimate for future rewards starting from the next state</span><br><span class="line">new = r +  max a q(s’, a) </span><br><span class="line"></span><br><span class="line"># the discrepancy between the 2 estimates</span><br><span class="line">error = new - old</span><br><span class="line"></span><br><span class="line"># update our estimate:</span><br><span class="line">q(s,a) = old + learning_rate * error</span><br></pre></td></tr></table></figure><p>What’s important to realize is we are making use of the same Q function to get our estimates twice. Once for <code>Q(s’,a)</code> to get the value of the new state action pair, and again when performing <code>max a</code> on <code>Q(s’,a)</code> to decide what value of <code>a</code> to use. </p><p>The double Q-learning solution to our problem says we should use two independent estimates of the state-action values. Our update is now as follows:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># the old/current estimate of taking action a in state s</span><br><span class="line">old = q1(s,a)</span><br><span class="line"></span><br><span class="line"># use q1 to estimate the best action to take in the next state</span><br><span class="line">best_action_in_state_s’ = argmax a q1(s’, a)</span><br><span class="line"></span><br><span class="line"># use q2 to determine what the value of the action is</span><br><span class="line">value of s’ = q2(s’, best_action_in_state_s’)</span><br><span class="line"></span><br><span class="line"># new estimate is the reward, plus our best estimate for future rewards starting from the next state</span><br><span class="line">new = r + value of s’</span><br><span class="line"></span><br><span class="line">error = new - old</span><br><span class="line"></span><br><span class="line">updated q1(s,a) = old + learning_rate * error</span><br></pre></td></tr></table></figure><p>The full double Q-learning algorithm is as follows:</p><center><img src="double_algo.png" alt="double q algorithm"><p><small>Double Q-learning (Sutton and Barto)</small></p></center><p>Since <code>Q1</code> is updated on different samples than <code>Q2</code>, they are not subject to the maximization bias. The algorithm does require more memory to store two Q functions. The computational cost stays the same.</p><h2 id="Code-Walkthrough"><a href="#Code-Walkthrough" class="headerlink" title="Code Walkthrough"></a>Code Walkthrough</h2><p>The first time I went through this, it was a bit of a head-scratcher, so let’s walk through the code in python to make it more concrete. We will be using the exact same example MDP as above. We will run both Q-learning and Double Q-learning and compare their results. The full code is available <a href="https://gist.github.com/BasedLukas/bda5cfed389e42108fc9f6a8daeb7cd7">here</a>.</p><h5 id="Create-a-Markov-process-Note-that-the-values-of-states-C-are-drawn-from-N-0-1-1"><a href="#Create-a-Markov-process-Note-that-the-values-of-states-C-are-drawn-from-N-0-1-1" class="headerlink" title="Create a Markov process. Note that the values of states C are drawn from N(-0.1, 1)."></a>Create a Markov process. Note that the values of states <code>C</code> are drawn from <code>N(-0.1, 1)</code>.</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_reward</span>(<span class="params">state: <span class="built_in">str</span></span>) -&gt; <span class="built_in">float</span>:</span><br><span class="line">   <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">   Returns the reward for transitioning into a given state.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">   Args:</span></span><br><span class="line"><span class="string">   - state: The state transitioned into.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">   Returns:</span></span><br><span class="line"><span class="string">   - A float representing the reward for that transition.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">   Raises:</span></span><br><span class="line"><span class="string">   - ValueError: If an invalid state is provided.</span></span><br><span class="line"><span class="string">   &quot;&quot;&quot;</span></span><br><span class="line">   <span class="keyword">if</span> state == <span class="string">&quot;a&quot;</span>:</span><br><span class="line">       <span class="keyword">raise</span> ValueError(<span class="string">&quot;a should not be passed as a param as it&#x27;s the starting state&quot;</span>)</span><br><span class="line">   <span class="keyword">if</span> state == <span class="string">&#x27;b&#x27;</span> <span class="keyword">or</span> state == <span class="string">&#x27;terminal&#x27;</span>:</span><br><span class="line">       <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">   <span class="keyword">if</span> <span class="string">&#x27;c&#x27;</span> <span class="keyword">in</span> state:</span><br><span class="line">       <span class="keyword">return</span> np.random.normal(-<span class="number">0.1</span>, <span class="number">1</span>)</span><br><span class="line">   <span class="keyword">raise</span> ValueError(<span class="string">f&quot;state: <span class="subst">&#123;state&#125;</span> not recognized&quot;</span>)</span><br><span class="line"></span><br><span class="line">transitions = &#123;</span><br><span class="line">    <span class="string">&quot;a&quot;</span>: [<span class="string">&quot;terminal&quot;</span>, <span class="string">&quot;b&quot;</span>],</span><br><span class="line">    <span class="string">&quot;b&quot;</span>: [<span class="string">&quot;c&quot;</span>+<span class="built_in">str</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(number_of_c_states)]</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(number_of_c_states):</span><br><span class="line">    transitions[<span class="string">f&quot;c<span class="subst">&#123;i&#125;</span>&quot;</span>] = [<span class="string">&quot;terminal&quot;</span>]</span><br></pre></td></tr></table></figure><h5 id="Our-Q-functions-are-simply-dictionaries"><a href="#Our-Q-functions-are-simply-dictionaries" class="headerlink" title="Our Q functions are simply dictionaries."></a>Our <code>Q</code> functions are simply dictionaries.</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">q:  Dict[Tuple[str, int], float] = &#123;&#125;</span><br><span class="line">q1: Dict[Tuple[str, int], float] = &#123;&#125;</span><br><span class="line">q2: Dict[Tuple[str, int], float] = &#123;&#125;</span><br></pre></td></tr></table></figure><h5 id="Now-define-a-function-to-do-the-Q-learning-update-max-a-uses-the-provided-q-to-find-the-best-next-value"><a href="#Now-define-a-function-to-do-the-Q-learning-update-max-a-uses-the-provided-q-to-find-the-best-next-value" class="headerlink" title="Now define a function to do the Q-learning update. max_a uses the provided q to find the best next value."></a>Now define a function to do the Q-learning update. <code>max_a</code> uses the provided <code>q</code> to find the best next value.</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">q_update</span>(<span class="params"></span></span><br><span class="line"><span class="params">       state: <span class="built_in">str</span>,</span></span><br><span class="line"><span class="params">       action: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">       new_state: <span class="built_in">str</span>,</span></span><br><span class="line"><span class="params">       reward: <span class="built_in">float</span>,</span></span><br><span class="line"><span class="params">       alpha: <span class="built_in">float</span>,</span></span><br><span class="line"><span class="params">       q: <span class="type">Dict</span></span></span><br><span class="line"><span class="params">   </span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">   <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">   In-place update of Q-values for Q-learning.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">   Args:</span></span><br><span class="line"><span class="string">       state: The current state.</span></span><br><span class="line"><span class="string">       action: The action taken in the current state.</span></span><br><span class="line"><span class="string">       new_state: The state reached after taking the action.</span></span><br><span class="line"><span class="string">       reward: The reward received after taking the action.</span></span><br><span class="line"><span class="string">       alpha: The learning rate.</span></span><br><span class="line"><span class="string">       q: The Q-values dictionary.</span></span><br><span class="line"><span class="string">   &quot;&quot;&quot;</span></span><br><span class="line">   current_q = q.get((state, action), <span class="number">0</span>)  <span class="comment"># Current Q-value estimation</span></span><br><span class="line">   max_next = max_a(new_state, q)  <span class="comment"># Maximum Q-value for the next state</span></span><br><span class="line">   target = reward + gamma * max_next  <span class="comment"># TD Target</span></span><br><span class="line">   td_error = target - current_q  <span class="comment"># TD Error</span></span><br><span class="line">   update = alpha * td_error  <span class="comment"># TD Update</span></span><br><span class="line">   q[(state, action)] = current_q + update</span><br></pre></td></tr></table></figure><h5 id="The-Double-Q-learning-update-argmax-a-finds-the-best-next-action-using-the-provided-Q-function"><a href="#The-Double-Q-learning-update-argmax-a-finds-the-best-next-action-using-the-provided-Q-function" class="headerlink" title="The Double Q-learning update. argmax_a finds the best next action, using the provided Q function."></a>The Double Q-learning update. <code>argmax_a</code> finds the best next action, using the provided Q function.</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">double_q_update</span>(<span class="params"></span></span><br><span class="line"><span class="params">       state: <span class="built_in">str</span>,</span></span><br><span class="line"><span class="params">       action: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">       new_state: <span class="built_in">str</span>,</span></span><br><span class="line"><span class="params">       reward: <span class="built_in">float</span>,</span></span><br><span class="line"><span class="params">       alpha: <span class="built_in">float</span>,</span></span><br><span class="line"><span class="params">       q1: <span class="type">Dict</span>,</span></span><br><span class="line"><span class="params">       q2: <span class="type">Dict</span></span></span><br><span class="line"><span class="params">   </span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">   <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">   In-place update of Q-values for Double Q-learning.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">   Args:</span></span><br><span class="line"><span class="string">       state: The current state.</span></span><br><span class="line"><span class="string">       action: The action taken in the current state.</span></span><br><span class="line"><span class="string">       new_state: The state reached after taking the action.</span></span><br><span class="line"><span class="string">       reward: The reward received after taking the action.</span></span><br><span class="line"><span class="string">       alpha: The learning rate.</span></span><br><span class="line"><span class="string">       q1: The first Q-values dictionary.</span></span><br><span class="line"><span class="string">       q2: The second Q-values dictionary.</span></span><br><span class="line"><span class="string">   &quot;&quot;&quot;</span></span><br><span class="line">   qs = [q1, q2]  <span class="comment"># List of Q dictionaries</span></span><br><span class="line">   random.shuffle(qs)  <span class="comment"># Randomly shuffle to choose one for updating</span></span><br><span class="line">   qa, qb = qs  <span class="comment"># qa is the Q to update, qb</span></span><br><span class="line"></span><br><span class="line"> <span class="keyword">is</span> used <span class="keyword">for</span> target calculation</span><br><span class="line"></span><br><span class="line">   current_q = qa.get((state, action), <span class="number">0</span>)  <span class="comment"># Current Q-value estimation</span></span><br><span class="line">   best_action = argmax_a(new_state, qa)  <span class="comment"># Best action based on qa</span></span><br><span class="line">   target = reward + gamma * qb.get((new_state, best_action), <span class="number">0</span>)  <span class="comment"># TD Target using qb</span></span><br><span class="line">   error = target - current_q  <span class="comment"># TD Error</span></span><br><span class="line">   update = alpha * error  <span class="comment"># TD Update</span></span><br><span class="line">   qa[(state, action)] = current_q + update</span><br></pre></td></tr></table></figure><h5 id="At-this-point-we-simulate-both"><a href="#At-this-point-we-simulate-both" class="headerlink" title="At this point, we simulate both."></a>At this point, we simulate both.</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">simulate</span>(<span class="params"></span></span><br><span class="line"><span class="params">       epoch: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">       q: <span class="type">Dict</span>,</span></span><br><span class="line"><span class="params">       q2: <span class="type">Optional</span>[<span class="type">Dict</span>] = <span class="literal">None</span></span></span><br><span class="line"><span class="params">   </span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">   <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">   Simulate an epoch of the agent&#x27;s interaction with the environment, updating Q-values based on observed transitions.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">   Args:</span></span><br><span class="line"><span class="string">       epoch: The current epoch of the simulation.</span></span><br><span class="line"><span class="string">       q: The Q-values dictionary for Q-learning or the primary Q-values dictionary for Double Q-learning.</span></span><br><span class="line"><span class="string">       q2: The secondary Q-values dictionary for Double Q-learning, if applicable.</span></span><br><span class="line"><span class="string">   &quot;&quot;&quot;</span></span><br><span class="line">   double = q2 <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">   state = <span class="string">&#x27;a&#x27;</span></span><br><span class="line">   <span class="keyword">while</span> state != <span class="string">&#x27;terminal&#x27;</span>:</span><br><span class="line">       <span class="keyword">if</span> double:</span><br><span class="line">           action = policy(state, epoch, q, q2)</span><br><span class="line">       <span class="keyword">else</span>:</span><br><span class="line">           action = policy(state, epoch, q)</span><br><span class="line">       new_state = transitions[state][action]</span><br><span class="line">       reward = get_reward(new_state)</span><br><span class="line">      </span><br><span class="line">       <span class="keyword">if</span> double:</span><br><span class="line">           double_q_update(</span><br><span class="line">               state=state,</span><br><span class="line">               action=action,</span><br><span class="line">               new_state=new_state,</span><br><span class="line">               reward=reward,</span><br><span class="line">               alpha=lr,</span><br><span class="line">               q1=q,</span><br><span class="line">               q2=q2</span><br><span class="line">               )</span><br><span class="line">       <span class="keyword">else</span>:</span><br><span class="line">           q_update(state, action, new_state, reward, lr, q)</span><br><span class="line"></span><br><span class="line">       state = new_state</span><br></pre></td></tr></table></figure><p>Now we can plot the results. They will differ every time, but mostly look something like this.</p><center><img src="double_q.png" alt="my results"><p><small>My results</small></p></center><p>We can clearly see how Q-learning has trouble converging on the correct result. I was so excited when I got this result, because it mirrors very closely with that of Sutton and Barto!</p><center><img src="sutton.png" alt="Suttons results"><p><small>Sutton's results (Sutton and Barto)</small></p></center><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>I went into this deep dive because I had trouble understanding this myself. I’m still not entirely sure I understand it, but writing out the code certainly helps. Sutton and Barto’s <a href="http://incompleteideas.net/book/the-book.html">book</a> on RL really is a must. Other useful resources are these series on YouTube:</p><ul><li><a href="https://www.youtube.com/playlist?list=PLwRJQ4m4UJjNymuBM9RdmB3Z9N5-0IlY0">Foundations of deep RL</a></li><li><a href="https://www.youtube.com/playlist?list=PLqYmG7hTraZBKeNJ-JE_eyJHZ7XgBoAyb">DeepMind x UCL | Reinforcement Learning Course</a></li></ul><p>And of course, Andrej Karpathy’s <a href="http://karpathy.github.io/2016/05/31/rl/">blog post</a>.</p>]]></content>
    
    
      
      
        
        
    <summary type="html"></summary>
        
      
    
    
    
    
    <category term="python" scheme="http://loreley.one/tags/python/"/>
    
    <category term="models" scheme="http://loreley.one/tags/models/"/>
    
    <category term="reinforcement learning" scheme="http://loreley.one/tags/reinforcement-learning/"/>
    
    <category term="probability" scheme="http://loreley.one/tags/probability/"/>
    
  </entry>
  
  <entry>
    <title>Will Gzip Replace Neural Networks for Text Classification?</title>
    <link href="http://loreley.one/2023-07-gzip/"/>
    <id>http://loreley.one/2023-07-gzip/</id>
    <published>2023-07-18T22:00:00.000Z</published>
    <updated>2025-11-05T00:10:42.655Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Large language models are currently all the rage when it comes to text classification. However, a recent <a href="https://aclanthology.org/2023.findings-acl.426/">research paper</a> offers an interesting non-parametric alternative. The proposed method is elegant in its simplicity: it combines a text compressor, gzip, with a k-nearest-neighbour classifier. This approach requires no training parameters, making it a lightweight and universally adaptable solution.</p><p>The cornerstone of this method lies in two key ideas: first, compressors are good at capturing regularities in data, and second, data points from the same category share more regularity than those from different categories. </p><p>In this article, we explore this approach to text classification, discussing its rationale, how it works, and its practical implementation in Python.</p><h2 id="The-Mechanics-of-Compression-and-Classification"><a href="#The-Mechanics-of-Compression-and-Classification" class="headerlink" title="The Mechanics of Compression and Classification"></a>The Mechanics of Compression and Classification</h2><p>The technique proposed in the paper uses combination of a text compressor and a k-nearest-neighbor classifier. But what makes this a viable method for text classification?</p><p>At the heart of this approach is the idea that data compressors, like gzip, are skilled at capturing patterns, or regularities, in data. As an example, consider the following string ‘aaaaaabbb’ which can be compressed as ‘6<em>a 3</em>b’. Similarly, a data compressor finds patterns and regularities in the data and uses them to compress it, which is exactly what gzip does with text data.</p><p>The second key idea is that data points from the same category have more in common, they share more regularity than data points from different categories. For example, English text contains more instances of ‘-ing’ or similar arrangements of letters than does French. This forms the basis for classifying texts using the compressed length, or how much the data can be compressed.</p><h4 id="LZ77-Patterns-and-References"><a href="#LZ77-Patterns-and-References" class="headerlink" title="LZ77: Patterns and References"></a>LZ77: Patterns and References</h4><p>At the core of the gzip algorithm are two primary compression techniques: LZ77 and Huffman encoding. Let’s begin our exploration with the former - the LZ77 algorithm.</p><p>LZ77 operates on a straightforward principle: it replaces repeating data with references to its prior occurrence. Imagine the phrase “apple apple”. LZ77 would replace the second “apple” with a reference to its first appearance, considerably reducing the data footprint.</p><p>The algorithm analyzes a ‘window’ of characters, retrospectively scanning for matches. When a match is found, it is replaced by a reference pointing to the initial occurrence of that string. This simple replacement procedure effectively compacts data, especially in instances of high repetition.</p><p>Consider a text filled with recurring phrases or words - LZ77 transforms this repetitiveness into a compression advantage. The same piece of data doesn’t need to be stored twice; a reference to its first occurrence will suffice. Thus, texts exhibiting more patterns are subject to more effective compression than random text. </p><p>Here is a simple example:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Never gonna give you up</span><br><span class="line">Never gonna let you down</span><br><span class="line">Never gonna run around and desert you</span><br><span class="line">Never gonna make you cry</span><br><span class="line">Never gonna say goodbye</span><br><span class="line">Never gonna tell a lie and hurt you</span><br><span class="line">We&#x27;ve known each other for so long</span><br><span class="line">Your heart&#x27;s been aching, but you&#x27;re too shy to say it (say it)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Post LZ77 compression, the lyrics transform into:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Never gonna(6, 2)i(12, 2) you up(24, 13)let(23, 5)down(49, 13)run ar(49, 2)nd(7, 2)n(4, 2)des(27, 2)t(43, 4)</span><br><span class="line">(38, 12)make(21, 4) cry(25, 13)say(35, 3)odbye(49, 13)tell (31, 2)lie(6, 2)nd hurt you</span><br><span class="line">We&#x27;(37, 2) known each other fo(4, 2)so(47, 2)ong</span><br><span class="line">Y(39, 2)r(49, 2)ea(50, 2)&#x27;s bee(41, 2)a(40, 2)i(25, 2),(13, 2)ut y(30, 2)&#x27;re to(46, 2)shy(8, 3) sa(7, 2)i(25, 2)((8, 6))</span><br></pre></td></tr></table></figure><p>Here, each backreference is denoted as <code>(number of places to go backwards, number of characters to replace)</code>. As the compression progresses, more words are replaced by these backreferences, significantly reducing the data footprint.</p><p>However, this process isn’t without its challenges. One potential issue is distinguishing between actual text (literals) and backreferences. Currently, each backreference is enclosed in braces, which becomes problematic when the original text also contains braces. This could be addressed by escaping them, but it introduces redundancy. Huffman encoding, the other key technique utilized by gzip, helps overcome this issue and further compresses the text, which we will explore in the following section.</p><h4 id="Huffman-Encoding-Shorter-Codes-for-Frequent-Characters"><a href="#Huffman-Encoding-Shorter-Codes-for-Frequent-Characters" class="headerlink" title="Huffman Encoding: Shorter Codes for Frequent Characters"></a>Huffman Encoding: Shorter Codes for Frequent Characters</h4><p>After the compression achieved by LZ77, gzip employs Huffman encoding to maximize the compression. Huffman encoding uses variable-length codes to represent each character in the text, where the code’s length is inversely related to the frequency of the character’s occurrence. This method allows us to save additional space by giving shorter codes to characters that appear more frequently.</p><p>Let’s demonstrate this process with an example, using the text <code>AAABBCDDDDEEEEE</code>. Our first task is to tally the frequency of each character. The order is irrelevant at this stage:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">A:3</span><br><span class="line">B:2</span><br><span class="line">C:1</span><br><span class="line">D:4</span><br><span class="line">E:5</span><br></pre></td></tr></table></figure><p>Next, we group the two least frequent characters (C and B) into a tree and keep it aside for future use:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  BC:3</span><br><span class="line">  /  \</span><br><span class="line">B:2  C:1</span><br></pre></td></tr></table></figure><p>We then add the total frequency of B and C, and reintroduce the newly created tree into our frequency table:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">A:3</span><br><span class="line">D:4</span><br><span class="line">E:5</span><br><span class="line">BC:3</span><br></pre></td></tr></table></figure><p>We repeat this process, this time selecting the two lowest frequencies (A and BC):</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ABC:6</span><br><span class="line">  /  \</span><br><span class="line">A:3  BC:3</span><br><span class="line">       /  \</span><br><span class="line">     B:2  C:1</span><br></pre></td></tr></table></figure><p>The frequency table updates to:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">D:4</span><br><span class="line">E:5</span><br><span class="line">ABC:6</span><br></pre></td></tr></table></figure><p>Next, we select D and E to form:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  DE: 9</span><br><span class="line">  /  \</span><br><span class="line">D:4  E:5</span><br></pre></td></tr></table></figure><p>This leads to an updated frequency table:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ABC:6</span><br><span class="line">DE:9</span><br></pre></td></tr></table></figure><p>With just two elements left, we can construct our final tree using all the mini trees generated so far:</p><center><img src="huffman.jpeg" alt="huffman tree"><p><small>The completed Huffman Tree</small></p></center><p>Finally, we assign unique codes for each letter: 0 for left and 1 for right. For instance, the code for D is 10. To reach D from the root, we move first to the right, then to the left. This system is efficient since we don’t need to denote when one code ends and the next begins — we simply stop whenever we reach a leaf. As a result, the most frequently appearing characters have the shortest codes:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">A = 00</span><br><span class="line">E = 11</span><br><span class="line">D = 10</span><br><span class="line">B = 010</span><br><span class="line">C = 011</span><br></pre></td></tr></table></figure><p>With this method, we can compress text quite effectively. Recall our earlier problem of distinguishing between backreferences and literals. Given the frequent recurrence of backreferences in the text, they would receive their own short, unique codes, thereby enhancing the compression efficiency. It’s important to note that while we’ve simplified the process here, the complete gzip algorithm is more complex.</p><h3 id="Classification-Through-Compression"><a href="#Classification-Through-Compression" class="headerlink" title="Classification Through Compression"></a>Classification Through Compression</h3><p>Now that we understand how compression works, we can go back to text classification. Consider three examples, <code>x1</code>, <code>x2</code>, and <code>x3</code>. <code>x1</code> and <code>x2</code> belong to the same category, while <code>x3</code> falls into a different category. Let’s define <code>C(·)</code> as the compressed length of an object. The assumption we make is that <code>C(x1x2) - C(x1) &lt; C(x1x3) - C(x1)</code>. Here, <code>C(x1x2)</code> represents the compressed length of the concatenated <code>x1</code> and <code>x2</code>. Essentially, <code>C(x1x2) - C(x1)</code> is the additional byte count required to encode <code>x2</code> given the prior information of <code>x1</code>.</p><p>This idea forms the basis of a distance metric inspired by the Kolmogorov complexity — a theoretical measure defined as the length of the shortest binary program that can generate a string <code>x</code>. Due to the inherent uncomputability of the Kolmogorov complexity, a normalized, and computable alternative, the Normalized Compression Distance (NCD), is used instead. NCD uses the compressed length <code>C(x)</code> as an approximation for the Kolmogorov complexity <code>K(x)</code>. Formally:</p><p><code>NCD(x, y) = [C(xy) - min&#123;C(x), C(y)&#125;] / max&#123;C(x), C(y)&#125;</code></p><p>The use of compressed length rests on the assumption that the length of <code>x</code> when maximally compressed by a compressor is approximately equal to <code>K(x)</code>. For our purposes, gzip is used as the compressor. Consequently, <code>C(x)</code> denotes the length of <code>x</code> after gzip compression, while <code>C(xy)</code> represents the compressed length of <code>x</code> and <code>y</code> concatenated. With the distance matrix provided by NCD, we can apply k-nearest-neighbors for classification.</p><p>Let’s delve into how this is implemented in practice using Python. We can start with two text strings representing different categories, GroupA and GroupB, and a third string, StringUnknown, which we wish to classify. We compress each of these strings using gzip and calculate their respective lengths. Then, we concatenate StringUnknown with each group, compress the result and calculate their lengths.</p><p>Next, we compute the NCD for the unknown string and each group by subtracting the smallest compressed length of the group or the unknown string from the compressed length of the concatenated string, then dividing by the maximum compressed length of the group or the unknown string. This gives us a normalized measure of the “distance” of the unknown string from each group.</p><p>The group with the smallest NCD to the unknown string is considered the most likely group that the unknown string belongs to. In essence, the smaller the NCD, the greater the similarity between the unknown string and the group.</p><script src="https://gist.github.com/BasedLukas/42022b382660a27a7044770893572b18.js"></script><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CompA:  110</span><br><span class="line">CompB:  102</span><br><span class="line">CompA_Unknown:  153</span><br><span class="line">CompB_Unknown:  163</span><br><span class="line">A_Diff:  43</span><br><span class="line">B_Diff:  61</span><br><span class="line"></span><br><span class="line">NCD_A:  0.42727272727272725</span><br><span class="line">NCD_B:  0.5754716981132075</span><br><span class="line">StringUnknown is in GroupA</span><br></pre></td></tr></table></figure><p>This approach, using the NCD offers is just one method for computing the distance. However, other distance metrics can be employed. </p><p>The following method estimates the cross entropy between the probability distribution built on class <code>c</code> and the document <code>d</code>: <code>Hc(d)</code>. The process involves the following steps:</p><ol><li>For each class <code>c</code>, concatenate all samples <code>dc</code> in the training set belonging to <code>c</code>.</li><li>Compress <code>dc</code> as one long document to get the compressed length <code>C(dc)</code>.</li><li>Concatenate the given test sample <code>du</code> with <code>dc</code> and compress to get <code>C(dcdu)</code>.</li><li>The predicted class is <code>arg min C(dcdu) - C(dc)</code>.</li></ol><p>In essence, this technique uses <code>C(dcdu) - C(dc)</code> as the distance metric, which is computationally more efficient than pairwise distance matrix computation on small datasets. However, it has some drawbacks. Most compressors, including gzip (as mentioned above), have a limited “window” they can use to search back through the repeated string or keep a record of. As a result, even with numerous training samples, the compressor may not fully exploit them. Additionally, compressing <code>dcdu</code> can be slow for large <code>dc</code>, a problem not solved by parallelization. These limitations prevent this method from being applicable to extremely large datasets. As such, the NCD-based approach presented above, which avoids these limitations, becomes particularly appealing for text classification tasks on large datasets.</p><h2 id="Downsides-and-Drawbacks"><a href="#Downsides-and-Drawbacks" class="headerlink" title="Downsides and Drawbacks"></a>Downsides and Drawbacks</h2><p>Despite the ingenuity of applying compression algorithms for text classification, the approach is not without its limitations. The size of the lookback window of the LZ77 algorithm, for instance, presents a potential drawback. This algorithm searches the prior text to find matches, but its effectiveness can diminish when dealing with very long text.</p><p>In addition, the method does not grasp the subtleties of language, such as synonyms or context-specific meanings. The absence of this linguistic sophistication may hinder its performance on tasks requiring a deeper understanding of language nuances. Furthermore, the computational intensity of this approach can escalate rapidly when dealing with larger datasets, given the O(n^2) complexity of K-Nearest Neighbors (KNN). </p><p>Lastly, some controversy surrounds the code and results of the original paper that introduced this method. Some of the datasets used seem to have issues, though not due to the authors’ mistakes. Additionally, there has been some debate regarding how the results have been scored. For a detailed look at these issues, you can check out <a href="https://kenschutte.com/gzip-knn-paper/">this blog post</a> and follow the discussion on <a href="https://github.com/bazingagin/npc_gzip/issues/3">GitHub</a>.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>This is a really interesting approach, and while it may not be perfect, it provides much food for thought.</p><p>To get hands-on with the concepts discussed in this blog, you can check out <a href="https://github.com/BasedLukas/zip_classification">this repository that I made</a>. It provides a simple implementation of LZ77 and Huffman encoding, allowing you to experiment with these algorithms and see how they work in practice.</p><p>For a more in-depth understanding of gzip, consider reading <a href="https://www.infinitepartitions.com/art001.html">this blog post</a>. <a href="https://www.zlib.net/feldspar.html">This article</a> offers an excellent explanation of the LZ77 algorithm. Of course, you should read the original paper, available <a href="https://aclanthology.org/2023.findings-acl.426/">here</a>, which is very well written and understandable.</p>]]></content>
    
    
      
      
        
        
    <summary type="html"></summary>
        
      
    
    
    
    
    <category term="python" scheme="http://loreley.one/tags/python/"/>
    
    <category term="models" scheme="http://loreley.one/tags/models/"/>
    
    <category term="NLP" scheme="http://loreley.one/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Designing a Simple 8 Bit CPU Emulator in Python</title>
    <link href="http://loreley.one/2023-07-cpu/"/>
    <id>http://loreley.one/2023-07-cpu/</id>
    <published>2023-07-03T22:00:00.000Z</published>
    <updated>2025-11-05T00:10:42.646Z</updated>
    
    <content type="html"><![CDATA[<p><em>(2024-12-07) Note: See <a href="https://loreley.one/2024-12-wasm/">this blogpost</a> of mine to interact with the CPU and solve the maze.</em></p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>A while back, I came across an interesting game on Steam called <a href="https://turingcomplete.game/">Turing Complete</a>. The objective of the game is to design a Turing complete CPU from scratch, and it guides players through each step of the process. Starting with the basics of logic gates, the building blocks of a CPU, players tackle progressively more complex challenges until they reach the point of designing their own CPU capable of performing simple tasks. I found the game captivating and spent an unhealthy amount of time in it.</p><center><img src="turing_complete.jpg" alt="turing_complete"><p><small>Turing Complete gameplay</small></p></center><p>After completing the game, I decided it would be fun to create a Python emulator for a similar CPU. While it’s relatively easy to make a basic emulator in Python due to its high-level nature, I chose a different approach. I wanted to build the CPU entirely from scratch, deriving the logic from the fundamental functionality of logic gates. This means that the CPU’s performance is determined solely by simple true&#x2F;false comparisons, without relying on if statements or other control flows. It does make the code a bit harder to read at times, but by merely changing the function of a single logic gate, you can witness how the entire CPU’s behavior is affected (spoiler alert: it stops working).</p><h2 id="Logic-Gates"><a href="#Logic-Gates" class="headerlink" title="Logic Gates"></a>Logic Gates</h2><p>The design starts with the logic gates. Here is an excerpt of the code</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">and_</span>(<span class="params">*args:<span class="built_in">bool</span></span>)-&gt;<span class="built_in">int</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">int</span>(<span class="built_in">all</span>(args))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">or_</span>(<span class="params">*args:<span class="built_in">bool</span></span>)-&gt;<span class="built_in">int</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">int</span>(<span class="built_in">any</span>(args))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">not_</span>(<span class="params">*args:<span class="built_in">bool</span></span>)-&gt;<span class="built_in">int</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">int</span>(<span class="keyword">not</span> <span class="built_in">all</span>(args))</span><br></pre></td></tr></table></figure><p>The logic gates perform simple comparisons and return true or false values. For example the AND gate returns true only if both of its inputs are true. The OR gate returns true if either (or both) of its inputs are true.</p><center><img src="logic_gates.jpg" alt="logic gates"><p><small>logic gate symbols</small></p></center><h2 id="Basic-Components"><a href="#Basic-Components" class="headerlink" title="Basic Components"></a>Basic Components</h2><p>Once we have our logic gates, we can utilize them to create essential components. Let’s focus on the full adder as an example. The purpose of a full adder is to take two binary inputs and perform addition on them. For instance, when adding 0 + 0, the output is 0. When adding 1 + 0, the output is 1.</p><p>However, adding 1 + 1 poses a challenge. In binary, we lack a way to represent the number 2 directly. To address this, we introduce a carry bit, similar to carrying over a digit when adding two decimal numbers that exceed 9. Additionally, we have a carry input to account for any carry bits from previous additions. </p><center><img src="full_adder.png" alt="full adder"><p><small>full adder design</small></p></center><p>A truth table is a structured table that presents the outputs corresponding to all possible input combinations. It allows us to systematically analyze the behavior of a component or system under consideration. In the case of a full adder, the truth table would display the sum output and carry output for each potential combination of binary inputs and carry input. By checking the truth table, we can see how the full adder operates under different input scenarios.</p><center><img src="full_adder_truth.png" alt="full adder truth table"><p><small>full adder truth table</small></p></center><p>The above design can now be implemented in code, using the logic gates we have created previously. In this case I first made a half adder (which doesn’t have a carry in) and used two of them to construct a full adder. Notice how all the logic is dependent on the operation of the logic gates.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">half_adder</span>(<span class="params">input1:<span class="built_in">bool</span>, input2:<span class="built_in">bool</span></span>):</span><br><span class="line">    <span class="keyword">return</span> sn(</span><br><span class="line">        <span class="built_in">sum</span>=xor(input1, input2),</span><br><span class="line">        carry=and_(input1, input2)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">full_adder</span>(<span class="params">input1:<span class="built_in">bool</span>, input2:<span class="built_in">bool</span>, carry_in:<span class="built_in">bool</span></span>):</span><br><span class="line">    half_adder1 = half_adder(input1, input2)</span><br><span class="line">    half_adder2 = half_adder(half_adder1.<span class="built_in">sum</span>, carry_in)</span><br><span class="line">    out = sn(</span><br><span class="line">        <span class="built_in">sum</span>=half_adder2.<span class="built_in">sum</span>,</span><br><span class="line">        carry=or_(half_adder1.carry, half_adder2.carry)</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><p>After creating a number of basic components such as these, we create more complex components such as an ALU (Arithmetic Logic Unit). The task of the ALU is to perform simple mathematical operations, such as adding 2 numbers together. In this case the ALU can only perform 4 simple operations; Addition, Subtraction, and two logical operations AND and OR. I took inspiration from this 32 bit ALU design, although mine only has 8 bits.</p><center><img src="ALU.png" alt="alu"><p><small>32 bit ALU design</small></p></center><h1 id="The-Full-CPU"><a href="#The-Full-CPU" class="headerlink" title="The Full CPU"></a>The Full CPU</h1><p>The design of the CPU includes six registers, an input, and an output, along with an ALU. Additionally, there is a unit that can carry out comparisons between a value and zero. The CPU operates by executing one of four distinct types of operations:</p><ul><li><p>Immediate: This operation is for moving a given value into register 0. </p></li><li><p>Arithmetic: The operands for any arithmetic operation are always register 1 and register 2, and the resultant output is stored in register 3.</p></li><li><p>Copy: This operation is used for duplicating values from one register to another or to the output. For example, ‘copy 0 6’ copies from register 0 to the output, and ‘copy 5 3’ copies from register 5 to register 3.</p></li><li><p>Evaluation: This operation evaluates the value in register 3 against zero. If the condition is true, it sets the program counter to the value in register 0.</p></li></ul><center><img src="full_cpu.jpeg" alt="full cpu"><p><small>CPU design</small></p></center><p>Using just these operations we have already created a turing complete CPU! However writing code for it would be challenging as we would have to write everything in binary. To solve this I made a simple assembler that converts assembly code to binary. It also adds a number of useful features such as labels. Labels allow you to mark a point in your program to jump to, making it much easier to create loops in your code. Let’s take a look at a simple program. </p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># create a marker at the beginning of the program where we can jump to</span><br><span class="line">label start </span><br><span class="line"></span><br><span class="line"># read from input (reg6) into reg1</span><br><span class="line">copy 6 1</span><br><span class="line"></span><br><span class="line"># add reg 1 and 2, store the result in reg3</span><br><span class="line">add</span><br><span class="line"></span><br><span class="line">#copy result from reg3 into reg2</span><br><span class="line">copy 3 2</span><br><span class="line"></span><br><span class="line"># loop back to start if the value is not negative</span><br><span class="line">start</span><br><span class="line">eval &gt;=</span><br><span class="line"></span><br><span class="line"># Once the value overflows and becomes negative send the result to the output</span><br><span class="line">copy 3 6</span><br></pre></td></tr></table></figure><p>In the simple program above, the value 1 is continuously being fed into the CPU. At each iteration we add it to the value stored in register 2, so that we increment the value by 1. Eventually the value grows large enough that it overflows and becomes negative. This is because in binary the most significant bit (MSB) is used to represent negative numbers. Once this happens the program stops running.</p><h1 id="Solve-the-Maze"><a href="#Solve-the-Maze" class="headerlink" title="Solve the Maze"></a>Solve the Maze</h1><p>Now that we have a working CPU, we need something to do with it. So I designed a simple maze that a robot needs to navigate through. The robot can only see one square ahead, and is controlled by the CPU. In order to solve the maze, I implemented a simple algorithm - following the wall. Using this technique the robot is able to find its way through the maze.</p><center><img src="maze.gif" alt="maze"><p><small>Robot solving maze</small></p></center><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># robot recognizes these commands:</span><br><span class="line"># 1 = turn left </span><br><span class="line"># 2 = turn right </span><br><span class="line"># 3 = step forward</span><br><span class="line"># To execute a command send the corresponding value to the output (reg 6)</span><br><span class="line"># robot inputs (reg 6) to CPU:</span><br><span class="line"># 1 = wall</span><br><span class="line"># 0 = clear</span><br><span class="line"></span><br><span class="line">start</span><br><span class="line">eval always</span><br><span class="line"></span><br><span class="line">### routines ###</span><br><span class="line"></span><br><span class="line">label uturn</span><br><span class="line">2</span><br><span class="line">copy 0 6</span><br><span class="line">copy 0 6</span><br><span class="line">start</span><br><span class="line">eval always</span><br><span class="line"></span><br><span class="line">label right </span><br><span class="line">2</span><br><span class="line">copy 0 6</span><br><span class="line">start</span><br><span class="line">eval always</span><br><span class="line"></span><br><span class="line">label left</span><br><span class="line">1</span><br><span class="line">copy 0 6</span><br><span class="line">start</span><br><span class="line">eval always</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### start main loop ###</span><br><span class="line"></span><br><span class="line"># take a step forwards</span><br><span class="line">label start</span><br><span class="line">3</span><br><span class="line">copy 0 6</span><br><span class="line"></span><br><span class="line"># check if wall to the left store in 3</span><br><span class="line">1</span><br><span class="line">copy 0 6</span><br><span class="line">copy 6 3</span><br><span class="line">2           # turn back to original direction</span><br><span class="line">copy 0 6</span><br><span class="line"></span><br><span class="line"># if no wall left, turn left until there is a wall. We follow wall on our left side</span><br><span class="line">left </span><br><span class="line">eval =</span><br><span class="line"></span><br><span class="line"># there is a wall left, check ahead</span><br><span class="line">copy 6 1</span><br><span class="line"># check right</span><br><span class="line">2</span><br><span class="line">copy 0 6</span><br><span class="line">copy 6 2</span><br><span class="line">1 </span><br><span class="line">copy 0 6</span><br><span class="line"># if wall ahead and right do a uturn</span><br><span class="line">and</span><br><span class="line">uturn</span><br><span class="line">eval !=</span><br><span class="line"></span><br><span class="line"># if wall ahead but no wall right turn right</span><br><span class="line">copy 1 3</span><br><span class="line">right</span><br><span class="line">eval !=</span><br><span class="line"></span><br><span class="line">start</span><br><span class="line">eval always </span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><center><img src="norway.jpeg" alt="NORway"><p><small>NOR-way</small></p></center><p>I thought this was quite a fun project, I certainly learnt a lot. While it may seem a little complicated, the truth is that by playing Turing Complete, you learn in a very intuitive manner. I can highly recommend it. If you want to write your own program or take a look at the code, it is all available on my <a href="https://github.com/BasedLukas/cpu_simulator">github</a>.</p>]]></content>
    
    
      
      
        
        
    <summary type="html"></summary>
        
      
    
    
    
    
    <category term="python" scheme="http://loreley.one/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Waltzing With Chance</title>
    <link href="http://loreley.one/2023-06-monte_carlo/"/>
    <id>http://loreley.one/2023-06-monte_carlo/</id>
    <published>2023-06-30T22:00:00.000Z</published>
    <updated>2025-11-05T00:10:42.657Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Suppose you were flying over the Pacific and suffered a plane crash. Luckily you survived the crash and are now stranded on a deserted island. Unfortunately, you struck your head quite badly and can no longer recall the value for pi. All you have with you is a short stick, a stone and your wits to guide you. Naturally you are concerned with the most pressing problem, viz. calculating the value of pi, which you have forgotten. How can you go about doing so?<br>Luckily for you, there exists a solution to this problem – the Monte Carlo simulation.</p><h2 id="The-Setup"><a href="#The-Setup" class="headerlink" title="The Setup"></a>The Setup</h2><p>Lets start by creating a square with sides the length of the stick “r” in the sand. Next create a circle with a radius of “r”. We can make the following statements;</p><p>The area of the circle &#x3D;  <code>Π × r²</code></p><p>The area of the square &#x3D;  <code>r²</code></p><p>Thus;</p><p>area of the circle &#x2F; area of the square &#x3D;</p><p><code>Π × r² / r²  = Π</code></p><p>All we have to do is calculate the area of the circle and the square and we can calculate pi. Unfortunately, that is quite difficult to do. Cue the Monte Carlo simulation.</p><h2 id="The-Simulation"><a href="#The-Simulation" class="headerlink" title="The Simulation"></a>The Simulation</h2><center><img src="setup.png" alt="setup"> </center> <p>Steps:</p><ul><li>Take the stone</li><li>Throw it up</li><li>Each time the stone lands make a note of whether it landed in the circle or square.</li></ul><p>Suppose you did this a few thousand times. Your final results would look something like this;</p><p>circle &#x3D;  <code>1591</code><br>square &#x3D;  <code>491</code></p><p>You can use the number of times that the stone fell inside the shape as a proxy for the area of the shape. This is the heart of a Monte Carlo simulation – using a random process to estimate some value. You can now calculate pi by simply dividing the one by the other. </p><p>To clarify, what we are doing here is estimating the area of a shape. This works because the probability that the stone will fall inside the shape is equal to the area of the shape.</p><h2 id="Simulation-in-Python"><a href="#Simulation-in-Python" class="headerlink" title="Simulation in Python"></a>Simulation in Python</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="comment"># Number of points that landed in the circle or square</span></span><br><span class="line">in_circle = <span class="number">0</span></span><br><span class="line">in_square = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Number of iterations</span></span><br><span class="line">iterations = <span class="number">10000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># the radius of the circle and side of the square</span></span><br><span class="line">r = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iterations):</span><br><span class="line">    <span class="comment"># randomly select 2 points between 0-20</span></span><br><span class="line">    x = random.uniform(<span class="number">0</span>, <span class="number">20</span>)</span><br><span class="line">    y = random.uniform(<span class="number">0</span>, <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># check if point (x,y) is in square (x &lt; r and y &lt; r) (bottom left corner of square is at the origin)</span></span><br><span class="line">    <span class="keyword">if</span> x &lt; r <span class="keyword">and</span> y &lt; r:</span><br><span class="line">        in_square += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># calculate the distance between point(x,y) and center of circle (r,r)</span></span><br><span class="line">    distance = math.sqrt((r - x)**<span class="number">2</span> + (r - y)**<span class="number">2</span>)</span><br><span class="line">    <span class="comment"># check if point (x, y) is in circle.</span></span><br><span class="line">    <span class="keyword">if</span> distance &lt; r:</span><br><span class="line">        in_circle += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            pi = in_circle / in_square</span><br><span class="line">        <span class="keyword">except</span> ZeroDivisionError:</span><br><span class="line">            pi = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Estimated value of pi after&#x27;</span>,i,<span class="string">&#x27;iterations is&#x27;</span>,pi)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Estimated value of pi after 0 iterations is 0</span><br><span class="line">Estimated value of pi after 1000 iterations is 2.7627737226277373</span><br><span class="line">Estimated value of pi after 2000 iterations is 2.911985018726592</span><br><span class="line">Estimated value of pi after 3000 iterations is 2.8870967741935485</span><br><span class="line">Estimated value of pi after 4000 iterations is 2.890334572490706</span><br><span class="line">Estimated value of pi after 5000 iterations is 2.951367781155015</span><br><span class="line">Estimated value of pi after 6000 iterations is 2.996168582375479</span><br><span class="line">Estimated value of pi after 7000 iterations is 3.0482796892341844</span><br><span class="line">Estimated value of pi after 8000 iterations is 3.040658276863504</span><br><span class="line">Estimated value of pi after 9000 iterations is 3.0583657587548636</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">Estimated value of pi after 1999000000 iterations is 3.1414178257741354</span><br></pre></td></tr></table></figure><p>Here is a plot of the points. The points that landed in the circle are red and orange. Those that landed in the square are blue and orange, and those that landed in neither are green. (Note that it makes no difference if the circle overlaps with the square, we merely want to estimate the area of each shape.)</p><center> <img src="pi.png" alt="pi"></center><p>And now for a graph of the estimated value of pi by iteration. This graph nicely illustrates the law of large numbers. In the beginning our estimate is wildly off. Over time it converges toward pi.</p><center> <img src="estimation.png" alt="setup"></center><h2 id="Snakes-and-ladders"><a href="#Snakes-and-ladders" class="headerlink" title="Snakes and ladders"></a>Snakes and ladders</h2><p>Suppose you bet on a game of snakes and ladders against somebody. Your opponent keeps winning and you suspect him of cheating. You want to calculate the probability of him finishing the game in a certain number of moves or less (you cannot remember what his dice rolls were, only the number of turns it took him to win). In order to do this you need to create a distribution of the number of moves it takes to finish the game.</p><p>The game works as follows;</p><ul><li>The players start at position 0</li><li>Each player rolls a 6 sided die</li><li>The player moves forward by the number rolled.</li><li>If the landing square is the base of a ladder, the player is transported to the top.</li><li>If the landing square is the top of a snake, the player is transported to the bottom.</li></ul><p>Calculating the distribution of the number of rolls it takes to complete the game could be quite complex. (This is called a Markov chain and can be solved algebraically). Instead we can simulate the game by randomly generating dice rolls and counting how many rolls it takes to complete the game. Doing this multiple times gives us the distribution.</p><h2 id="Simulation-in-Python-1"><a href="#Simulation-in-Python-1" class="headerlink" title="Simulation in Python"></a>Simulation in Python</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment">#Number of iterations to simulate</span></span><br><span class="line">iterations = <span class="number">100000</span></span><br><span class="line"></span><br><span class="line">game = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">51</span>)]</span><br><span class="line"><span class="comment">#ladders</span></span><br><span class="line">game[<span class="number">2</span>] = <span class="number">15</span></span><br><span class="line">game[<span class="number">5</span>] = <span class="number">17</span></span><br><span class="line">game[<span class="number">9</span>] = <span class="number">27</span></span><br><span class="line">game[<span class="number">17</span>] = <span class="number">41</span></span><br><span class="line">game[<span class="number">25</span>] = <span class="number">35</span></span><br><span class="line">game[<span class="number">34</span>] = <span class="number">48</span></span><br><span class="line"><span class="comment">#snakes</span></span><br><span class="line">game[<span class="number">33</span>] = <span class="number">20</span></span><br><span class="line">game[<span class="number">30</span>] = <span class="number">10</span></span><br><span class="line">game[<span class="number">20</span>] = <span class="number">1</span></span><br><span class="line">game[<span class="number">49</span>] = <span class="number">36</span></span><br><span class="line">game[<span class="number">18</span>] = <span class="number">8</span></span><br><span class="line">game[<span class="number">40</span>] = <span class="number">6</span></span><br><span class="line">game[<span class="number">26</span>] = <span class="number">19</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># the number of rolls it takes to win the game</span></span><br><span class="line">rolls = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">simulate_game</span>():</span><br><span class="line">    <span class="comment"># start at square 0</span></span><br><span class="line">    player1 = <span class="number">0</span></span><br><span class="line">    number_of_rolls = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="comment"># simulate a roll of the die</span></span><br><span class="line">        roll = random.randint(<span class="number">1</span>,<span class="number">6</span>)</span><br><span class="line">        <span class="comment"># count how many rolls the player has taken</span></span><br><span class="line">        number_of_rolls += <span class="number">1</span></span><br><span class="line">        <span class="comment"># move forward by the number rolled</span></span><br><span class="line">        player1 += roll</span><br><span class="line">        <span class="comment"># Stop once the player reaches square 50</span></span><br><span class="line">        <span class="keyword">if</span> player1 &gt;= <span class="number">50</span>:</span><br><span class="line">            <span class="keyword">return</span> number_of_rolls</span><br><span class="line">        <span class="comment"># if the player lands on a snake or ladder, move accordingly</span></span><br><span class="line">        player1 = game[player1]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iterations):</span><br><span class="line">    rolls.append(simulate_game())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#plot distribution of rolls</span></span><br><span class="line">plt.hist(rolls, bins=<span class="number">100</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><center> <img src="results.png" alt="results"></center><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Monte Carlo simulations are a simple but powerful tool for estimating unknown values. They are especially helpful if you don’t know probability theory (like me) and just want to know the odds of some event occurring. <a href="https://library.lanl.gov/cgi-bin/getfile?00326866.pdf">This article</a> (PDF) talks about how the first Monte Carlo simulations were used in the Manhattan project. If you would like to learn more about Markov chains, <a href="https://math.libretexts.org/Bookshelves/Applied_Mathematics/Applied_Finite_Mathematics_%28Sekhon_and_Bloom%29/10%3A_Markov_Chains/10.01%3A_Introduction_to_Markov_Chains">this article</a> is a good place to start. Creating simple simulations in python is relatively straightforward and fun.<br>I hope you get rescued from that island soon!</p>]]></content>
    
    
      
      
        
        
    <summary type="html"></summary>
        
      
    
    
    
    
    <category term="python" scheme="http://loreley.one/tags/python/"/>
    
    <category term="probability" scheme="http://loreley.one/tags/probability/"/>
    
  </entry>
  
  <entry>
    <title>Text Vectorization</title>
    <link href="http://loreley.one/2023-03-embeddings/"/>
    <id>http://loreley.one/2023-03-embeddings/</id>
    <published>2023-03-21T23:00:00.000Z</published>
    <updated>2025-11-05T00:10:42.653Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Introduction-to-Embeddings"><a href="#Introduction-to-Embeddings" class="headerlink" title="Introduction to Embeddings"></a>Introduction to Embeddings</h2><p>Embeddings play a crucial role in natural language processing (NLP) and text analysis. Simply put, word embeddings represent words or phrases as vectors, which are lists of numbers. These vectors help encode the meaning of words so that similar words or phrases have closer vector representations. For example, the sentences “The cat quickly climbed the tree” and “The feline swiftly ascended the tree” have different words but similar meanings. Embeddings allow us to capture this similarity in meaning by placing these phrases close together in the embedding space. Embeddings make it more efficient to perform machine learning on large batches of text. By translating high-dimensional data (text) into a lower-dimensional space (vectors), embeddings efficiently capture the semantics&#x2F;meaning of inputs, which can be useful for tasks such as text search.</p><h2 id="An-Example"><a href="#An-Example" class="headerlink" title="An Example"></a>An Example</h2><p>Let’s consider a simple example of using vectors to cluster movies based on their characteristics. Imagine arranging a set of movies on a one-dimensional number line, where movies that are more closely related are placed closer together. This arrangement might represent movies based on their appeal to different age groups, such as children versus adults. -1 would represent a children’s movie and 1 would represent an adult movie. Each movie could be given a score based on its audience’s age group.  </p><p>However, there are other aspects, like genre, that can also contribute to the similarity between movies. To capture this, we can extend the arrangement to a two-dimensional space, where one dimension represents the age group and the other represents the genre. For instance, a movie could be represented as a vector (0.3, 0.2), where 0.3 corresponds to the age group, and 0.2 corresponds to the genre. This embedding helps us identify and group movies based on their appeal to different age groups and genres.</p><center><img src="embedding2d.jpg" alt="embedding2d"><p><small><a href="https://developers.google.com/machine-learning/crash-course/embeddings/motivation-from-collaborative-filtering">source</a></small></p></center><h2 id="Generalising-to-Higher-Dimensions"><a href="#Generalising-to-Higher-Dimensions" class="headerlink" title="Generalising to Higher Dimensions"></a>Generalising to Higher Dimensions</h2><p>As we’ve seen with the simple movie clustering example, we can represent data points in multi-dimensional spaces to capture various characteristics. The same concept can be applied to more complex data, such as geographical locations or even natural language text.</p><center><img src="linear-relationships.jpg" alt="linear-relationships"><p><small><a href="https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space">source</a></small></p></center><p>In the image above, capital cities are positioned close to their respective countries. In this space, not only are capital cities near their countries, but similar countries are also close to one another but along a different dimension. This arrangement effectively captures the relationships between countries and their capitals.</p><p>Generalising this concept to even higher dimensions allows us to represent more complex information, such as text. In this case, each dimension represents a different concept or aspect of the text, which enables us to encode an entire paragraph or a block of text as a single vector. By representing text in this manner, we can capture the underlying semantic meaning and relationships between words, phrases, or even entire paragraphs.</p><h2 id="Using-Vectors"><a href="#Using-Vectors" class="headerlink" title="Using Vectors"></a>Using Vectors</h2><p>To determine if two vectors are close to one another, we can use the cosine similarity. The cosine similarity measures the cosine of the angle between the two vectors, resulting in a value between -1 and 1. A value closer to 1 indicates a higher similarity, while a value closer to -1 indicates a lower similarity.</p><p>One example of using vectors in text analysis is classifying reviews as positive or negative. In this case, we can represent each review as a vector and train a machine learning model to classify them based on their vector representations. The model could learn to classify reviews as positive or negative based on the similarity between the review vectors and vectors representing positive or negative sentiment.</p><p>Another example is searching within a large body of text. In this scenario, we can break the text into smaller chunks or segments, each represented as a vector. When a user submits a search query, we convert the query into a vector in the same embedding space. To find the most relevant match, we measure the similarity between the search query vector and the vectors representing the chunks of text using cosine similarity. The chunk with the highest similarity to the search query vector is considered the best match for the user’s query.</p><h2 id="Trying-it-out"><a href="#Trying-it-out" class="headerlink" title="Trying it out"></a>Trying it out</h2><p>Generating or computing word vectors involves training a machine learning model on a large corpus of text. The model learns to represent words or phrases as vectors based on the context in which they appear, capturing semantic meaning and relationships between words. Take a look at this example of 3 vectors constructed using the OpenAI Ada-2 model (input 2 says ”I really like running” in Dutch). </p><center><img src="python_embeddings.png" alt="python embeddings"></center><p>When we compute the cosine similarity between them this is what we get;</p><center><img src="python_embeddings2.png" alt="python embeddings"></center><p>The two sentences about running are very close together, even though the languages are different. The cat sentence is farther apart. Now look at what happens if we compare the sentence “I enjoy jogging” with “I do not enjoy jogging”.</p><center><img src="python_embeddings3.png" alt="python embeddings"></center><p>They are really close together! Closer than the English – Dutch pair. Can you figure out why?</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>In this article we have learned what embeddings are, and seen how they can be used for various natural language processing tasks. If you found this interesting you can read more in depth <a href="http://www.offconvex.org/2015/12/12/word-embeddings-1/">here</a> or <a href="https://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html/">here</a>. </p>]]></content>
    
    
      
      
        
        
    <summary type="html"></summary>
        
      
    
    
    
    
    <category term="python" scheme="http://loreley.one/tags/python/"/>
    
    <category term="models" scheme="http://loreley.one/tags/models/"/>
    
    <category term="NLP" scheme="http://loreley.one/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Therapeutic Plasma Exchange</title>
    <link href="http://loreley.one/2022-06-tpe/"/>
    <id>http://loreley.one/2022-06-tpe/</id>
    <published>2022-06-01T22:00:00.000Z</published>
    <updated>2025-11-05T00:10:42.688Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Why-do-we-age"><a href="#1-Why-do-we-age" class="headerlink" title="1. Why do we age?"></a>1. Why do we age?</h2><p>When you are injured your body can heal itself. So why do people age? Why can’t they continually repair themselves and remain eternally youthful? While it’s generally agreed that aging is the effect of systemic degradation on the body’s ability to repair itself, what causes this degradation is less clear. To answer this let’s look at parabiosis.</p><h4 id="A-background-on-parabiosis"><a href="#A-background-on-parabiosis" class="headerlink" title="A background on parabiosis"></a>A background on parabiosis</h4><p>Parabiosis is a surgical procedure where two animals’ circulatory systems are stitched together. Why would you want to do that? Well, if you attach a younger mouse to an older one (heterochronic parabiosis) the older mouse grows younger, while the younger mouse actually ages! Importantly, the cells of the older mice actually regenerate - they are able to repair themselves, something older cells typically can’t do.</p><p>The above-mentioned experiment was conducted by the Conboys, a couple researching aging at Berkley, back in 2005. To quote from their paper:</p><blockquote><p>   “Our experiments suggest… … that the systemic environment of a young animal is one that promotes successful regeneration, whereas that of an older animal either fails to promote or actively inhibits successful tissue regeneration… …Our studies also demonstrate that the decline of tissue regenerative potential with age can be reversed through the modulation of systemic factors, suggesting that tissues specific stem and progenitor cells retain much of their intrinsic proliferative potential even when old, but that age-related changes in the systemic environment and niche in which progenitor cells reside preclude full activation of these cells for productive tissue regeneration.” <a href="https://www.nature.com/articles/nature03260">https://www.nature.com/articles/nature03260</a></p></blockquote><p>The fact that an older mouse’s tissue can regenerate is important. It is a clear indication that the mouse’s cells retain the ability to regenerate, given the correct environment. Furthermore, young mice, given a bad environment suddenly grow old and lose some of their ability to repair themselves. This means that it’s something environmental (systemic milieu) that’s causing aging to happen.</p><h4 id="Isolating-the-cause"><a href="#Isolating-the-cause" class="headerlink" title="Isolating the cause"></a>Isolating the cause</h4><p>The problem with this experiment is that parabiosis is complex. There are many factors that could be responsible for influencing the systemic milieu other than just the blood (shared organs etc). In order to verify that it is the blood, and not something else causing the regeneration, we need to conduct a better experiment.</p><h2 id="2-An-improved-experiment"><a href="#2-An-improved-experiment" class="headerlink" title="2. An improved experiment"></a>2. An improved experiment</h2><p>The Conboys developed a new system. One that would allow them to exchange blood between two mice without having to stitch them together. This isn’t as easy as it sounds.</p><blockquote><p>“We developed a blood exchange system where animals are connected and disconnected at will, removing the influence of shared organs, adaptation to being joined and so on.” <a href="https://doi.org/10.1038/ncomms13363">https://doi.org/10.1038/ncomms13363</a></p></blockquote><p>Now it’s possible to see what happens when we give older mice blood transfusions from young mice, and vice versa. Do the effects seen in the previous experiment still take place? Let’s look at the effect on 3 different types of mouse cells when they received a heterochronous blood transfusion.</p><h4 id="Muscle-cells"><a href="#Muscle-cells" class="headerlink" title="Muscle cells:"></a>Muscle cells:</h4><blockquote><p>“These data extrapolate the findings obtained with heterochronic parabiosis, and establish that the beneficial effects of young blood for the regeneration of old muscle take place right away and without the contribution of young organ systems”</p></blockquote><p>Old muscle cells regenerate when supplied with young blood.</p><h4 id="Brain-Cells"><a href="#Brain-Cells" class="headerlink" title="Brain Cells:"></a>Brain Cells:</h4><blockquote><p>“One exchange of heterochronic blood severely decreased hippocampal neurogenesis in young mice, and surprisingly, there was no significant positive effect in the old mice that had been exchanged with young blood. Of note, these were the same old animals that showed improvement in muscle regeneration… …Muscle injury after blood exchange might add to the magnitude of the negative effects of old blood on young neurogenesis, and even without muscle injury, young hippocampal neurogenesis quickly declines after one old blood exchange.”</p></blockquote><p>Old blood damages young brain cells, but young blood doesn’t seem to improve old brain cells.</p><h4 id="Liver-cells"><a href="#Liver-cells" class="headerlink" title="Liver cells:"></a>Liver cells:</h4><blockquote><p>“…interestingly, transfusion with young blood somewhat reduced old liver adiposity, (while there was no significant increase in young liver adiposity). These results demonstrate that heterochronic blood exchange and heterochronic parabiosis yield similar enhancement of old hepatogenesis and decline of young hepatogenesis…. …Additionally, the fibrotic regions and adiposity rapidly decline in old livers after the exposure to young blood. Such effects manifest after just a single procedure of blood exchange and in the absence of the influences from heterochronic organ systems.”</p></blockquote><center><img src="liver_adiposity.webp" alt="liver_adiposity"><p><small>Liver Adiposity showing the effects of different blood on old and young mice.</small></p></center><h4 id="To-summarize"><a href="#To-summarize" class="headerlink" title="To summarize:"></a>To summarize:</h4><ul><li>The effects of parabiosis hold even with only a single blood transfusion.</li><li>Young blood appears to have a positive impact on old mice.</li><li>Old blood appears to have a negative impact on young mice.</li><li>Regeneration in old mice could be due to positive factors in the young blood or due to negative factors in the old blood being diluted.</li></ul><h2 id="3-It’s-not-the-blood"><a href="#3-It’s-not-the-blood" class="headerlink" title="3. It’s not the blood"></a>3. It’s not the blood</h2><p>At this point, it’s important not to get carried away. The media immediately focused on the vampirism aspect and startups touted transfusion treatments, generating hysterical headlines and an FDA-issued <a href="https://www.fda.gov/news-events/press-announcements/statement-fda-commissioner-scott-gottlieb-md-and-director-fdas-center-biologics-evaluation-and-0">warning</a>. </p><h4 id="It’s-the-old-blood-we-should-focus-on"><a href="#It’s-the-old-blood-we-should-focus-on" class="headerlink" title="It’s the old blood we should focus on"></a>It’s the old blood we should focus on</h4><p>Maybe there’s nothing special about young blood. Maybe there’s something in old blood that’s inhibiting cell regeneration. When you switch blood between two mice, what you are actually doing is removing the harmful blood from the old mouse and giving it to the young mouse. This would explain why the younger mouse ages! And it means that simply removing harmful factors from old blood could cause regeneration!</p><blockquote><p>   “It was not formally established that young blood is necessary for this multi-tissue rejuvenation. …we replaced half of the plasma in mice with saline containing 5% albumin (terming it a “neutral” age blood exchange, NBE) thus diluting the plasma factors and replenishing the albumin that would be diminished if only saline was used.”</p></blockquote><blockquote><p>   “Our data demonstrate that a single NBE suffices to meet or exceed the rejuvenative effects of enhancing muscle repair, reducing liver adiposity and fibrosis, and increasing hippocampal neurogenesis in old mice, all the key outcomes seen after blood heterochronicity.” <a href="https://doi.org/10.18632/aging.103418">https://doi.org/10.18632/aging.103418</a></p></blockquote><p>They removed some of the mouse’s plasma (a component of blood), and replaced it with an albumin solution and still got positive results! There’s no need for young blood, all you need to do is remove negative factors from the old blood!</p><blockquote><p>“Summarily, these results establish broad tissues rejuvenation by a single replacement of old blood plasma with physiologic fluid: muscle repair was improved, fibrosis was attenuated, and inhibition of myogenic proliferation was switched to enhancement; liver adiposity and fibrosis were reduced; and hippocampal neurogenesis was increased. This rejuvenation is similar to (liver) or is stronger than (muscle and brain) that seen after heterochronic parabiosis or blood exchange.” </p></blockquote><blockquote><p>“These findings are most consistent with the conclusion that the age-altered systemic milieu inhibits the health and repair of multiple tissues in the old mice, and also exerts a dominant progeric effect on the young partners in parabiosis or blood exchange.”</p></blockquote><h4 id="Dilution-of-old-blood-causes-rejuvenation"><a href="#Dilution-of-old-blood-causes-rejuvenation" class="headerlink" title="Dilution of old blood causes rejuvenation"></a>Dilution of old blood causes rejuvenation</h4><p>So actually, just replacing the plasma from old blood with an albumin solution seems to provide all the benefits of young blood! It seems that plasma factors in the blood of old animals are somehow inhibiting cell regeneration – causing aging. Once these harmful factors are removed from the blood, the cells are able to regenerate, making them young again!</p><h2 id="4-Treatments-in-humans"><a href="#4-Treatments-in-humans" class="headerlink" title="4. Treatments in humans"></a>4. Treatments in humans</h2><p>This is all good and well, but I’m no mouse. How can we benefit? Well, the good news is that a treatment called TPE already exists. TPE; therapeutic plasma exchange is similar to the NBE that was tried in mice. It’s used to treat various conditions and is FDA approved.</p><p>Here’s how it works. Blood is drawn, and the plasma fraction separated and removed. The blood cells are then resuspended in an albumin solution and returned to the patient. This allows us to achieve the plasma dilution necessary to remove the harmful factors from the blood. All that’s being done is that the blood is being “cleaned” of the harmful factors in the plasma that have been shown to contribute to aging, no young blood is needed at all.</p><center><img src="effect_of_tpe.webp" alt="effect_of_tpe"><p><small>Effects of TPE on the blood. https://doi.org/10.1016/j.transci.2021.103162 </small></p></center><h4 id="Moving-forward"><a href="#Moving-forward" class="headerlink" title="Moving forward"></a>Moving forward</h4><p>Currently, trials are underway to better understand the effects of TPE. One interesting insight we already have is that not only does TPE reduce the levels of certain factors in the blood but it also raises the levels of other proteins such as EPO (erythropoietin). This suggests that the factors removed from the blood have some sort of inhibitory effect and that once they’re removed certain proteins are better expressed! </p><center><img src="protein_levels.webp" alt="protein_levels"><p><small>https://doi.org/10.1016/j.transci.2021.103162 </small></p></center><blockquote><p>   “Because repositioning TPE as a rejuvenative therapeutic is a relatively new concept, there are many unexplored questions regarding its potential and utility. Our recent 2020 studies demonstrated rejuvenation of three key tissues –muscle, liver and brain, as well as improved cognition and shortmemory in old mice – but other areas of health that decline withage are yet to be explored. Furthermore, it is unknown how long theserejuvenative effects persist. The health of the studied tissues is inter-estingly closer to the young than the old mammal (e.g. robustly reju-venated), but it is unknown if the rate of tissue health decay will be akinto a middle-aged mouse or if it will decline at a different rate. Perhaps,TPE will continue to stave off tissue decline for a longer period.Aging results in a near-endless list of systemic changes on tissue,cellular and molecular levels, and multiple methods of therapeutics willbe required to address these alterations. More research is clearly needed to develop and explore the applications of rejuvenative plasmapheresis alone or in combination with other therapeutics” doi.org&#x2F;10.1016&#x2F;j.transci.2021.103162</p></blockquote><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>We will have to await the results of further trials to see exactly how much TPE helps. But the results seem pretty robust thus far. The real advantage of this method is that it doesn’t require developing a new drug, it works by removing the harmful elements from the body. This is a major advantage in terms of getting treatments approved and ensuring their safety.</p><p>Let me know what you think, and tell me if I’ve missed something or made any mistakes. This is my attempt to summarize the research, but I certainly could’ve gotten something wrong.</p><p><a href="https://neo.life/2021/06/perspective-therapeutic-plasma-exchange-the-future-of-aging/">https://neo.life/2021/06/perspective-therapeutic-plasma-exchange-the-future-of-aging/ </a><br><a href="https://www.nature.com/articles/nature03260">https://www.nature.com/articles/nature03260 </a><br><a href="http://doi.org/10.1038/ncomms13363">http://doi.org/10.1038/ncomms13363 </a><br><a href="https://www.aging-us.com/article/103418/text">https://www.aging-us.com/article/103418/text </a><br><a href="https://doi.org/10.1016/j.transci.2021.103162">https://doi.org/10.1016/j.transci.2021.103162 </a></p>]]></content>
    
    
      
      
        
        
    <summary type="html"></summary>
        
      
    
    
    
    
    <category term="longevity" scheme="http://loreley.one/tags/longevity/"/>
    
  </entry>
  
</feed>
