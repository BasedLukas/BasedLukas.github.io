<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>

    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>

    <!--Description-->

    

    
        <meta name="description" content="My incoherent ramblings"/>
    

    <!--Author-->
    
        <meta name="author" content="Lukas"/>
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="Double Q-Learning Explained"/>
    

    <!--Open Graph Description-->
    
        <meta property="og:description" content="My incoherent ramblings"/>
    

    <!--Open Graph Site Name-->
        <meta property="og:site_name" content="Lukas&#39;s Blog"/>

    <!--Type page-->
    
        <meta property="og:type" content="article"/>
    

    <!--Page Cover-->
    
    
        <meta property="og:image" content="http://loreley.one//2024-03-double_q/double_q.png"/>
    

        <meta name="twitter:card" content="summary_large_image"/>

    

    
        <meta name="twitter:image" content="http://loreley.one//2024-03-double_q/double_q.png"/>
    

    <!-- Title -->
    
    <title>Double Q-Learning Explained - Lukas&#39;s Blog</title>

    <!-- Bootstrap Core CSS -->
    <link href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" rel="stylesheet"/>

    <!-- Custom CSS -->
    
<link rel="stylesheet" href="/css/style.css">


    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/>
    <link href="//fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css"/>
    <link href="//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css"/>

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="//oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="//oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Gallery -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.css" type="text/css" rel="stylesheet"/>

    <!-- Google Analytics -->
    


    <!-- favicon -->
    
        <link rel="icon" href="/img/favicon.ico"/>
        
    
<meta name="generator" content="Hexo 6.3.0"></head>


<body>

    <!-- Menu -->
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Home</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">
                
                    <li>
                        <a href="/">
                            
                                Home
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="/about">
                            
                                About
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="/links">
                            
                                Links
                            
                        </a>
                    </li>
                
                    <li>
                        <a target="_blank" rel="noopener" href="https://github.com/BasedLukas">
                            
                                <i class="fa fa-github fa-stack-2x"></i>
                            
                        </a>
                    </li>
                
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>

    <!-- Main Content -->
    <!-- Page Header -->
<!-- Set your background image for this header in your post front-matter: cover -->

<header class="intro-header" style="background-image: url('/img/bg.webp')">
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-1 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <h1>Double Q-Learning Explained</h1>
                    
                    <h2 class="post-subheading">
                        What is Maximization Bias and Double Q-learning?
                    </h2>
                    
                    <span class="meta">
                        <!-- Date and Author -->
                        
                            Posted by Lukas on
                        
                        
                            2024-03-16
                        
                    </span>
                </div>
            </div>
        </div>
    </div>
</header>

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

            <!-- Tags and categories -->
           
                <div class="col-lg-4 col-lg-offset-1 col-md-5 col-md-offset-1 post-tags">
                    
                        


<a href="/tags/python/">#python</a> <a href="/tags/reinforcement-learning/">#reinforcement learning</a> <a href="/tags/probability/">#probability</a>


                    
                </div>
                <div class="col-lg-4 col-md-5 post-categories">
                    
                </div>
            

            <!-- Gallery -->
            

            <!-- Post Main Content -->
            <div class="col-lg-10 col-lg-offset-1 col-md-10 col-md-offset-1">
                <h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Q-learning is a popular reinforcement learning algorithm, used for solving Markov Decision Processes (MDP). In some cases, Q-learning doesn’t work well and takes a long time to converge, due to an issue known as the optimizer’s curse or maximization bias. In this post, we’ll take a look at the problem as well as a proposed solution. What follows is a brief recapitulation of MDP’s and Q-learning, followed by a deep dive into Double Q learning, the proposed solution to the problem.</p>
<h2 id="Recap-of-Q-learning"><a href="#Recap-of-Q-learning" class="headerlink" title="Recap of Q learning"></a>Recap of Q learning</h2><p>A Markov chain consists of states connected through transition probabilities, which determine the likelihood of moving from one state to another. This probability depends only on the current state, not on the sequence of events that preceded it. Some states are accessible only from specific other states, forming a directed network.</p>
<p>A Markov Decision Process (MDP) extends the concept of a Markov chain by incorporating decisions. It substitutes transition probabilities with actions that represent available choices. Each state in an MDP is linked with a reward, indicating the value of reaching that state. The distinct feature of MDPs is the decision-making aspect, where an agent selects actions to transition between states and accumulate rewards. The goal in an MDP is to find an optimal policy, which is a set of rules defining the best action to take in each state to maximize rewards.</p>
<p>A trajectory through an MDP is represented using the notation: starting at state <code>S</code>, an action <code>A</code> is chosen from the available options in state <code>S</code>. This leads to a transition to state <code>S'</code> with probability <code>P</code>, and a reward <code>R</code> is received. The tuple <code>(S, A, P, R)</code> describes this process, where <code>P</code> is defined as <code>Pr(S' | S, A)</code>. This sequence repeats at each step until a terminal state is reached, outlining the full trajectory:</p>
<p><code>S<sub>0</sub>, A<sub>0</sub>, R<sub>1</sub>, S<sub>1</sub>, A<sub>1</sub>, R<sub>2</sub>, S<sub>2</sub>, A<sub>2</sub>, R<sub>3</sub>, ...</code></p>
<p>The Q(action, value) function under a policy <code>π</code> is formally defined as:</p>
<p><code>Q<sub>π</sub>(s, a) = E[G | s, a, π]</code></p>
<p>where <code>E[G | s, a, π]</code> represents the expected total (discounted) reward given that we start in state <code>s</code>, take action <code>a</code>, and then follow policy <code>π</code> for all subsequent decisions. This expectation accounts for the sum of rewards received, starting from <code>s</code> and <code>a</code>, under the guidance of policy <code>π</code>.</p>
<p>In Q-learning, the objective is to approximate the optimal Q function, which represents the best action values under an optimal policy, regardless of the initial policy used to generate training data. The policy generating our training data decides actions, which might not be optimal. Our aim is to iteratively refine our Q function based on these examples. The algorithm is as follows:</p>

<center>
<img src="q_algo.png" alt="q learning algorithm">
<p><small>Q-learning algorithm (Sutton and Barto)</small></p>
</center>


<p>For a full discussion of Q-learning I recommend the following 2 sources:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/learn/deep-rl-course/unit2/introduction">Hugging Face course on Q-learning</a>. This is a nice quick overview.</li>
<li>For a full treatment see the RL book by Sutton and Barto, which is available free <a target="_blank" rel="noopener" href="http://incompleteideas.net/book/the-book.html">here</a>.</li>
</ul>
<h2 id="Dissection-of-the-problem"><a href="#Dissection-of-the-problem" class="headerlink" title="Dissection of the problem"></a>Dissection of the problem</h2><p>A common issue with Q-learning involves how it handles variance in rewards. Consider an MDP where we start in state <code>A</code> with the options to move to <code>B</code> or directly to a terminal state <code>T</code>, neither transition offering any reward. From <code>B</code>, we can transition to several secondary states, <code>C<sub>1</sub>, C<sub>2</sub>, ..., C<sub>n</sub></code>, each associated with a reward from a normal distribution with a negative mean (e.g., -0.1) and a variance (e.g., 1). Transitioning from any <code>C<sub>n</sub></code> to <code>T</code> yields no reward. Ideally, the optimal strategy is to move from <code>A</code> to <code>T</code>, avoiding negative rewards in the <code>C</code> states. However, the stochastic nature of rewards means a visit to any <code>C</code> state might yield a positive reward. The likelihood of receiving a positive reward increases with the number of <code>C</code> states.</p>
<p>This variance introduces a challenge in Q-learning. The algorithm estimates the Q-value for transitioning from <code>A</code> to <code>B</code> based on the maximum reward obtainable from moving to any <code>C</code> state. Given the rewards are drawn from a distribution, it’s probable to encounter a positive reward in one of the <code>C</code> states during exploration. Consequently, the Q-function may overestimate the value of moving from A to B. Essentially, the “max” operation in the update rule can cause a single overoptimistic estimate to skew the Q-values, leading to incorrect policy decisions.</p>
<p>A more in-depth explanation can be found in the paper <a href="optimizers_curse.pdf">The Optimizer’s Curse: Skepticism and Postdecision Surprise in Decision Analysis (pdf)</a> by Smith and Winkler.</p>
<h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><p>A solution to this problem, Double Q-learning, was proposed by <a target="_blank" rel="noopener" href="https://papers.nips.cc/paper_files/paper/2010/file/091d584fced301b442654dd8c23b3fc9-Paper.pdf">Hasselt (pdf)</a>. Let’s view our problem through a slightly different lens. The issue arises because we are using the same samples of <code>C</code> twice. Once to estimate the value of taking an action; <code>Q(B, move to C<sub>i</sub>)</code>. Secondly when performing the maximizing operation to determine which <code>C<sub>i</sub></code> is best to move to from state b; <code>max<sub>i</sub> Q(B, C<sub>i</sub>)</code>. If we instead use 2 independent estimates, <code>Q<sub>1</sub></code> and <code>Q<sub>2</sub></code>, we alleviate this problem. <code>Q<sub>1</sub></code> might overestimate the value of moving to a particular state <code>C<sub>i</sub></code>, but it’s very unlikely that <code>Q<sub>2</sub></code> also estimates moving to <code>C<sub>i</sub></code> to be the best action to take from <code>B</code>.</p>
<p>This sounds confusing, so let’s walk through the original Q-learning update again. After removing the discount (unimportant for our purposes) we are left with:</p>
<p><code>Q(s, a) = Q(s, a) + α * (reward + max a Q(s’,a) - Q(s,a))</code></p>
<p>Conceptually we are doing the following:</p>
<pre class="line-numbers language-none"><code class="language-none"># the old&#x2F;current estimate of taking action a in state s
old &#x3D; q(s,a)

# new estimate is the reward, plus our best estimate for future rewards starting from the next state
new &#x3D; r +  max a q(s’, a) 

# the discrepancy between the 2 estimates
error &#x3D; new - old

# update our estimate:
q(s,a) &#x3D; old + learning_rate * error<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>What’s important to realize is we are making use of the same Q function to get our estimates twice. Once for <code>Q(s’,a)</code> to get the value of the new state action pair, and again when performing <code>max a</code> on <code>Q(s’,a)</code> to decide what value of <code>a</code> to use. </p>
<p>The double Q-learning solution to our problem says we should use two independent estimates of the state-action values. Our update is now as follows:</p>
<pre class="line-numbers language-none"><code class="language-none"># the old&#x2F;current estimate of taking action a in state s
old &#x3D; q1(s,a)

# use q1 to estimate the best action to take in the next state
best_action_in_state_s’ &#x3D; argmax a q1(s’, a)

# use q2 to determine what the value of the action is
value of s’ &#x3D; q2(s’, best_action_in_state_s’)

# new estimate is the reward, plus our best estimate for future rewards starting from the next state
new &#x3D; r + value of s’

error &#x3D; new - old

updated q1(s,a) &#x3D; old + learning_rate * error<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>The full double Q-learning algorithm is as follows:</p>

<center>
<img src="double_algo.png" alt="double q algorithm">
<p><small>Double Q-learning (Sutton and Barto)</small></p>
</center>



<p>Since <code>Q1</code> is updated on different samples than <code>Q2</code>, they are not subject to the maximization bias. The algorithm does require more memory to store two Q functions. The computational cost stays the same.</p>
<h2 id="Code-Walkthrough"><a href="#Code-Walkthrough" class="headerlink" title="Code Walkthrough"></a>Code Walkthrough</h2><p>The first time I went through this, it was a bit of a head-scratcher, so let’s walk through the code in python to make it more concrete. We will be using the exact same example MDP as above. We will run both Q-learning and Double Q-learning and compare their results. The full code is available <a target="_blank" rel="noopener" href="https://gist.github.com/BasedLukas/bda5cfed389e42108fc9f6a8daeb7cd7">here</a>.</p>
<h5 id="Create-a-Markov-process-Note-that-the-values-of-states-C-are-drawn-from-N-0-1-1"><a href="#Create-a-Markov-process-Note-that-the-values-of-states-C-are-drawn-from-N-0-1-1" class="headerlink" title="Create a Markov process. Note that the values of states C are drawn from N(-0.1, 1)."></a>Create a Markov process. Note that the values of states <code>C</code> are drawn from <code>N(-0.1, 1)</code>.</h5><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">get_reward</span><span class="token punctuation">(</span>state<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token builtin">float</span><span class="token punctuation">:</span>
   <span class="token triple-quoted-string string">"""
   Returns the reward for transitioning into a given state.

   Args:
   - state: The state transitioned into.

   Returns:
   - A float representing the reward for that transition.

   Raises:
   - ValueError: If an invalid state is provided.
   """</span>
   <span class="token keyword">if</span> state <span class="token operator">==</span> <span class="token string">"a"</span><span class="token punctuation">:</span>
       <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"a should not be passed as a param as it's the starting state"</span><span class="token punctuation">)</span>
   <span class="token keyword">if</span> state <span class="token operator">==</span> <span class="token string">'b'</span> <span class="token keyword">or</span> state <span class="token operator">==</span> <span class="token string">'terminal'</span><span class="token punctuation">:</span>
       <span class="token keyword">return</span> <span class="token number">0</span>
   <span class="token keyword">if</span> <span class="token string">'c'</span> <span class="token keyword">in</span> state<span class="token punctuation">:</span>
       <span class="token keyword">return</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
   <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"state: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>state<span class="token punctuation">&#125;</span></span><span class="token string"> not recognized"</span></span><span class="token punctuation">)</span>

transitions <span class="token operator">=</span> <span class="token punctuation">&#123;</span>
    <span class="token string">"a"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"terminal"</span><span class="token punctuation">,</span> <span class="token string">"b"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"b"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"c"</span><span class="token operator">+</span><span class="token builtin">str</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>number_of_c_states<span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token punctuation">&#125;</span>
<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>number_of_c_states<span class="token punctuation">)</span><span class="token punctuation">:</span>
    transitions<span class="token punctuation">[</span><span class="token string-interpolation"><span class="token string">f"c</span><span class="token interpolation"><span class="token punctuation">&#123;</span>i<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"terminal"</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h5 id="Our-Q-functions-are-simply-dictionaries"><a href="#Our-Q-functions-are-simply-dictionaries" class="headerlink" title="Our Q functions are simply dictionaries."></a>Our <code>Q</code> functions are simply dictionaries.</h5><pre class="line-numbers language-none"><code class="language-none">q:  Dict[Tuple[str, int], float] &#x3D; &#123;&#125;
q1: Dict[Tuple[str, int], float] &#x3D; &#123;&#125;
q2: Dict[Tuple[str, int], float] &#x3D; &#123;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<h5 id="Now-define-a-function-to-do-the-Q-learning-update-max-a-uses-the-provided-q-to-find-the-best-next-value"><a href="#Now-define-a-function-to-do-the-Q-learning-update-max-a-uses-the-provided-q-to-find-the-best-next-value" class="headerlink" title="Now define a function to do the Q-learning update. max_a uses the provided q to find the best next value."></a>Now define a function to do the Q-learning update. <code>max_a</code> uses the provided <code>q</code> to find the best next value.</h5><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">q_update</span><span class="token punctuation">(</span>
       state<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">,</span>
       action<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>
       new_state<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">,</span>
       reward<span class="token punctuation">:</span> <span class="token builtin">float</span><span class="token punctuation">,</span>
       alpha<span class="token punctuation">:</span> <span class="token builtin">float</span><span class="token punctuation">,</span>
       q<span class="token punctuation">:</span> Dict
   <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token boolean">None</span><span class="token punctuation">:</span>
   <span class="token triple-quoted-string string">"""
   In-place update of Q-values for Q-learning.

   Args:
       state: The current state.
       action: The action taken in the current state.
       new_state: The state reached after taking the action.
       reward: The reward received after taking the action.
       alpha: The learning rate.
       q: The Q-values dictionary.
   """</span>
   current_q <span class="token operator">=</span> q<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token punctuation">(</span>state<span class="token punctuation">,</span> action<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>  <span class="token comment"># Current Q-value estimation</span>
   max_next <span class="token operator">=</span> max_a<span class="token punctuation">(</span>new_state<span class="token punctuation">,</span> q<span class="token punctuation">)</span>  <span class="token comment"># Maximum Q-value for the next state</span>
   target <span class="token operator">=</span> reward <span class="token operator">+</span> gamma <span class="token operator">*</span> max_next  <span class="token comment"># TD Target</span>
   td_error <span class="token operator">=</span> target <span class="token operator">-</span> current_q  <span class="token comment"># TD Error</span>
   update <span class="token operator">=</span> alpha <span class="token operator">*</span> td_error  <span class="token comment"># TD Update</span>
   q<span class="token punctuation">[</span><span class="token punctuation">(</span>state<span class="token punctuation">,</span> action<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> current_q <span class="token operator">+</span> update<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h5 id="The-Double-Q-learning-update-argmax-a-finds-the-best-next-action-using-the-provided-Q-function"><a href="#The-Double-Q-learning-update-argmax-a-finds-the-best-next-action-using-the-provided-Q-function" class="headerlink" title="The Double Q-learning update. argmax_a finds the best next action, using the provided Q function."></a>The Double Q-learning update. <code>argmax_a</code> finds the best next action, using the provided Q function.</h5><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">double_q_update</span><span class="token punctuation">(</span>
       state<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">,</span>
       action<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>
       new_state<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">,</span>
       reward<span class="token punctuation">:</span> <span class="token builtin">float</span><span class="token punctuation">,</span>
       alpha<span class="token punctuation">:</span> <span class="token builtin">float</span><span class="token punctuation">,</span>
       q1<span class="token punctuation">:</span> Dict<span class="token punctuation">,</span>
       q2<span class="token punctuation">:</span> Dict
   <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token boolean">None</span><span class="token punctuation">:</span>
   <span class="token triple-quoted-string string">"""
   In-place update of Q-values for Double Q-learning.

   Args:
       state: The current state.
       action: The action taken in the current state.
       new_state: The state reached after taking the action.
       reward: The reward received after taking the action.
       alpha: The learning rate.
       q1: The first Q-values dictionary.
       q2: The second Q-values dictionary.
   """</span>
   qs <span class="token operator">=</span> <span class="token punctuation">[</span>q1<span class="token punctuation">,</span> q2<span class="token punctuation">]</span>  <span class="token comment"># List of Q dictionaries</span>
   random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>qs<span class="token punctuation">)</span>  <span class="token comment"># Randomly shuffle to choose one for updating</span>
   qa<span class="token punctuation">,</span> qb <span class="token operator">=</span> qs  <span class="token comment"># qa is the Q to update, qb</span>

 <span class="token keyword">is</span> used <span class="token keyword">for</span> target calculation

   current_q <span class="token operator">=</span> qa<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token punctuation">(</span>state<span class="token punctuation">,</span> action<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>  <span class="token comment"># Current Q-value estimation</span>
   best_action <span class="token operator">=</span> argmax_a<span class="token punctuation">(</span>new_state<span class="token punctuation">,</span> qa<span class="token punctuation">)</span>  <span class="token comment"># Best action based on qa</span>
   target <span class="token operator">=</span> reward <span class="token operator">+</span> gamma <span class="token operator">*</span> qb<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token punctuation">(</span>new_state<span class="token punctuation">,</span> best_action<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>  <span class="token comment"># TD Target using qb</span>
   error <span class="token operator">=</span> target <span class="token operator">-</span> current_q  <span class="token comment"># TD Error</span>
   update <span class="token operator">=</span> alpha <span class="token operator">*</span> error  <span class="token comment"># TD Update</span>
   qa<span class="token punctuation">[</span><span class="token punctuation">(</span>state<span class="token punctuation">,</span> action<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> current_q <span class="token operator">+</span> update<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h5 id="At-this-point-we-simulate-both"><a href="#At-this-point-we-simulate-both" class="headerlink" title="At this point, we simulate both."></a>At this point, we simulate both.</h5><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">simulate</span><span class="token punctuation">(</span>
       epoch<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>
       q<span class="token punctuation">:</span> Dict<span class="token punctuation">,</span>
       q2<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Dict<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span>
   <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token boolean">None</span><span class="token punctuation">:</span>
   <span class="token triple-quoted-string string">"""
   Simulate an epoch of the agent's interaction with the environment, updating Q-values based on observed transitions.

   Args:
       epoch: The current epoch of the simulation.
       q: The Q-values dictionary for Q-learning or the primary Q-values dictionary for Double Q-learning.
       q2: The secondary Q-values dictionary for Double Q-learning, if applicable.
   """</span>
   double <span class="token operator">=</span> q2 <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span>
   state <span class="token operator">=</span> <span class="token string">'a'</span>
   <span class="token keyword">while</span> state <span class="token operator">!=</span> <span class="token string">'terminal'</span><span class="token punctuation">:</span>
       <span class="token keyword">if</span> double<span class="token punctuation">:</span>
           action <span class="token operator">=</span> policy<span class="token punctuation">(</span>state<span class="token punctuation">,</span> epoch<span class="token punctuation">,</span> q<span class="token punctuation">,</span> q2<span class="token punctuation">)</span>
       <span class="token keyword">else</span><span class="token punctuation">:</span>
           action <span class="token operator">=</span> policy<span class="token punctuation">(</span>state<span class="token punctuation">,</span> epoch<span class="token punctuation">,</span> q<span class="token punctuation">)</span>
       new_state <span class="token operator">=</span> transitions<span class="token punctuation">[</span>state<span class="token punctuation">]</span><span class="token punctuation">[</span>action<span class="token punctuation">]</span>
       reward <span class="token operator">=</span> get_reward<span class="token punctuation">(</span>new_state<span class="token punctuation">)</span>
      
       <span class="token keyword">if</span> double<span class="token punctuation">:</span>
           double_q_update<span class="token punctuation">(</span>
               state<span class="token operator">=</span>state<span class="token punctuation">,</span>
               action<span class="token operator">=</span>action<span class="token punctuation">,</span>
               new_state<span class="token operator">=</span>new_state<span class="token punctuation">,</span>
               reward<span class="token operator">=</span>reward<span class="token punctuation">,</span>
               alpha<span class="token operator">=</span>lr<span class="token punctuation">,</span>
               q1<span class="token operator">=</span>q<span class="token punctuation">,</span>
               q2<span class="token operator">=</span>q2
               <span class="token punctuation">)</span>
       <span class="token keyword">else</span><span class="token punctuation">:</span>
           q_update<span class="token punctuation">(</span>state<span class="token punctuation">,</span> action<span class="token punctuation">,</span> new_state<span class="token punctuation">,</span> reward<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> q<span class="token punctuation">)</span>

       state <span class="token operator">=</span> new_state<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>Now we can plot the results. They will differ every time, but mostly look something like this.</p>

<center>
<img src="double_q.png" alt="my results">
<p><small>My results</small></p>
</center>



<p>We can clearly see how Q-learning has trouble converging on the correct result. I was so excited when I got this result, because it mirrors very closely with that of Sutton and Barto!</p>

<center>
<img src="sutton.png" alt="Suttons results">
<p><small>Sutton's results (Sutton and Barto)</small></p>
</center>



<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>I went into this deep dive because I had trouble understanding this myself. I’m still not entirely sure I understand it, but writing out the code certainly helps. Sutton and Barto’s <a target="_blank" rel="noopener" href="http://incompleteideas.net/book/the-book.html">book</a> on RL really is a must. Other useful resources are these series on YouTube:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLwRJQ4m4UJjNymuBM9RdmB3Z9N5-0IlY0">Foundations of deep RL</a></li>
<li><a target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLqYmG7hTraZBKeNJ-JE_eyJHZ7XgBoAyb">DeepMind x UCL | Reinforcement Learning Course</a></li>
</ul>
<p>And of course, Andrej Karpathy’s <a target="_blank" rel="noopener" href="http://karpathy.github.io/2016/05/31/rl/">blog post</a>.</p>


                
            </div>

            <!-- Comments -->
            
                <div class="col-lg-8 col-lg-offset-1 col-md-10 col-md-offset-1">
                    


                </div>
            
        </div>
    </div>
</article>

    <!-- Footer -->
    <hr />

<!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                    

                    

                    

                    

                    
                        <li>
                            <a href="mailto:info@loreley.one" target="_blank">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-envelope-o fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                    

                    
                </ul>
                <p class="copyright text-muted">&copy; 2024 Lukas<br></p>



                
            </div>
        </div>
    </div>
</footer>


    <!-- After footer scripts -->
    
<!-- jQuery -->
<script src="//code.jquery.com/jquery-2.1.4.min.js"></script>

<!-- Bootstrap -->
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>

<!-- Gallery -->
<script src="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.js" type="text/javascript" charset="utf-8"></script>

<!-- Disqus Comments -->



</body>

</html>