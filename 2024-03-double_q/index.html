<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>

    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>

    <!-- background image -->
    <link rel="preload" href="/img/bg.webp" as="image" fetchpriority="high">

    <!-- Bootstrap Core CSS -->
    <link href="/assets/css/bootstrap.min.css" rel="stylesheet"/>

    <!-- Custom CSS -->
    
<link rel="stylesheet" href="/css/style.css">


    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/> 
    <link href="//fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css"/>
    <link href="//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css"/>

    <!-- Gallery -->
    <!-- <link href="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.css" type="text/css" rel="stylesheet"/> -->


    <!--Description-->
    

    
        <meta name="description" content="My incoherent ramblings"/>
        <meta property="og:description" content="My incoherent ramblings"/>
        <meta name="twitter:description" content="My incoherent ramblings">
    

    <!--Author-->
    
        <meta name="author" content="Lukas"/>
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="Double Q-Learning Explained"/>
        <meta name="twitter:title" content="Double Q-Learning Explained">
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="Lukas&#39;s Blog"/>

    <!--Type page-->
    
        <meta property="og:type" content="article"/>
    

    <!--Page Cover-->
    
    
        <meta property="og:image" content="http://loreley.one/2024-03-double_q/double_q.png"/>
        <meta name="twitter:image" content="http://loreley.one/2024-03-double_q/double_q.png">
    

        <meta name="twitter:card" content="summary_large_image"/>

    
        <meta name="twitter:site" content="@LukasBogacz"/>
    

    
        <meta name="twitter:image" content="http://loreley.one/2024-03-double_q/double_q.png"/>
    

    <!-- Title -->
    
    <title>Double Q-Learning Explained - Lukas&#39;s Blog</title>

    <!-- favicon -->
    
        <link rel="icon" href="/img/favicon.ico"/>
        
    
<meta name="generator" content="Hexo 7.1.1"></head>


<body>

    <!-- Menu -->
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Home</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">
                
                    <li>
                        <a href="/">
                            
                                Home
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="/about">
                            
                                About
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="/links">
                            
                                Links
                            
                        </a>
                    </li>
                
                    <li>
                        <a target="_blank" rel="noopener" href="https://github.com/BasedLukas">
                            
                                <i class="fa fa-github fa-stack-2x"></i>
                            
                        </a>
                    </li>
                
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>

    <!-- Main Content -->
    <!-- Page Header -->
<!-- Set your background image for this header in your post front-matter: cover -->

<header class="intro-header" style="background-image: url('/img/bg.webp')">
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-1 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <h1>Double Q-Learning Explained</h1>
                    
                    <h2 class="post-subheading">
                        What is Maximization Bias and Double Q-learning?
                    </h2>
                    
                    <span class="meta">
                        <!-- Date and Author -->
                        
                            Posted by Lukas on
                        
                        
                            2024-03-16
                        
                    </span>
                </div>
            </div>
        </div>
    </div>
</header>

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

            <!-- Tags and categories -->
           
                <div class="col-lg-4 col-lg-offset-1 col-md-5 col-md-offset-1 post-tags">
                    
                        


<a href="/tags/python/">#python</a> <a href="/tags/reinforcement-learning/">#reinforcement learning</a> <a href="/tags/probability/">#probability</a>


                    
                </div>
                <div class="col-lg-4 col-md-5 post-categories">
                    
                </div>
            

            <!-- Gallery -->
            

            <!-- Post Main Content -->
            <div class="col-lg-10 col-lg-offset-1 col-md-10 col-md-offset-1">
                <h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Q-learning is a popular reinforcement learning algorithm, used for solving Markov Decision Processes (MDP). In some cases, Q-learning doesn’t work well and takes a long time to converge, due to an issue known as the optimizer’s curse or maximization bias. In this post, we’ll take a look at the problem as well as a proposed solution. What follows is a brief recapitulation of MDP’s and Q-learning, followed by a deep dive into Double Q learning, the proposed solution to the problem.</p>
<h2 id="Recap-of-Q-learning"><a href="#Recap-of-Q-learning" class="headerlink" title="Recap of Q learning"></a>Recap of Q learning</h2><p>A Markov chain consists of states connected through transition probabilities, which determine the likelihood of moving from one state to another. This probability depends only on the current state, not on the sequence of events that preceded it. Some states are accessible only from specific other states, forming a directed network.</p>
<p>A Markov Decision Process (MDP) extends the concept of a Markov chain by incorporating decisions. It substitutes transition probabilities with actions that represent available choices. Each state in an MDP is linked with a reward, indicating the value of reaching that state. The distinct feature of MDPs is the decision-making aspect, where an agent selects actions to transition between states and accumulate rewards. The goal in an MDP is to find an optimal policy, which is a set of rules defining the best action to take in each state to maximize rewards.</p>
<p>A trajectory through an MDP is represented using the notation: starting at state <code>S</code>, an action <code>A</code> is chosen from the available options in state <code>S</code>. This leads to a transition to state <code>S'</code> with probability <code>P</code>, and a reward <code>R</code> is received. The tuple <code>(S, A, P, R)</code> describes this process, where <code>P</code> is defined as <code>Pr(S' | S, A)</code>. This sequence repeats at each step until a terminal state is reached, outlining the full trajectory:</p>
<p><code>S<sub>0</sub>, A<sub>0</sub>, R<sub>1</sub>, S<sub>1</sub>, A<sub>1</sub>, R<sub>2</sub>, S<sub>2</sub>, A<sub>2</sub>, R<sub>3</sub>, ...</code></p>
<p>The Q(action, value) function under a policy <code>π</code> is formally defined as:</p>
<p><code>Q<sub>π</sub>(s, a) = E[G | s, a, π]</code></p>
<p>where <code>E[G | s, a, π]</code> represents the expected total (discounted) reward given that we start in state <code>s</code>, take action <code>a</code>, and then follow policy <code>π</code> for all subsequent decisions. This expectation accounts for the sum of rewards received, starting from <code>s</code> and <code>a</code>, under the guidance of policy <code>π</code>.</p>
<p>In Q-learning, the objective is to approximate the optimal Q function, which represents the best action values under an optimal policy, regardless of the initial policy used to generate training data. The policy generating our training data decides actions, which might not be optimal. Our aim is to iteratively refine our Q function based on these examples. The algorithm is as follows:</p>

<center>
<img src="q_algo.png" alt="q learning algorithm">
<p><small>Q-learning algorithm (Sutton and Barto)</small></p>
</center>


<p>For a full discussion of Q-learning I recommend the following 2 sources:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/learn/deep-rl-course/unit2/introduction">Hugging Face course on Q-learning</a>. This is a nice quick overview.</li>
<li>For a full treatment see the RL book by Sutton and Barto, which is available free <a target="_blank" rel="noopener" href="http://incompleteideas.net/book/the-book.html">here</a>.</li>
</ul>
<h2 id="Dissection-of-the-problem"><a href="#Dissection-of-the-problem" class="headerlink" title="Dissection of the problem"></a>Dissection of the problem</h2><p>A common issue with Q-learning involves how it handles variance in rewards. Consider an MDP where we start in state <code>A</code> with the options to move to <code>B</code> or directly to a terminal state <code>T</code>, neither transition offering any reward. From <code>B</code>, we can transition to several secondary states, <code>C<sub>1</sub>, C<sub>2</sub>, ..., C<sub>n</sub></code>, each associated with a reward from a normal distribution with a negative mean (e.g., -0.1) and a variance (e.g., 1). Transitioning from any <code>C<sub>n</sub></code> to <code>T</code> yields no reward. Ideally, the optimal strategy is to move from <code>A</code> to <code>T</code>, avoiding negative rewards in the <code>C</code> states. However, the stochastic nature of rewards means a visit to any <code>C</code> state might yield a positive reward. The likelihood of receiving a positive reward increases with the number of <code>C</code> states.</p>
<p>This variance introduces a challenge in Q-learning. The algorithm estimates the Q-value for transitioning from <code>A</code> to <code>B</code> based on the maximum reward obtainable from moving to any <code>C</code> state. Given the rewards are drawn from a distribution, it’s probable to encounter a positive reward in one of the <code>C</code> states during exploration. Consequently, the Q-function may overestimate the value of moving from A to B. Essentially, the “max” operation in the update rule can cause a single overoptimistic estimate to skew the Q-values, leading to incorrect policy decisions.</p>
<p>A more in-depth explanation can be found in the paper <a href="optimizers_curse.pdf">The Optimizer’s Curse: Skepticism and Postdecision Surprise in Decision Analysis (pdf)</a> by Smith and Winkler.</p>
<h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><p>A solution to this problem, Double Q-learning, was proposed by <a target="_blank" rel="noopener" href="https://papers.nips.cc/paper_files/paper/2010/file/091d584fced301b442654dd8c23b3fc9-Paper.pdf">Hasselt (pdf)</a>. Let’s view our problem through a slightly different lens. The issue arises because we are using the same samples of <code>C</code> twice. Once to estimate the value of taking an action; <code>Q(B, move to C<sub>i</sub>)</code>. Secondly when performing the maximizing operation to determine which <code>C<sub>i</sub></code> is best to move to from state b; <code>max<sub>i</sub> Q(B, C<sub>i</sub>)</code>. If we instead use 2 independent estimates, <code>Q<sub>1</sub></code> and <code>Q<sub>2</sub></code>, we alleviate this problem. <code>Q<sub>1</sub></code> might overestimate the value of moving to a particular state <code>C<sub>i</sub></code>, but it’s very unlikely that <code>Q<sub>2</sub></code> also estimates moving to <code>C<sub>i</sub></code> to be the best action to take from <code>B</code>.</p>
<p>This sounds confusing, so let’s walk through the original Q-learning update again. After removing the discount (unimportant for our purposes) we are left with:</p>
<p><code>Q(s, a) = Q(s, a) + α * (reward + max a Q(s’,a) - Q(s,a))</code></p>
<p>Conceptually we are doing the following:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># the old/current estimate of taking action a in state s</span><br><span class="line">old = q(s,a)</span><br><span class="line"></span><br><span class="line"># new estimate is the reward, plus our best estimate for future rewards starting from the next state</span><br><span class="line">new = r +  max a q(s’, a) </span><br><span class="line"></span><br><span class="line"># the discrepancy between the 2 estimates</span><br><span class="line">error = new - old</span><br><span class="line"></span><br><span class="line"># update our estimate:</span><br><span class="line">q(s,a) = old + learning_rate * error</span><br></pre></td></tr></table></figure>

<p>What’s important to realize is we are making use of the same Q function to get our estimates twice. Once for <code>Q(s’,a)</code> to get the value of the new state action pair, and again when performing <code>max a</code> on <code>Q(s’,a)</code> to decide what value of <code>a</code> to use. </p>
<p>The double Q-learning solution to our problem says we should use two independent estimates of the state-action values. Our update is now as follows:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># the old/current estimate of taking action a in state s</span><br><span class="line">old = q1(s,a)</span><br><span class="line"></span><br><span class="line"># use q1 to estimate the best action to take in the next state</span><br><span class="line">best_action_in_state_s’ = argmax a q1(s’, a)</span><br><span class="line"></span><br><span class="line"># use q2 to determine what the value of the action is</span><br><span class="line">value of s’ = q2(s’, best_action_in_state_s’)</span><br><span class="line"></span><br><span class="line"># new estimate is the reward, plus our best estimate for future rewards starting from the next state</span><br><span class="line">new = r + value of s’</span><br><span class="line"></span><br><span class="line">error = new - old</span><br><span class="line"></span><br><span class="line">updated q1(s,a) = old + learning_rate * error</span><br></pre></td></tr></table></figure>

<p>The full double Q-learning algorithm is as follows:</p>

<center>
<img src="double_algo.png" alt="double q algorithm">
<p><small>Double Q-learning (Sutton and Barto)</small></p>
</center>



<p>Since <code>Q1</code> is updated on different samples than <code>Q2</code>, they are not subject to the maximization bias. The algorithm does require more memory to store two Q functions. The computational cost stays the same.</p>
<h2 id="Code-Walkthrough"><a href="#Code-Walkthrough" class="headerlink" title="Code Walkthrough"></a>Code Walkthrough</h2><p>The first time I went through this, it was a bit of a head-scratcher, so let’s walk through the code in python to make it more concrete. We will be using the exact same example MDP as above. We will run both Q-learning and Double Q-learning and compare their results. The full code is available <a target="_blank" rel="noopener" href="https://gist.github.com/BasedLukas/bda5cfed389e42108fc9f6a8daeb7cd7">here</a>.</p>
<h5 id="Create-a-Markov-process-Note-that-the-values-of-states-C-are-drawn-from-N-0-1-1"><a href="#Create-a-Markov-process-Note-that-the-values-of-states-C-are-drawn-from-N-0-1-1" class="headerlink" title="Create a Markov process. Note that the values of states C are drawn from N(-0.1, 1)."></a>Create a Markov process. Note that the values of states <code>C</code> are drawn from <code>N(-0.1, 1)</code>.</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_reward</span>(<span class="params">state: <span class="built_in">str</span></span>) -&gt; <span class="built_in">float</span>:</span><br><span class="line">   <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">   Returns the reward for transitioning into a given state.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">   Args:</span></span><br><span class="line"><span class="string">   - state: The state transitioned into.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">   Returns:</span></span><br><span class="line"><span class="string">   - A float representing the reward for that transition.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">   Raises:</span></span><br><span class="line"><span class="string">   - ValueError: If an invalid state is provided.</span></span><br><span class="line"><span class="string">   &quot;&quot;&quot;</span></span><br><span class="line">   <span class="keyword">if</span> state == <span class="string">&quot;a&quot;</span>:</span><br><span class="line">       <span class="keyword">raise</span> ValueError(<span class="string">&quot;a should not be passed as a param as it&#x27;s the starting state&quot;</span>)</span><br><span class="line">   <span class="keyword">if</span> state == <span class="string">&#x27;b&#x27;</span> <span class="keyword">or</span> state == <span class="string">&#x27;terminal&#x27;</span>:</span><br><span class="line">       <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">   <span class="keyword">if</span> <span class="string">&#x27;c&#x27;</span> <span class="keyword">in</span> state:</span><br><span class="line">       <span class="keyword">return</span> np.random.normal(-<span class="number">0.1</span>, <span class="number">1</span>)</span><br><span class="line">   <span class="keyword">raise</span> ValueError(<span class="string">f&quot;state: <span class="subst">&#123;state&#125;</span> not recognized&quot;</span>)</span><br><span class="line"></span><br><span class="line">transitions = &#123;</span><br><span class="line">    <span class="string">&quot;a&quot;</span>: [<span class="string">&quot;terminal&quot;</span>, <span class="string">&quot;b&quot;</span>],</span><br><span class="line">    <span class="string">&quot;b&quot;</span>: [<span class="string">&quot;c&quot;</span>+<span class="built_in">str</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(number_of_c_states)]</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(number_of_c_states):</span><br><span class="line">    transitions[<span class="string">f&quot;c<span class="subst">&#123;i&#125;</span>&quot;</span>] = [<span class="string">&quot;terminal&quot;</span>]</span><br></pre></td></tr></table></figure>

<h5 id="Our-Q-functions-are-simply-dictionaries"><a href="#Our-Q-functions-are-simply-dictionaries" class="headerlink" title="Our Q functions are simply dictionaries."></a>Our <code>Q</code> functions are simply dictionaries.</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">q:  Dict[Tuple[str, int], float] = &#123;&#125;</span><br><span class="line">q1: Dict[Tuple[str, int], float] = &#123;&#125;</span><br><span class="line">q2: Dict[Tuple[str, int], float] = &#123;&#125;</span><br></pre></td></tr></table></figure>

<h5 id="Now-define-a-function-to-do-the-Q-learning-update-max-a-uses-the-provided-q-to-find-the-best-next-value"><a href="#Now-define-a-function-to-do-the-Q-learning-update-max-a-uses-the-provided-q-to-find-the-best-next-value" class="headerlink" title="Now define a function to do the Q-learning update. max_a uses the provided q to find the best next value."></a>Now define a function to do the Q-learning update. <code>max_a</code> uses the provided <code>q</code> to find the best next value.</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">q_update</span>(<span class="params"></span></span><br><span class="line"><span class="params">       state: <span class="built_in">str</span>,</span></span><br><span class="line"><span class="params">       action: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">       new_state: <span class="built_in">str</span>,</span></span><br><span class="line"><span class="params">       reward: <span class="built_in">float</span>,</span></span><br><span class="line"><span class="params">       alpha: <span class="built_in">float</span>,</span></span><br><span class="line"><span class="params">       q: <span class="type">Dict</span></span></span><br><span class="line"><span class="params">   </span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">   <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">   In-place update of Q-values for Q-learning.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">   Args:</span></span><br><span class="line"><span class="string">       state: The current state.</span></span><br><span class="line"><span class="string">       action: The action taken in the current state.</span></span><br><span class="line"><span class="string">       new_state: The state reached after taking the action.</span></span><br><span class="line"><span class="string">       reward: The reward received after taking the action.</span></span><br><span class="line"><span class="string">       alpha: The learning rate.</span></span><br><span class="line"><span class="string">       q: The Q-values dictionary.</span></span><br><span class="line"><span class="string">   &quot;&quot;&quot;</span></span><br><span class="line">   current_q = q.get((state, action), <span class="number">0</span>)  <span class="comment"># Current Q-value estimation</span></span><br><span class="line">   max_next = max_a(new_state, q)  <span class="comment"># Maximum Q-value for the next state</span></span><br><span class="line">   target = reward + gamma * max_next  <span class="comment"># TD Target</span></span><br><span class="line">   td_error = target - current_q  <span class="comment"># TD Error</span></span><br><span class="line">   update = alpha * td_error  <span class="comment"># TD Update</span></span><br><span class="line">   q[(state, action)] = current_q + update</span><br></pre></td></tr></table></figure>

<h5 id="The-Double-Q-learning-update-argmax-a-finds-the-best-next-action-using-the-provided-Q-function"><a href="#The-Double-Q-learning-update-argmax-a-finds-the-best-next-action-using-the-provided-Q-function" class="headerlink" title="The Double Q-learning update. argmax_a finds the best next action, using the provided Q function."></a>The Double Q-learning update. <code>argmax_a</code> finds the best next action, using the provided Q function.</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">double_q_update</span>(<span class="params"></span></span><br><span class="line"><span class="params">       state: <span class="built_in">str</span>,</span></span><br><span class="line"><span class="params">       action: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">       new_state: <span class="built_in">str</span>,</span></span><br><span class="line"><span class="params">       reward: <span class="built_in">float</span>,</span></span><br><span class="line"><span class="params">       alpha: <span class="built_in">float</span>,</span></span><br><span class="line"><span class="params">       q1: <span class="type">Dict</span>,</span></span><br><span class="line"><span class="params">       q2: <span class="type">Dict</span></span></span><br><span class="line"><span class="params">   </span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">   <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">   In-place update of Q-values for Double Q-learning.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">   Args:</span></span><br><span class="line"><span class="string">       state: The current state.</span></span><br><span class="line"><span class="string">       action: The action taken in the current state.</span></span><br><span class="line"><span class="string">       new_state: The state reached after taking the action.</span></span><br><span class="line"><span class="string">       reward: The reward received after taking the action.</span></span><br><span class="line"><span class="string">       alpha: The learning rate.</span></span><br><span class="line"><span class="string">       q1: The first Q-values dictionary.</span></span><br><span class="line"><span class="string">       q2: The second Q-values dictionary.</span></span><br><span class="line"><span class="string">   &quot;&quot;&quot;</span></span><br><span class="line">   qs = [q1, q2]  <span class="comment"># List of Q dictionaries</span></span><br><span class="line">   random.shuffle(qs)  <span class="comment"># Randomly shuffle to choose one for updating</span></span><br><span class="line">   qa, qb = qs  <span class="comment"># qa is the Q to update, qb</span></span><br><span class="line"></span><br><span class="line"> <span class="keyword">is</span> used <span class="keyword">for</span> target calculation</span><br><span class="line"></span><br><span class="line">   current_q = qa.get((state, action), <span class="number">0</span>)  <span class="comment"># Current Q-value estimation</span></span><br><span class="line">   best_action = argmax_a(new_state, qa)  <span class="comment"># Best action based on qa</span></span><br><span class="line">   target = reward + gamma * qb.get((new_state, best_action), <span class="number">0</span>)  <span class="comment"># TD Target using qb</span></span><br><span class="line">   error = target - current_q  <span class="comment"># TD Error</span></span><br><span class="line">   update = alpha * error  <span class="comment"># TD Update</span></span><br><span class="line">   qa[(state, action)] = current_q + update</span><br></pre></td></tr></table></figure>

<h5 id="At-this-point-we-simulate-both"><a href="#At-this-point-we-simulate-both" class="headerlink" title="At this point, we simulate both."></a>At this point, we simulate both.</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">simulate</span>(<span class="params"></span></span><br><span class="line"><span class="params">       epoch: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">       q: <span class="type">Dict</span>,</span></span><br><span class="line"><span class="params">       q2: <span class="type">Optional</span>[<span class="type">Dict</span>] = <span class="literal">None</span></span></span><br><span class="line"><span class="params">   </span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">   <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">   Simulate an epoch of the agent&#x27;s interaction with the environment, updating Q-values based on observed transitions.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">   Args:</span></span><br><span class="line"><span class="string">       epoch: The current epoch of the simulation.</span></span><br><span class="line"><span class="string">       q: The Q-values dictionary for Q-learning or the primary Q-values dictionary for Double Q-learning.</span></span><br><span class="line"><span class="string">       q2: The secondary Q-values dictionary for Double Q-learning, if applicable.</span></span><br><span class="line"><span class="string">   &quot;&quot;&quot;</span></span><br><span class="line">   double = q2 <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">   state = <span class="string">&#x27;a&#x27;</span></span><br><span class="line">   <span class="keyword">while</span> state != <span class="string">&#x27;terminal&#x27;</span>:</span><br><span class="line">       <span class="keyword">if</span> double:</span><br><span class="line">           action = policy(state, epoch, q, q2)</span><br><span class="line">       <span class="keyword">else</span>:</span><br><span class="line">           action = policy(state, epoch, q)</span><br><span class="line">       new_state = transitions[state][action]</span><br><span class="line">       reward = get_reward(new_state)</span><br><span class="line">      </span><br><span class="line">       <span class="keyword">if</span> double:</span><br><span class="line">           double_q_update(</span><br><span class="line">               state=state,</span><br><span class="line">               action=action,</span><br><span class="line">               new_state=new_state,</span><br><span class="line">               reward=reward,</span><br><span class="line">               alpha=lr,</span><br><span class="line">               q1=q,</span><br><span class="line">               q2=q2</span><br><span class="line">               )</span><br><span class="line">       <span class="keyword">else</span>:</span><br><span class="line">           q_update(state, action, new_state, reward, lr, q)</span><br><span class="line"></span><br><span class="line">       state = new_state</span><br></pre></td></tr></table></figure>

<p>Now we can plot the results. They will differ every time, but mostly look something like this.</p>

<center>
<img src="double_q.png" alt="my results">
<p><small>My results</small></p>
</center>



<p>We can clearly see how Q-learning has trouble converging on the correct result. I was so excited when I got this result, because it mirrors very closely with that of Sutton and Barto!</p>

<center>
<img src="sutton.png" alt="Suttons results">
<p><small>Sutton's results (Sutton and Barto)</small></p>
</center>



<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>I went into this deep dive because I had trouble understanding this myself. I’m still not entirely sure I understand it, but writing out the code certainly helps. Sutton and Barto’s <a target="_blank" rel="noopener" href="http://incompleteideas.net/book/the-book.html">book</a> on RL really is a must. Other useful resources are these series on YouTube:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLwRJQ4m4UJjNymuBM9RdmB3Z9N5-0IlY0">Foundations of deep RL</a></li>
<li><a target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLqYmG7hTraZBKeNJ-JE_eyJHZ7XgBoAyb">DeepMind x UCL | Reinforcement Learning Course</a></li>
</ul>
<p>And of course, Andrej Karpathy’s <a target="_blank" rel="noopener" href="http://karpathy.github.io/2016/05/31/rl/">blog post</a>.</p>
<p><a target="_blank" rel="noopener" href="https://news.ycombinator.com/item?id=39729731">Comments</a></p>


                
            </div>

        </div>
    </div>
</article>

    <!-- Footer -->
    <hr />

<!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                    

                    

                    

                    

                    
                        <li>
                            <a href="mailto:info@loreley.one" target="_blank">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-envelope-o fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                    

                    
                </ul>
                <p class="copyright text-muted">&copy; 2024 Lukas<br></p>



                
            </div>
        </div>
    </div>
</footer>


    <!-- After footer scripts -->
    
<!--Tiny bird analytics-->
<script defer src="/js/analytics.js" data-host="https://api.tinybird.co" data-token="p.eyJ1IjogImYxMGY0NmY5LWI2MTQtNGY3Mi1hNWRkLTJkZWMyZTYzOGU0ZCIsICJpZCI6ICJmM2Y3MDk5Ny03YTZiLTRlYzUtYTdkNC01ZTVkMmY1ZjZjMDcifQ.uBS7CItgVesD7IqUpKSyA_g6UNTH-YHuH3-XfpWy1o0"></script>




</body>

</html>